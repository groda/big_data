{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZzCm85z_kSp"
      },
      "source": [
        "# Ngrams with PySpark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Job Nr.1 (Common Crawl)"
      ],
      "metadata": {
        "id": "sk3SzSIYySMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve the data"
      ],
      "metadata": {
        "id": "VaH8VIEhFnXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us download some text data from the [Common Crawl archive](https://data.commoncrawl.org/) (to know why I chose this dataset see my [conversation with AI](https://gemini.google.com/share/e94f879fa56a))."
      ],
      "metadata": {
        "id": "g9w42smTFpuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import gzip\n",
        "\n",
        "# The URL for the index of the January 2026 crawl archive (https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/index.html)\n",
        "index_url = \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/wet.paths.gz\"\n",
        "\n",
        "local_filename = \"wet.paths.gz\"\n",
        "\n",
        "print(\"Downloading index...\")\n",
        "with requests.get(index_url, stream=True) as r:\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "print(\"Download done! Now you can open it normally with gzip.open().\")\n",
        "\n",
        "# Read the content of the gzipped file\n",
        "with gzip.open('wet.paths.gz', 'rb') as f:\n",
        "    # Read all content and decode it\n",
        "    content = f.read().decode('utf-8')\n",
        "\n",
        "# Split the content into lines, removing any empty strings from splitting\n",
        "paths = [line for line in content.strip().split('\\n') if line]\n",
        "\n",
        "# Define the output filename\n",
        "output_filename = \"wet_paths_list.txt\"\n",
        "\n",
        "# Save the paths to a new text file, one path per line\n",
        "with open(output_filename, 'w') as out_f:\n",
        "    for path in paths:\n",
        "        out_f.write(path + '\\n')\n",
        "\n",
        "print(f\"Paths saved to {output_filename}\")\n",
        "print(\"\\nFirst 10 paths:\")\n",
        "for i, path in enumerate(paths[:10]):\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5UyqZZxD5UB",
        "outputId": "1b893e42-66de-484f-e2e8-7e9b9c26aa79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading index...\n",
            "Download done! Now you can open it normally with gzip.open().\n",
            "Paths saved to wet_paths_list.txt\n",
            "\n",
            "First 10 paths:\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00000.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00001.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00002.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00003.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00004.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00005.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00006.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00007.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00008.warc.wet.gz\n",
            "crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00009.warc.wet.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "n = 5\n",
        "print(f\"Download data from the first {n} paths:\")\n",
        "for i, path in enumerate(paths[:n]):\n",
        "    full_url = \"https://data.commoncrawl.org/\" + path\n",
        "    local_filename = os.path.basename(path) # Extract filename from the path\n",
        "    print(f\"Downloading {full_url} to {local_filename}\")\n",
        "    try:\n",
        "        with requests.get(full_url, stream=True) as r:\n",
        "            r.raise_for_status() # Raise an exception for bad status codes\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        print(f\"Successfully downloaded {local_filename}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {full_url}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VLfPfrBEgTJ",
        "outputId": "50054b28-85ef-482e-9c03-e22c512739af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download data from the first 5 paths:\n",
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00000.warc.wet.gz to CC-MAIN-20260112161239-20260112191239-00000.warc.wet.gz\n",
            "Successfully downloaded CC-MAIN-20260112161239-20260112191239-00000.warc.wet.gz\n",
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00001.warc.wet.gz to CC-MAIN-20260112161239-20260112191239-00001.warc.wet.gz\n",
            "Successfully downloaded CC-MAIN-20260112161239-20260112191239-00001.warc.wet.gz\n",
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00002.warc.wet.gz to CC-MAIN-20260112161239-20260112191239-00002.warc.wet.gz\n",
            "Successfully downloaded CC-MAIN-20260112161239-20260112191239-00002.warc.wet.gz\n",
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00003.warc.wet.gz to CC-MAIN-20260112161239-20260112191239-00003.warc.wet.gz\n",
            "Successfully downloaded CC-MAIN-20260112161239-20260112191239-00003.warc.wet.gz\n",
            "Downloading https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/wet/CC-MAIN-20260112161239-20260112191239-00004.warc.wet.gz to CC-MAIN-20260112161239-20260112191239-00004.warc.wet.gz\n",
            "Successfully downloaded CC-MAIN-20260112161239-20260112191239-00004.warc.wet.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data"
      ],
      "metadata": {
        "id": "dmAE0sIi1P5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract some English text from the archives. For that we are going to use the `warcio` library."
      ],
      "metadata": {
        "id": "1YAfpHxzNLwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install warcio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLwIPXCBjMET",
        "outputId": "cb855849-2da0-4d6e-cecd-ea998b1f5361"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: warcio in /usr/local/lib/python3.12/dist-packages (1.7.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from warcio) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some filter to:\n",
        "\n",
        "\n",
        "*  Select records in the English language (`WARC-Identified-Content-Language` = `eng`)\n",
        "*  Pick records with long lines (function `is_high_quality_record`) in the hope that the record will contain narrative text and not junk\n",
        "\n"
      ],
      "metadata": {
        "id": "W4XYFuMKOKqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "from pathlib import Path\n",
        "from warcio.archiveiterator import ArchiveIterator\n",
        "\n",
        "def is_high_quality_record(record_text, min_line_len=80, threshold=0.7):\n",
        "    # Split and clean lines for calculation\n",
        "    lines = [l.strip() for l in record_text.split('\\n') if l.strip()]\n",
        "    if not lines:\n",
        "        return False\n",
        "\n",
        "    total_chars = sum(len(l) for l in lines)\n",
        "    long_line_chars = sum(len(l) for l in lines if len(l) >= min_line_len)\n",
        "\n",
        "    narrative_ratio = long_line_chars / total_chars\n",
        "    return narrative_ratio >= threshold\n",
        "\n",
        "def process_all_archives(directory_path, output_path):\n",
        "    # Open in 'w' mode to ensure the file is empty or created before writing.\n",
        "    # Subsequent writes will then append to this cleared/new file.\n",
        "    with open(output_path, 'w', encoding='utf-8') as out_f:\n",
        "        # Looking for the compressed WET files\n",
        "        for file_path in Path(directory_path).rglob('*.wet.gz'):\n",
        "            print(f\"Processing: {file_path}\")\n",
        "\n",
        "            with open(file_path, 'rb') as stream:\n",
        "                for record in ArchiveIterator(stream):\n",
        "                    if record.rec_type == 'conversion':\n",
        "\n",
        "                        # 1. Language Check\n",
        "                        lang = record.rec_headers.get_header('WARC-Identified-Content-Language')\n",
        "                        if lang != 'eng':\n",
        "                            continue\n",
        "\n",
        "                        # 2. Extract Body\n",
        "                        try:\n",
        "                            text_content = record.content_stream().read().decode('utf-8', 'ignore')\n",
        "                        except Exception:\n",
        "                            continue\n",
        "\n",
        "                        # 3. Quality Filter\n",
        "                        if is_high_quality_record(text_content):\n",
        "                            # Write the raw text block exactly as it is\n",
        "                            out_f.write(text_content.strip())\n",
        "\n",
        "                            # Add a separator so the next record doesn't start on the same line\n",
        "                            out_f.write(\"\\n\\n\" + \"-\"*40 + \"\\n\\n\")\n",
        "\n",
        "# Run the process\n",
        "process_all_archives('.', 'narrative_corpus.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otV1BwldhqqO",
        "outputId": "6ff6c35f-91db-4c82-ff99-2390b02da55c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: CC-MAIN-20260112161239-20260112191239-00001.warc.wet.gz\n",
            "Processing: CC-MAIN-20260112161239-20260112191239-00004.warc.wet.gz\n",
            "Processing: CC-MAIN-20260112161239-20260112191239-00003.warc.wet.gz\n",
            "Processing: CC-MAIN-20260112161239-20260112191239-00002.warc.wet.gz\n",
            "Processing: CC-MAIN-20260112161239-20260112191239-00000.warc.wet.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count number of lines in `narrative_corpus.txt`."
      ],
      "metadata": {
        "id": "4bzod3nyNSe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh narrative_corpus.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYtN-3ddkl_i",
        "outputId": "bb7ac047-9189-4b56-971a-c05578015bae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 84M Feb 22 10:46 narrative_corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Spark context\n",
        "\n",
        "A Spark context (or a session, that encapsulates a context) is the entry gate for Spark.\n",
        "It represents the Spark engine (whether on the local machine or on a cluster) and provides an API for creating and running data pipelines.\n",
        "\n",
        "In this example, we're going to load a text file into a RDD, split the text into ngrams, and count the frequency of ngrams."
      ],
      "metadata": {
        "id": "vD3YDS8JMgGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A note on memory configuration in Google Colab\n",
        "\n",
        "In a typical distributed Spark cluster, `spark.driver.memory` and `spark.executor.memory` refer to distinct JVMs on different machines, so they would be additive. However, in our current setup (the default for PySpark is `local[*]`), both the driver and executor essentially run within the same JVM process on your single Colab instance.\n",
        "\n",
        "Because they share the same JVM, it makes sense to optimize memory allocation for the 12GB Colab environment by setting `spark.driver.memory` as well as `spark.executor.memory` to 10g. This will allow Spark to utilize up to $10$GB of the available RAM, while leaving some buffer for the operating system and Python interpreter. This should provide a good balance."
      ],
      "metadata": {
        "id": "Ia5v_d_o8RBp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QLCYd7Bu_kSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f84fbae-b265-4003-ee72-14d9968aa635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext initialized with increased memory settings.\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from operator import add\n",
        "\n",
        "# Configure Spark to use more memory\n",
        "conf = (SparkConf()\n",
        "    .setAppName(\"Ngrams with PySpark\")\n",
        "    .set(\"spark.driver.memory\", \"10g\")  # Allocate 10GB for the driver (and implicitly, the single local executor)\n",
        "    .set(\"spark.executor.memory\", \"10g\") # Keep executor memory aligned with driver for local mode\n",
        ")\n",
        "sc = SparkContext(\n",
        "    conf=conf\n",
        ")\n",
        "print(\"SparkContext initialized with increased memory settings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ETZPHL0E_kSt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "84cc2d10-3df7-4cfb-a8c5-b3dffb901378"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Ngrams with PySpark>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1d7172405f47:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Ngrams with PySpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2JtRpdh_kSt"
      },
      "source": [
        "We are going to use the file `narrative_corpus.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq35ZGIh_kSu"
      },
      "source": [
        "### Create RDD from file\n",
        "\n",
        "The second parameter ($8$)  indicates the desired number of partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "k-woCjQS_kSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52525213-d0c4-4e05-ec8e-e06e5efe83a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "textFile is of type: <class 'pyspark.core.rdd.RDD'>\n",
            "Number of partitions: 8\n"
          ]
        }
      ],
      "source": [
        "textFile = sc.textFile(\"narrative_corpus.txt\", 8)\n",
        "print(\"textFile is of type: {}\\nNumber of partitions: {}\". \\\n",
        "      format(type(textFile), textFile.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV4OQ6zz_kSv"
      },
      "source": [
        "## Extract trigrams\n",
        "\n",
        "The next pipeline only contains transformations (`flatMap`, `map`, `reduceByKey`, `sortBy`), this means that no actual computation takes place due to Spark's _laziness_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8CrbMDAF_kSv"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "ngrams = (textFile\n",
        "    .flatMap(lambda x: [x.split()])  # Each line is split into words, creating an RDD of lists of words (one list per line)\n",
        "    .flatMap(lambda x: [tuple(y) for y in zip(*[x[i:] for i in range(n)])])  # Generates word ngrams from each list of words\n",
        "    .map(lambda x: (x, 1))\n",
        "    .reduceByKey(add)\n",
        "    .sortBy(lambda x: x[1], ascending=False) # This transformation performs a global sort and is computationally expensive. It is only executed when an action (like .take() in the next cell) is called.\n",
        ")\n",
        "\n",
        "# Note: Spark transformations are lazy. The code above only defines the computation plan.\n",
        "# The actual work (and thus the perceived slowness) will occur when an action (e.g., .take(), .count(), .saveAsTextFile()) is invoked on this 'ngrams' RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ngrams` is an object of type `RDD`"
      ],
      "metadata": {
        "id": "Q2Fqn4BSYhk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rj7GdBIR_kSv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "a49f021d-6202-4a24-8ccb-9efc5e4ad4e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.core.rdd.PipelinedRDD"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.core.rdd.PipelinedRDD</b><br/>def __init__(prev: RDD[T], func: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool=False, isFromBarrier: bool=False)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/pyspark/core/rdd.py</a>Examples\n",
              "--------\n",
              "Pipelined maps:\n",
              "\n",
              "&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()\n",
              "[4, 8, 12, 16]\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()\n",
              "[4, 8, 12, 16]\n",
              "\n",
              "Pipelined reduces:\n",
              "\n",
              "&gt;&gt;&gt; from operator import add\n",
              "&gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)\n",
              "20\n",
              "&gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)\n",
              "20</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 5267);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "type(ngrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNY0GqY8_kSv"
      },
      "source": [
        "Up to now we've just carried out a series of _transformations_. Spark hasn't jet done any computation. By applying the _action_ `take` we first act on the data to get a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fyzLH4uH_kSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "327813da-0738-441e-cca2-8617abbedb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1 1               34998\n",
            "one of the          2801\n",
            "as well as          2412\n",
            "Skip to content     1671\n",
            "be able to          1594\n",
            "â˜… â˜… â˜…               1453\n",
            "If you are          1431\n",
            "a lot of            1429\n",
            "Replicas Bags Replicas1409\n",
            "Bags Replicas Bags  1409\n"
          ]
        }
      ],
      "source": [
        "for (ngram, count) in ngrams.take(10):\n",
        "    print(\"{:<20}{:>d}\".format(' '.join(ngram), count))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our small sample of the Common Crawl already contains some spammy content, apparently (\"_Replicas Bags Replicas_\" trigram), despite the basic filtering.\n",
        "\n",
        "Well, if you explore the Common Crawl expecting a pristine snapshot of human knowledge you're going to be disappointed! You are going to need quite some data engineering work to filter the \"raw sewage\". Common Crawl is in fact designed to be a neutral, unfiltered crawl of the open web, and unfortunately, a massive percentage of the open web is composed of SEO-optimized adult content, spam, and \"junk\" domains ðŸ˜•.\n",
        "\n",
        "The good news is that you won't run out of work with your Spark tools any time soon ðŸ™‚.\n"
      ],
      "metadata": {
        "id": "ypDebvHBZDZJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNnyTWBn_kSw"
      },
      "source": [
        "### Transformations and actions seen so far\n",
        "\n",
        "**Transformations**\n",
        "- `map`\n",
        "- `flatMap`\n",
        "- `filter`\n",
        "- `reduceByKey`\n",
        "- `sortBy`\n",
        "\n",
        "**Actions**\n",
        "- `take`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Job Nr. 2 (genomic data)"
      ],
      "metadata": {
        "id": "SFtFReud04o2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxiqGZD7_kSw"
      },
      "source": [
        "## Download genomic file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6189e2ff"
      },
      "source": [
        "You can find the `GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna` file on the NCBI FTP server. I will download the gzipped version and then decompress it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e80825c",
        "outputId": "c6ff459a-8237-4aed-c3eb-23f50d1c456e"
      },
      "source": [
        "import requests\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "file_url = \"https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/711/455/GCA_003711455.1_HG02106_EEE_SV-Pop.1/GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna.gz\"\n",
        "local_gzipped_filename = \"GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna.gz\"\n",
        "local_uncompressed_filename = \"GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna\"\n",
        "\n",
        "print(f\"Downloading {file_url}...\")\n",
        "\n",
        "try:\n",
        "    with requests.get(file_url, stream=True) as r:\n",
        "        r.raise_for_status()  # Raise an exception for bad status codes\n",
        "        with open(local_gzipped_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    print(f\"Successfully downloaded {local_gzipped_filename}\")\n",
        "\n",
        "    print(f\"Decompressing {local_gzipped_filename}...\")\n",
        "    with gzip.open(local_gzipped_filename, 'rb') as f_in:\n",
        "        with open(local_uncompressed_filename, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    print(f\"Successfully decompressed to {local_uncompressed_filename}\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading {file_url}: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during decompression: {e}\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/003/711/455/GCA_003711455.1_HG02106_EEE_SV-Pop.1/GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna.gz...\n",
            "Successfully downloaded GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna.gz\n",
            "Decompressing GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna.gz...\n",
            "Successfully decompressed to GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dc040ce"
      },
      "source": [
        "Now the file `GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna` should be available in your Colab environment. It is a pretty large file ($1.4$GB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1ywIKMjV_kSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2a3def-7da7-4c08-d8a3-011d49c3a201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 1.4G Feb 22 10:50 GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna\n"
          ]
        }
      ],
      "source": [
        "!ls -lh GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LFOQ7Al7_kSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892b330c-2744-443a-c426-a55b10c4b92d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['>QVRK01000602.1 Homo sapiens isolate HG02106 chromosome 1 1-100500000:0, whole genome shotgun sequence',\n",
              " 'CCCCAGCCACCCTTgcttccctgccccagccttccatcTCATCTCTCTTGCTTCCATCTCTGGCTTTTCCACTCCAGCCA']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "genomeFile = sc.textFile(\"GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fna\", minPartitions=12)\n",
        "genomeFile.take(2)[:140]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's a shotgun sequence (in the context of genomic files)?\n",
        "\n",
        "**A \"shotgun sequence\"** in a genomic file almost always refers to a **DNA sequencing read** (or a short DNA sequence fragment) that comes from **shotgun sequencing**, a widely used method for determining the sequence of genomes.\n",
        "\n",
        "#### What is shotgun sequencing?\n",
        "Shotgun sequencing is a technique for reading the DNA of an entire genome (or large genomic regions). The name comes from the idea of a \"shotgun blast\" â€” random and widespread fragmentation.\n",
        "\n",
        "The basic process works like this:\n",
        "\n",
        "1. Take the long DNA molecule (e.g., a whole chromosome or entire genome)  \n",
        "2. Randomly break it into many small overlapping fragments (mechanically or enzymatically)  \n",
        "3. Sequence each of these small fragments individually â†’ each resulting short sequence is called a **read**  \n",
        "4. Use powerful computers to find overlapping regions between all these reads  \n",
        "5. Piece the overlapping reads back together computationally to reconstruct the original long sequence (this is called **assembly**)\n",
        "\n",
        "This random, parallel approach made it possible to sequence very large genomes much faster and cheaper than earlier ordered methods.\n",
        "\n",
        "#### What you'll actually see in a genomic file\n",
        "In FASTA, FASTQ, or other sequence files from a shotgun sequencing project, each entry is typically one of these short pieces â€” one \"shotgun sequence\":\n",
        "\n",
        "- Header line (e.g. `@read_name` or `>contig_001`)  \n",
        "- Followed by the actual DNA sequence (usually 50â€“300 bp for short-read technologies like Illumina, or thousands of bp for long-read tech like PacBio/Oxford Nanopore)\n",
        "\n",
        "Example of what one might look like in a file:\n",
        "\n",
        "```\n",
        ">SRR123456.1 length=151\n",
        "GATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATCGATC\n",
        "```\n",
        "\n",
        "That string of A's, C's, G's and T's **is** a shotgun sequence â€” a small random piece of the genome that came from the fragmentation step.\n",
        "\n",
        "#### Common variations/contexts\n",
        "- **Whole-genome shotgun sequencing (WGS)** â€” shotgun applied to the entire genome at once (most common today)  \n",
        "- **Hierarchical shotgun** â€” older approach where the genome was first broken into large mapped clones (BACs), then shotgun-sequenced individually  \n",
        "- **Shotgun metagenomics** â€” same principle, but applied to mixed microbial communities (e.g. gut, soil, ocean samples) instead of one organism\n",
        "\n",
        "In modern genomics files (especially from next-generation sequencing), almost everything you see labeled as sequences in raw data files are shotgun sequences/reads â€” unless the file is already a finished assembly (contigs/chromosomes).\n",
        "\n",
        "So when someone says \"shotgun sequence\" in the context of a genomic file, they're usually referring to one of those raw, short, randomly sampled DNA reads that are the building blocks used to reconstruct the full genome."
      ],
      "metadata": {
        "id": "iJxO5gYt2JhD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM6Y6rbU_kSx"
      },
      "source": [
        "This passage is to remove newlines and use comment lines (beginning with \">\") as block delimiters. The new file is saved in `GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fnaNN`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess"
      ],
      "metadata": {
        "id": "aXPX3Dwo1AHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fnaNN"
      ],
      "metadata": {
        "id": "1Az6ZV9F-q-l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "umR0iJOD_kSx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "genomeFile \\\n",
        "  .map(lambda x: re.sub('^>.*', '---', x)) \\\n",
        "  .map(lambda x: x.upper()) \\\n",
        "  .map(lambda x: re.sub('^$', '\\n', x)) \\\n",
        "  .saveAsTextFile(\"GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fnaNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-LMDIQ2v_kSx"
      },
      "outputs": [],
      "source": [
        "genomeFile = sc.textFile(\"GCA_003711455.1_HG02106_EEE_SV-Pop.1_genomic.fnaNN\", minPartitions=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract n-grams"
      ],
      "metadata": {
        "id": "cxEAx2VB1Dke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bKTBuTbP_kSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623da6d7-bf81-4aae-90ca-2051bcce3e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 186 ms, sys: 50.4 ms, total: 237 ms\n",
            "Wall time: 22min 37s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "n = 3\n",
        "ngrams = (genomeFile\n",
        "          .map(lambda x: re.sub('\\n', '', x))\n",
        "          .flatMap(lambda x: x.split())\n",
        "          .flatMap(lambda x: [tuple(y) for y in zip(*[x[i:] for i in range(n)])])\n",
        "          .map(lambda x: (x, 1))\n",
        "          .reduceByKey(add)\n",
        "          .sortBy(lambda x: x[1], ascending=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "njpjrTtk_kSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc92852d-925e-4880-c1cb-c5eff2ca29c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T T T               46984458\n",
            "A A A               46913491\n",
            "A A T               30131208\n",
            "A T T               30115609\n",
            "C A G               29844683\n",
            "C T G               29717935\n",
            "A G A               29179399\n",
            "T C T               29140174\n",
            "A C A               26962506\n",
            "T G T               26932345\n",
            "CPU times: user 20.8 ms, sys: 3.63 ms, total: 24.4 ms\n",
            "Wall time: 9.57 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for (ngram, count) in ngrams.take(10):\n",
        "    print(\"{:<20}{:>d}\".format(' '.join(ngram), count))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}