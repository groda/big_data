{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Hadoop_Setting_up_a_Single_Node_Cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEF3qldGPj3T"
   },
   "source": [
    "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
    "\n",
    "# HDFS and MapReduce on a single-node Hadoop cluster\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
    "\n",
    "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
    "\n",
    "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
    "\n",
    "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
    "\n",
    "Finally, we'll proceed to shut down the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "hGm3LhVEWXr9"
   },
   "source": [
    "**TABLE OF CONTENTS**\n",
    "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
    "\n",
    " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
    "\n",
    " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
    "\n",
    "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
    "\n",
    " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
    "\n",
    " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
    "\n",
    " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
    "\n",
    " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
    "\n",
    "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
    "\n",
    "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
    "\n",
    "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
    "\n",
    "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
    "\n",
    "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
    "\n",
    "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
    "\n",
    "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
    "\n",
    "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
    "\n",
    "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
    "\n",
    "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
    "\n",
    "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
    "\n",
    "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
    "\n",
    "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUuQjW2oNMcJ"
   },
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFfOrktMPq8M"
   },
   "source": [
    "## Check the available Java version\n",
    " Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only). See: https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuWqBiV89ryq"
   },
   "source": [
    "Check if Java version is one of `8`, `11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:43.821886Z",
     "iopub.status.busy": "2025-07-13T15:33:43.821681Z",
     "iopub.status.idle": "2025-07-13T15:33:44.040484Z",
     "shell.execute_reply": "2025-07-13T15:33:44.039601Z"
    },
    "id": "C7X0EZaMPrsD",
    "outputId": "2bf18811-0898-4d12-ac7c-20558aec8437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.27\" 2025-04-15\r\n",
      "OpenJDK Runtime Environment Temurin-11.0.27+6 (build 11.0.27+6)\r\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.27+6 (build 11.0.27+6, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:44.043300Z",
     "iopub.status.busy": "2025-07-13T15:33:44.042787Z",
     "iopub.status.idle": "2025-07-13T15:33:44.111490Z",
     "shell.execute_reply": "2025-07-13T15:33:44.110795Z"
    },
    "id": "lABuOV124G4x",
    "outputId": "05c1947a-8370-4813-89b7-76cb6984d4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version is one of 8, 11 âœ“\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
    "if [[ $JAVA_MAJOR_VERSION -eq 8 || $JAVA_MAJOR_VERSION -eq 11 ]]\n",
    " then\n",
    " echo \"Java version is one of 8, 11 âœ“\"\n",
    " fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWROofISgKKW"
   },
   "source": [
    "Find the variable for the environment variable `JAVA_HOME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH4AGbkLP3iK"
   },
   "source": [
    "Find the path for the environment variable `JAVA_HOME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:44.114178Z",
     "iopub.status.busy": "2025-07-13T15:33:44.113524Z",
     "iopub.status.idle": "2025-07-13T15:33:44.226780Z",
     "shell.execute_reply": "2025-07-13T15:33:44.225992Z"
    },
    "id": "mCmk5GOqv0Y-",
    "outputId": "4efba03e-3fc7-4214-c37b-dd238a840f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/temurin-11-jdk-amd64/bin/java\r\n"
     ]
    }
   ],
   "source": [
    "!readlink -f $(which java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGHKH3Vu9Nwl"
   },
   "source": [
    "Extract JAVA_HOME from the Java path by removing the `bin/java` part in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:44.229156Z",
     "iopub.status.busy": "2025-07-13T15:33:44.228925Z",
     "iopub.status.idle": "2025-07-13T15:33:44.247147Z",
     "shell.execute_reply": "2025-07-13T15:33:44.246486Z"
    },
    "id": "Dd7en2Cv68ce",
    "outputId": "40430612-b7e6-4c82-99ca-3a348ba9d99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/temurin-11-jdk-amd64\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE7kSYSXQYLf"
   },
   "source": [
    "## Download core Hadoop\n",
    "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
    "\n",
    "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:44.249529Z",
     "iopub.status.busy": "2025-07-13T15:33:44.249126Z",
     "iopub.status.idle": "2025-07-13T15:33:46.946241Z",
     "shell.execute_reply": "2025-07-13T15:33:46.945535Z"
    },
    "id": "54LqS5Rkgyli",
    "outputId": "6ab71854-2018-4711-fda9-2e268cf6024c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-13 15:33:44--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\r\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 965537117 (921M) [application/x-gzip]\r\n",
      "Saving to: â€˜hadoop-3.4.0.tar.gzâ€™\r\n",
      "\r\n",
      "\r",
      "hadoop-3.4.0.tar.gz   0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz   7%[>                   ]  69.33M   347MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  15%[==>                 ] 146.97M   367MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  24%[===>                ] 223.00M   372MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  32%[=====>              ] 297.16M   371MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  40%[=======>            ] 372.16M   372MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  48%[========>           ] 447.43M   373MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  56%[==========>         ] 518.61M   370MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  64%[===========>        ] 594.76M   372MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  72%[=============>      ] 663.67M   369MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  79%[==============>     ] 735.00M   367MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  87%[================>   ] 805.46M   366MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  95%[==================> ] 880.58M   367MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz 100%[===================>] 920.81M   367MB/s    in 2.5s    \r\n",
      "\r\n",
      "2025-07-13 15:33:46 (367 MB/s) - â€˜hadoop-3.4.0.tar.gzâ€™ saved [965537117/965537117]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um2CARkgg22j"
   },
   "source": [
    "Uncompress archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:46.948892Z",
     "iopub.status.busy": "2025-07-13T15:33:46.948369Z",
     "iopub.status.idle": "2025-07-13T15:33:56.349079Z",
     "shell.execute_reply": "2025-07-13T15:33:56.348312Z"
    },
    "id": "C17WYI0mQRE8"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"hadoop-3.4.0\" ]; then\n",
    "  tar xzf hadoop-3.4.0.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGI4TNXPamMr"
   },
   "source": [
    "### Verify the downloaded file\n",
    "\n",
    "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATofMJRXhJ4K"
   },
   "source": [
    "Download sha512 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:56.351816Z",
     "iopub.status.busy": "2025-07-13T15:33:56.351404Z",
     "iopub.status.idle": "2025-07-13T15:33:56.502034Z",
     "shell.execute_reply": "2025-07-13T15:33:56.501376Z"
    },
    "id": "NhTinHLqCrFQ",
    "outputId": "7d7e0b60-0ed0-4239-c91b-a2b6daa25315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-13 15:33:56--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512\r\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\r\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 160 [text/plain]\r\n",
      "Saving to: â€˜hadoop-3.4.0.tar.gz.sha512â€™\r\n",
      "\r\n",
      "\r",
      "          hadoop-3.   0%[                    ]       0  --.-KB/s               \r",
      "hadoop-3.4.0.tar.gz 100%[===================>]     160  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2025-07-13 15:33:56 (15.1 MB/s) - â€˜hadoop-3.4.0.tar.gz.sha512â€™ saved [160/160]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL8FxjalhFAn"
   },
   "source": [
    "Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:56.504332Z",
     "iopub.status.busy": "2025-07-13T15:33:56.504112Z",
     "iopub.status.idle": "2025-07-13T15:33:57.881905Z",
     "shell.execute_reply": "2025-07-13T15:33:57.881243Z"
    },
    "id": "zL302M1OhFMH",
    "outputId": "a070f00d-3860-4a51-e33c-a86c20688ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "A=$(sha512sum hadoop-3.4.0.tar.gz | cut - -d' ' -f1)\n",
    "B=$(cut hadoop-3.4.0.tar.gz.sha512 -d' ' -f4)\n",
    "printf \"%s\\n%s\\n\" $A $B\n",
    "[[ $A == $B ]] && echo \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlgP1ytnRtUK"
   },
   "source": [
    "## Configure `PATH`\n",
    "\n",
    "Add the Hadoop folder to the `PATH` environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:57.884383Z",
     "iopub.status.busy": "2025-07-13T15:33:57.883948Z",
     "iopub.status.idle": "2025-07-13T15:33:57.994996Z",
     "shell.execute_reply": "2025-07-13T15:33:57.994112Z"
    },
    "id": "49xx-zSxIdxa",
    "outputId": "9403498c-8e5e-4401-df95-44de81e11c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:57.997627Z",
     "iopub.status.busy": "2025-07-13T15:33:57.997405Z",
     "iopub.status.idle": "2025-07-13T15:33:58.001564Z",
     "shell.execute_reply": "2025-07-13T15:33:58.000843Z"
    },
    "id": "6V03we10Igek"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.0')\n",
    "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
    "#os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.003968Z",
     "iopub.status.busy": "2025-07-13T15:33:58.003562Z",
     "iopub.status.idle": "2025-07-13T15:33:58.006783Z",
     "shell.execute_reply": "2025-07-13T15:33:58.006152Z"
    },
    "id": "Aif21X1ONvwH",
    "outputId": "729bbd33-1098-431e-f31b-a0dd06b35b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'LANG': 'C.UTF-8', 'PATH': '/home/runner/work/big_data/big_data/hadoop-3.4.0/bin:/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'MAIL': '/var/mail/root', 'LOGNAME': 'root', 'USER': 'root', 'HOME': '/root', 'SHELL': '/bin/bash', 'TERM': 'xterm-color', 'SUDO_COMMAND': '/usr/bin/env PATH=/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin jupyter nbconvert --log-level=ERROR --to notebook --execute Hadoop_Setting_up_a_Single_Node_Cluster.ipynb', 'SUDO_USER': 'runner', 'SUDO_UID': '1001', 'SUDO_GID': '118', 'ImageVersion': '20250710.1.0', 'ImageOS': 'ubuntu24', 'ACCEPT_EULA': 'Y', 'XDG_CONFIG_HOME': '/home/runner/.config', 'AGENT_TOOLSDIRECTORY': '/opt/hostedtoolcache', 'RUNNER_TOOL_CACHE': '/opt/hostedtoolcache', 'ACTIONS_RUNNER_ACTION_ARCHIVE_CACHE': '/opt/actionarchivecache', 'AZURE_EXTENSION_DIR': '/opt/az/azcliextensions', 'SWIFT_PATH': '/usr/share/swift/usr/bin', 'DOTNET_SKIP_FIRST_TIME_EXPERIENCE': '1', 'DOTNET_NOLOGO': '1', 'DOTNET_MULTILEVEL_LOOKUP': '0', 'EDGEWEBDRIVER': '/usr/local/share/edge_driver', 'GECKOWEBDRIVER': '/usr/local/share/gecko_driver', 'CHROME_BIN': '/usr/bin/google-chrome', 'CHROMEWEBDRIVER': '/usr/local/share/chromedriver-linux64', 'BOOTSTRAP_HASKELL_NONINTERACTIVE': '1', 'GHCUP_INSTALL_BASE_PREFIX': '/usr/local', 'JAVA_HOME_8_X64': '/usr/lib/jvm/temurin-8-jdk-amd64', 'JAVA_HOME_11_X64': '/usr/lib/jvm/temurin-11-jdk-amd64', 'JAVA_HOME': '/usr/lib/jvm/temurin-17-jdk-amd64', 'JAVA_HOME_17_X64': '/usr/lib/jvm/temurin-17-jdk-amd64', 'JAVA_HOME_21_X64': '/usr/lib/jvm/temurin-21-jdk-amd64', 'ANT_HOME': '/usr/share/ant', 'GRADLE_HOME': '/usr/share/gradle-8.14.3', 'CONDA': '/usr/share/miniconda', 'NVM_DIR': '/home/runner/.nvm', 'SELENIUM_JAR_PATH': '/usr/share/java/selenium-server.jar', 'VCPKG_INSTALLATION_ROOT': '/usr/local/share/vcpkg', 'DEBIAN_FRONTEND': 'noninteractive', 'ANDROID_SDK_ROOT': '/usr/local/lib/android/sdk', 'ANDROID_HOME': '/usr/local/lib/android/sdk', 'ANDROID_NDK': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_HOME': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_ROOT': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_LATEST_HOME': '/usr/local/lib/android/sdk/ndk/28.2.13676358', 'PIPX_BIN_DIR': '/opt/pipx_bin', 'PIPX_HOME': '/opt/pipx', 'GOROOT_1_22_X64': '/opt/hostedtoolcache/go/1.22.12/x64', 'GOROOT_1_23_X64': '/opt/hostedtoolcache/go/1.23.10/x64', 'GOROOT_1_24_X64': '/opt/hostedtoolcache/go/1.24.4/x64', 'HOMEBREW_NO_AUTO_UPDATE': '1', 'HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS': '3650', 'RUNNER_USER': 'runner', 'DEPLOYMENT_BASEPATH': '/opt/runner', 'PERFLOG_LOCATION_SETTING': 'RUNNER_PERFLOG', 'POWERSHELL_DISTRIBUTION_CHANNEL': 'GitHub-Actions-ubuntu24', 'XDG_RUNTIME_DIR': '/run/user/1001', 'JPY_PARENT_PID': '2619', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'HADOOP_HOME': '/home/runner/work/big_data/big_data/hadoop-3.4.0'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.008792Z",
     "iopub.status.busy": "2025-07-13T15:33:58.008403Z",
     "iopub.status.idle": "2025-07-13T15:33:58.119344Z",
     "shell.execute_reply": "2025-07-13T15:33:58.118498Z"
    },
    "id": "GcM-idgZQqfV",
    "outputId": "44c3158a-e8e8-4283-cce3-466f9a31a79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/runner/work/big_data/big_data/hadoop-3.4.0/bin:/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLmxLQeJSb4A"
   },
   "source": [
    "## Configure `core-site.xml` and `hdfs-site.xml`\n",
    "\n",
    "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
    "\n",
    "**`etc/hadoop/core-site.xml`**\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "**`etc/hadoop/hdfs-site.xml`**\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.122276Z",
     "iopub.status.busy": "2025-07-13T15:33:58.121843Z",
     "iopub.status.idle": "2025-07-13T15:33:58.131725Z",
     "shell.execute_reply": "2025-07-13T15:33:58.131040Z"
    },
    "id": "_n2d2lqXSLU1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"<configuration> \\n\\\n",
    "    <property> \\n\\\n",
    "        <name>fs.defaultFS</name> \\n\\\n",
    "        <value>hdfs://localhost:9000</value> \\n\\\n",
    "    </property> \\n\\\n",
    "</configuration>\" >hadoop-3.4.0/etc/hadoop/core-site.xml\n",
    "\n",
    "echo -e \"<configuration> \\n\\\n",
    "    <property> \\n\\\n",
    "        <name>dfs.replication</name> \\n\\\n",
    "        <value>1</value> \\n\\\n",
    "    </property> \\n\\\n",
    "</configuration>\" >hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mdkNb-Cg9HW"
   },
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.134135Z",
     "iopub.status.busy": "2025-07-13T15:33:58.133660Z",
     "iopub.status.idle": "2025-07-13T15:33:58.245202Z",
     "shell.execute_reply": "2025-07-13T15:33:58.244473Z"
    },
    "id": "-ISxE4Gqg_LG",
    "outputId": "a69ca821-a48b-4979-e663-3bdad46c3219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<configuration> \r\n",
      "    <property> \r\n",
      "        <name>dfs.replication</name> \r\n",
      "        <value>1</value> \r\n",
      "    </property> \r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "cat hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXbSKFyeMqr2"
   },
   "source": [
    "## Set environment variables\n",
    "\n",
    "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.0/sbin`).\n",
    "```\n",
    "export HDFS_NAMENODE_USER=root\n",
    "export HDFS_DATANODE_USER=root\n",
    "export HDFS_SECONDARYNAMENODE_USER=root\n",
    "export YARN_RESOURCEMANAGER_USER=root\n",
    "export YARN_NODEMANAGER_USER=root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.248042Z",
     "iopub.status.busy": "2025-07-13T15:33:58.247596Z",
     "iopub.status.idle": "2025-07-13T15:33:58.260450Z",
     "shell.execute_reply": "2025-07-13T15:33:58.259699Z"
    },
    "id": "2_vn-TGyPe9V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: warning: behavior of -n is non-portable and may change in future; use --update=none instead\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cp -n hadoop-3.4.0/etc/hadoop/hadoop-env.sh hadoop-3.4.0/etc/hadoop/hadoop-env.sh.org\n",
    "cat <<ðŸ˜ƒ >hadoop-3.4.0/etc/hadoop/hadoop-env.sh\n",
    "export HDFS_NAMENODE_USER=root\n",
    "export HDFS_DATANODE_USER=root\n",
    "export HDFS_SECONDARYNAMENODE_USER=root\n",
    "export YARN_RESOURCEMANAGER_USER=root\n",
    "export YARN_NODEMANAGER_USER=root\n",
    "ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2-Fdp73cF0V"
   },
   "source": [
    "## Setup localhost access via SSH key\n",
    "\n",
    "We are going to allow passphraseless access to `localhost` with a secure key.\n",
    "\n",
    "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uxmv3RdUwiF"
   },
   "source": [
    "\n",
    "### Install `openssh` and start server\n",
    "\n",
    "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
    "\n",
    "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:58.263257Z",
     "iopub.status.busy": "2025-07-13T15:33:58.262747Z",
     "iopub.status.idle": "2025-07-13T15:33:59.915288Z",
     "shell.execute_reply": "2025-07-13T15:33:59.914504Z"
    },
    "id": "yOxz683FNuYH",
    "outputId": "06b37eb9-a0ec-49ab-b1f0-22b3688cf080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [144 B]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:2 http://azure.archive.ubuntu.com/ubuntu noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:3 http://azure.archive.ubuntu.com/ubuntu noble-updates InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:4 http://azure.archive.ubuntu.com/ubuntu noble-backports InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:5 http://azure.archive.ubuntu.com/ubuntu noble-security InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:6 https://packages.microsoft.com/repos/azure-cli noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:7 https://packages.microsoft.com/ubuntu/24.04/prod noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dependency tree...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading state information...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openssh-server is already the newest version (1:9.6p1-3ubuntu13.12).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting ssh (via systemctl): ssh.service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install openssh-server\n",
    "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config\n",
    "sudo /etc/init.d/ssh restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYKoSlaENuyG"
   },
   "source": [
    "### Generate key\n",
    "Generate an SSH key that does not require a password.\n",
    "\n",
    "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
    "\n",
    "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:33:59.917902Z",
     "iopub.status.busy": "2025-07-13T15:33:59.917481Z",
     "iopub.status.idle": "2025-07-13T15:34:00.358980Z",
     "shell.execute_reply": "2025-07-13T15:34:00.358310Z"
    },
    "id": "YHOjUaxHSsQD",
    "outputId": "475e9552-4f57-4108-f4a0-c632b52a58dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private rsa key pair.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your identification has been saved in /root/.ssh/id_rsa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public key has been saved in /root/.ssh/id_rsa.pub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key fingerprint is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA256:2pr28+wRpR9CSGvlONDu37pX3vpo6a8I3dgHO8fSETs root@fv-az1277-496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key's randomart image is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---[RSA 3072]----+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      ... .      |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       o.*       |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       .* o .  . |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       ..o o    o|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       .S + . .E |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       o.  = =.=o|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      . ..o.+o*o=|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|      .o...oo.+*.|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     .o..+*+.o==o|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm $HOME/.ssh/id_rsa\n",
    "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwA6rKpScnVi"
   },
   "source": [
    "### Check SSH connection to localhost\n",
    "\n",
    "The following command should output \"hi!\" if the connection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:00.361553Z",
     "iopub.status.busy": "2025-07-13T15:34:00.361073Z",
     "iopub.status.idle": "2025-07-13T15:34:02.591515Z",
     "shell.execute_reply": "2025-07-13T15:34:02.590628Z"
    },
    "id": "hqIRVxcfce0F",
    "outputId": "a6a130c9-1717-4b74-a93a-f945c7fdd911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ðŸ‘‹\r\n"
     ]
    }
   ],
   "source": [
    "!ssh localhost \"echo hi ðŸ‘‹\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68C4cDySyek"
   },
   "source": [
    "# Launch a single-node Hadoop cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTDPwnVlSbHS"
   },
   "source": [
    "## Initialize the namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:02.594336Z",
     "iopub.status.busy": "2025-07-13T15:34:02.594110Z",
     "iopub.status.idle": "2025-07-13T15:34:04.673394Z",
     "shell.execute_reply": "2025-07-13T15:34:04.672668Z"
    },
    "id": "Q-aicnKKLVKQ",
    "outputId": "4778340e-d7ea-472b-97ce-631e91e6e850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /home/runner/work/big_data/big_data/hadoop-3.4.0/logs does not exist. Creating.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:03,282 INFO namenode.NameNode: STARTUP_MSG: \r\n",
      "/************************************************************\r\n",
      "STARTUP_MSG: Starting NameNode\r\n",
      "STARTUP_MSG:   host = fv-az1277-496/10.1.0.42\r\n",
      "STARTUP_MSG:   args = [-format, -nonInteractive]\r\n",
      "STARTUP_MSG:   version = 3.4.0\r\n",
      "STARTUP_MSG:   classpath = /home/runner/work/big_data/big_data/hadoop-3.4.0/etc/hadoop:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-common-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-compress-1.24.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-client-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-codec-1.15.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-core-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsch-0.1.55.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-client-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/token-provider-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-server-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-core-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/metrics-core-3.2.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/httpcore-4.4.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-math3-3.6.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/httpclient-4.5.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-cli-1.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/failureaccess-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/re2j-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsr305-3.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-config-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-text-1.10.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-logging-1.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/reload4j-1.2.22.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-framework-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-identity-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/gson-2.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-core-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/stax2-api-4.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-admin-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/checker-qual-2.5.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-net-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-json-1.20.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-server-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-io-2.14.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jline-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jettison-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/dnsjava-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-registry-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-kms-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-nfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/gson-2.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jline-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/codemodel-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/asm-tree-9.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jna-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/asm-commons-9.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/guice-4.2.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/objenesis-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar\r\n",
      "STARTUP_MSG:   build = git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760; compiled by 'root' on 2024-03-04T06:35Z\r\n",
      "STARTUP_MSG:   java = 17.0.15\r\n",
      "************************************************************/\r\n",
      "2025-07-13 15:34:03,290 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:03,387 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,018 INFO namenode.NameNode: Formatting using clusterid: CID-84e57919-64e3-4d57-a8cd-573dc564ff7c\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,034 INFO namenode.FSEditLog: Edit logging is async:true\r\n",
      "2025-07-13 15:34:04,072 INFO namenode.FSNamesystem: KeyProvider: null\r\n",
      "2025-07-13 15:34:04,073 INFO namenode.FSNamesystem: fsLock is fair: true\r\n",
      "2025-07-13 15:34:04,073 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,168 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\r\n",
      "2025-07-13 15:34:04,168 INFO namenode.FSNamesystem: supergroup             = supergroup\r\n",
      "2025-07-13 15:34:04,168 INFO namenode.FSNamesystem: isPermissionEnabled    = true\r\n",
      "2025-07-13 15:34:04,168 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\r\n",
      "2025-07-13 15:34:04,168 INFO namenode.FSNamesystem: HA Enabled: false\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,202 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,277 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,284 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\r\n",
      "2025-07-13 15:34:04,284 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\r\n",
      "2025-07-13 15:34:04,286 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\r\n",
      "2025-07-13 15:34:04,286 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jul 13 15:34:04\r\n",
      "2025-07-13 15:34:04,288 INFO util.GSet: Computing capacity for map BlocksMap\r\n",
      "2025-07-13 15:34:04,288 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-13 15:34:04,289 INFO util.GSet: 2.0% max memory 3.9 GB = 80 MB\r\n",
      "2025-07-13 15:34:04,289 INFO util.GSet: capacity      = 2^23 = 8388608 entries\r\n",
      "2025-07-13 15:34:04,296 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\r\n",
      "2025-07-13 15:34:04,296 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\r\n",
      "2025-07-13 15:34:04,300 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\r\n",
      "2025-07-13 15:34:04,300 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\r\n",
      "2025-07-13 15:34:04,300 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\r\n",
      "2025-07-13 15:34:04,300 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\r\n",
      "2025-07-13 15:34:04,300 INFO blockmanagement.BlockManager: defaultReplication         = 1\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: maxReplication             = 512\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: minReplication             = 1\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\r\n",
      "2025-07-13 15:34:04,301 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,338 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\r\n",
      "2025-07-13 15:34:04,338 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-13 15:34:04,338 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-13 15:34:04,338 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-13 15:34:04,344 INFO util.GSet: Computing capacity for map INodeMap\r\n",
      "2025-07-13 15:34:04,344 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-13 15:34:04,345 INFO util.GSet: 1.0% max memory 3.9 GB = 40 MB\r\n",
      "2025-07-13 15:34:04,345 INFO util.GSet: capacity      = 2^22 = 4194304 entries\r\n",
      "2025-07-13 15:34:04,347 INFO namenode.FSDirectory: ACLs enabled? true\r\n",
      "2025-07-13 15:34:04,347 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\r\n",
      "2025-07-13 15:34:04,347 INFO namenode.FSDirectory: XAttrs enabled? true\r\n",
      "2025-07-13 15:34:04,347 INFO namenode.NameNode: Caching file names occurring more than 10 times\r\n",
      "2025-07-13 15:34:04,350 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\r\n",
      "2025-07-13 15:34:04,350 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\r\n",
      "2025-07-13 15:34:04,351 INFO snapshot.SnapshotManager: SkipList is disabled\r\n",
      "2025-07-13 15:34:04,355 INFO util.GSet: Computing capacity for map cachedBlocks\r\n",
      "2025-07-13 15:34:04,355 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-13 15:34:04,355 INFO util.GSet: 0.25% max memory 3.9 GB = 10 MB\r\n",
      "2025-07-13 15:34:04,355 INFO util.GSet: capacity      = 2^20 = 1048576 entries\r\n",
      "2025-07-13 15:34:04,361 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\r\n",
      "2025-07-13 15:34:04,361 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\r\n",
      "2025-07-13 15:34:04,361 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\r\n",
      "2025-07-13 15:34:04,364 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\r\n",
      "2025-07-13 15:34:04,364 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\r\n",
      "2025-07-13 15:34:04,365 INFO util.GSet: Computing capacity for map NameNodeRetryCache\r\n",
      "2025-07-13 15:34:04,365 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-13 15:34:04,365 INFO util.GSet: 0.029999999329447746% max memory 3.9 GB = 1.2 MB\r\n",
      "2025-07-13 15:34:04,365 INFO util.GSet: capacity      = 2^17 = 131072 entries\r\n",
      "2025-07-13 15:34:04,380 INFO namenode.FSImage: Allocated new BlockPoolId: BP-974222560-10.1.0.42-1752420844375\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,406 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\r\n",
      "2025-07-13 15:34:04,434 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:04,495 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 396 bytes saved in 0 seconds .\r\n",
      "2025-07-13 15:34:04,505 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\r\n",
      "2025-07-13 15:34:04,510 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\r\n",
      "2025-07-13 15:34:04,532 INFO namenode.FSNamesystem: Stopping services started for active state\r\n",
      "2025-07-13 15:34:04,532 INFO namenode.FSNamesystem: Stopping services started for standby state\r\n",
      "2025-07-13 15:34:04,535 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\r\n",
      "2025-07-13 15:34:04,535 INFO namenode.NameNode: SHUTDOWN_MSG: \r\n",
      "/************************************************************\r\n",
      "SHUTDOWN_MSG: Shutting down NameNode at fv-az1277-496/10.1.0.42\r\n",
      "************************************************************/\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs namenode -format -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMrEiLB_VAeR"
   },
   "source": [
    "## Start cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:04.675749Z",
     "iopub.status.busy": "2025-07-13T15:34:04.675530Z",
     "iopub.status.idle": "2025-07-13T15:34:14.614866Z",
     "shell.execute_reply": "2025-07-13T15:34:14.614027Z"
    },
    "id": "FXHowFfFEwAF",
    "outputId": "deb79696-2f24-410a-90e1-e09111c68b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting datanodes\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting secondary namenodes [fv-az1277-496]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fv-az1277-496: Warning: Permanently added 'fv-az1277-496' (ED25519) to the list of known hosts.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:14.617435Z",
     "iopub.status.busy": "2025-07-13T15:34:14.617232Z",
     "iopub.status.idle": "2025-07-13T15:34:16.043165Z",
     "shell.execute_reply": "2025-07-13T15:34:16.042456Z"
    },
    "id": "pOHsiWv9or7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namenode is not in safe mode.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check if HDFS is in safe mode\n",
    "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
    "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
    "  hdfs dfsadmin -safemode leave\n",
    "else\n",
    "  echo \"Namenode is not in safe mode.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKRRbwDFv3ZQ"
   },
   "source": [
    "# Run some simple HDFS commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:16.045656Z",
     "iopub.status.busy": "2025-07-13T15:34:16.045234Z",
     "iopub.status.idle": "2025-07-13T15:34:23.742595Z",
     "shell.execute_reply": "2025-07-13T15:34:23.741752Z"
    },
    "id": "73wuvOJTxX4O",
    "outputId": "25357ddf-891c-4639-8cc2-a6ce862b96dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup         20 2025-07-13 15:34 my_dir/mnist_test.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# create directory \"my_dir\" in HDFS home\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
    "hdfs dfs -mkdir my_dir\n",
    "\n",
    "# if sampls_data does not exist, create it (so that the notebook can run also outside of Colab)\n",
    "mkdir -p sample_data\n",
    "touch sample_data/mnist_test.csv\n",
    "\n",
    "# Check if the file is empty and fill it if needed\n",
    "if [ ! -s sample_data/mnist_test.csv ]; then\n",
    "  echo -e \"0 1 2 3 4\\n5 6 7 8 9\" > sample_data/mnist_test.csv\n",
    "fi\n",
    "\n",
    "\n",
    "# upload file mnist_test.csv to my_dir\n",
    "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
    "\n",
    "# show contents of directory my_dir\n",
    "hdfs dfs -ls -h my_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3KBe4R65bl1"
   },
   "source": [
    "# Run some simple MapReduce jobs\n",
    "\n",
    "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
    "\n",
    "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVJA-3jSATGV"
   },
   "source": [
    "## Simplest MapReduce job\n",
    "\n",
    "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
    "\n",
    "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
    "\n",
    "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ICcKO2jcHl"
   },
   "source": [
    "Now run the MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:23.745670Z",
     "iopub.status.busy": "2025-07-13T15:34:23.745157Z",
     "iopub.status.idle": "2025-07-13T15:34:28.372425Z",
     "shell.execute_reply": "2025-07-13T15:34:28.371473Z"
    },
    "id": "VDuQYWGi5b7J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_simplest': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "namenode is running as process 3461.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,243 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,320 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,320 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,331 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,545 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,553 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,912 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local932099876_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:26,912 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,009 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,010 INFO mapreduce.Job: Running job: job_local932099876_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,010 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,011 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,016 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,016 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,046 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,047 INFO mapred.LocalJobRunner: Starting task: attempt_local932099876_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,065 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,065 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,080 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,086 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,107 INFO mapred.MapTask: numReduceTasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,126 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,126 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,126 INFO mapred.MapTask: soft limit at 83886080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,126 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,126 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,128 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,130 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,134 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,136 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,136 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,136 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,136 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,137 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,138 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,138 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,138 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,138 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,139 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,139 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,213 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,213 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,214 INFO streaming.PipeMapRed: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,214 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,216 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,216 INFO mapred.MapTask: Starting flush of map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,216 INFO mapred.MapTask: Spilling map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,216 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,216 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,221 INFO mapred.MapTask: Finished spill 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,229 INFO mapred.Task: Task:attempt_local932099876_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,232 INFO mapred.LocalJobRunner: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,232 INFO mapred.Task: Task 'attempt_local932099876_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,237 INFO mapred.Task: Final Counters for attempt_local932099876_0001_m_000000_0: Counters: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=141934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=856567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=157286400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,237 INFO mapred.LocalJobRunner: Finishing task: attempt_local932099876_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,237 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,240 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,240 INFO mapred.LocalJobRunner: Starting task: attempt_local932099876_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,246 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,246 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,246 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,248 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7ebd293c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,249 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,265 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2936012800, maxSingleShuffleLimit=734003200, mergeThreshold=1937768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,270 INFO reduce.EventFetcher: attempt_local932099876_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,300 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local932099876_0001_m_000000_0 decomp: 28 len: 32 to MEMORY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,303 INFO reduce.InMemoryMapOutput: Read 28 bytes from map-output for attempt_local932099876_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,305 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,306 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,307 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,307 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,311 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,311 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,313 INFO reduce.MergeManagerImpl: Merged 1 segments, 28 bytes to disk to satisfy reduce memory limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,314 INFO reduce.MergeManagerImpl: Merging 1 files, 32 bytes from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,314 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,314 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,315 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,316 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,317 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,320 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,321 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,352 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,355 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,355 INFO streaming.PipeMapRed: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,356 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,395 INFO mapred.Task: Task:attempt_local932099876_0001_r_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,397 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,398 INFO mapred.Task: Task attempt_local932099876_0001_r_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,411 INFO output.FileOutputCommitter: Saved output of task 'attempt_local932099876_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,412 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,412 INFO mapred.Task: Task 'attempt_local932099876_0001_r_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,413 INFO mapred.Task: Final Counters for attempt_local932099876_0001_r_000000_0: Counters: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=142030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=856599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=157286400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,413 INFO mapred.LocalJobRunner: Finishing task: attempt_local932099876_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:27,417 INFO mapred.LocalJobRunner: reduce task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:28,012 INFO mapreduce.Job: Job job_local932099876_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:28,013 INFO mapreduce.Job:  map 100% reduce 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:28,015 INFO mapreduce.Job: Job job_local932099876_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:28,021 INFO mapreduce.Job: Counters: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=283964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=1713166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=314572800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:28,021 INFO streaming.StreamJob: Output directory: output_simplest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
    "mapred streaming \\\n",
    "  -input my_dir \\\n",
    "  -output output_simplest \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiZ6FH2gFfE5"
   },
   "source": [
    "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHPEoIIWFubx"
   },
   "source": [
    "Check the output of the MapReduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:28.375549Z",
     "iopub.status.busy": "2025-07-13T15:34:28.375043Z",
     "iopub.status.idle": "2025-07-13T15:34:29.939613Z",
     "shell.execute_reply": "2025-07-13T15:34:29.938759Z"
    },
    "id": "rB7FXYTbwNzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      10      22\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output_simplest/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDObCPW2F39S"
   },
   "source": [
    "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbosNo0TD3oH"
   },
   "source": [
    "## Another MapReduce example: filter a log file\n",
    "\n",
    "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
    "\n",
    "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
    "\n",
    "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVdUuulwGzq5"
   },
   "source": [
    "Download the logfile `Linux_2k.log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:29.942393Z",
     "iopub.status.busy": "2025-07-13T15:34:29.942172Z",
     "iopub.status.idle": "2025-07-13T15:34:30.156929Z",
     "shell.execute_reply": "2025-07-13T15:34:30.156129Z"
    },
    "id": "yJIm4SPZFPxy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-13 15:34:29--  https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 216485 (211K) [text/plain]\r\n",
      "Saving to: â€˜Linux_2k.logâ€™\r\n",
      "\r\n",
      "\r",
      "Linux_2k.log          0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Linux_2k.log        100%[===================>] 211.41K  --.-KB/s    in 0.004s  \r\n",
      "\r\n",
      "2025-07-13 15:34:30 (49.5 MB/s) - â€˜Linux_2k.logâ€™ saved [216485/216485]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:30.159614Z",
     "iopub.status.busy": "2025-07-13T15:34:30.159202Z",
     "iopub.status.idle": "2025-07-13T15:34:34.522038Z",
     "shell.execute_reply": "2025-07-13T15:34:34.521174Z"
    },
    "id": "M1WgyQE3MYWI"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir input || true\n",
    "hdfs dfs -put Linux_2k.log input/ || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILUOCdzEH3Gm"
   },
   "source": [
    "Define the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:34.524654Z",
     "iopub.status.busy": "2025-07-13T15:34:34.524436Z",
     "iopub.status.idle": "2025-07-13T15:34:34.529217Z",
     "shell.execute_reply": "2025-07-13T15:34:34.528628Z"
    },
    "id": "4-rraIUdfdj0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip()\n",
    "    fields = line.split()\n",
    "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
    "      print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8AxdFFPIuDo"
   },
   "source": [
    "Test the script (after setting the correct permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:34.531290Z",
     "iopub.status.busy": "2025-07-13T15:34:34.531092Z",
     "iopub.status.idle": "2025-07-13T15:34:34.641708Z",
     "shell.execute_reply": "2025-07-13T15:34:34.640971Z"
    },
    "id": "QwOk_y7egbGM"
   },
   "outputs": [],
   "source": [
    "!chmod 700 mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhv95VzfIAnz"
   },
   "source": [
    "Look at the first 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:34.644416Z",
     "iopub.status.busy": "2025-07-13T15:34:34.643869Z",
     "iopub.status.idle": "2025-07-13T15:34:34.756196Z",
     "shell.execute_reply": "2025-07-13T15:34:34.755434Z"
    },
    "id": "9qf1dFqIKgoJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r",
      "\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r",
      "\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 Linux_2k.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ09Y1AqR6Fy"
   },
   "source": [
    "Test the mapper in the shell (not using MapReduce):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:34.758548Z",
     "iopub.status.busy": "2025-07-13T15:34:34.758339Z",
     "iopub.status.idle": "2025-07-13T15:34:34.888280Z",
     "shell.execute_reply": "2025-07-13T15:34:34.887589Z"
    },
    "id": "TJ0kDRsigCC2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\r\n",
      "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\r\n",
      "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\r\n",
      "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\r\n",
      "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\r\n",
      "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\r\n",
      "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 Linux_2k.log| ./mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXbgh5g7OraF"
   },
   "source": [
    "Now run the MapReduce job on the pseudo-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:34.890786Z",
     "iopub.status.busy": "2025-07-13T15:34:34.890354Z",
     "iopub.status.idle": "2025-07-13T15:34:39.324632Z",
     "shell.execute_reply": "2025-07-13T15:34:39.323814Z"
    },
    "id": "G7SEzMC2OqWW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_filter': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:36,675 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [mapper.py] [] /tmp/streamjob4722781552366300910.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,314 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,394 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,394 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,405 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,607 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,616 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,758 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local623326603_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,758 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,904 INFO mapred.LocalDistributedCacheManager: Localized file:/home/runner/work/big_data/big_data/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local623326603_0001_be014539-6c75-43d6-a513-5be3f3b851b3/mapper.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,963 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,964 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,965 INFO mapreduce.Job: Running job: job_local623326603_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,965 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,971 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:37,971 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,015 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,018 INFO mapred.LocalJobRunner: Starting task: attempt_local623326603_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,043 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,043 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,060 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,074 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,095 INFO mapred.MapTask: numReduceTasks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,133 INFO streaming.PipeMapRed: PipeMapRed exec [/home/runner/work/big_data/big_data/./mapper.py]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,139 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,140 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,140 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,140 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,140 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,141 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,142 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,142 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,142 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,142 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,143 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,143 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,191 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,191 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,193 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,201 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,204 INFO streaming.PipeMapRed: Records R/W=1294/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,214 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,218 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,220 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,254 INFO mapred.Task: Task:attempt_local623326603_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,257 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,257 INFO mapred.Task: Task attempt_local623326603_0001_m_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,266 INFO output.FileOutputCommitter: Saved output of task 'attempt_local623326603_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,267 INFO mapred.LocalJobRunner: Records R/W=1294/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,268 INFO mapred.Task: Task 'attempt_local623326603_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,272 INFO mapred.Task: Final Counters for attempt_local623326603_0001_m_000000_0: Counters: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=715633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=111149056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,272 INFO mapred.LocalJobRunner: Finishing task: attempt_local623326603_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,272 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,969 INFO mapreduce.Job: Job job_local623326603_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,970 INFO mapreduce.Job:  map 100% reduce 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,972 INFO mapreduce.Job: Job job_local623326603_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,975 INFO mapreduce.Job: Counters: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=715633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=111149056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:38,975 INFO streaming.StreamJob: Output directory: output_filter\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r output_filter\n",
    "\n",
    "mapred streaming \\\n",
    "  -file mapper.py \\\n",
    "  -input input \\\n",
    "  -output output_filter \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer NONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuZJ2ACzSTJi"
   },
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:39.327520Z",
     "iopub.status.busy": "2025-07-13T15:34:39.327093Z",
     "iopub.status.idle": "2025-07-13T15:34:40.806836Z",
     "shell.execute_reply": "2025-07-13T15:34:40.805999Z"
    },
    "id": "RhLA5HZEhfmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2025-07-13 15:34 output_filter/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup      85436 2025-07-13 15:34 output_filter/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:40.809845Z",
     "iopub.status.busy": "2025-07-13T15:34:40.809284Z",
     "iopub.status.idle": "2025-07-13T15:34:42.415697Z",
     "shell.execute_reply": "2025-07-13T15:34:42.414898Z"
    },
    "id": "Ffi4RvXnPH14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output_filter/part-00000 |head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sam22f-YT1xR"
   },
   "source": [
    "## Aggregate data with MapReduce\n",
    "\n",
    "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:42.418365Z",
     "iopub.status.busy": "2025-07-13T15:34:42.418140Z",
     "iopub.status.idle": "2025-07-13T15:34:42.422933Z",
     "shell.execute_reply": "2025-07-13T15:34:42.422380Z"
    },
    "id": "fMKEqUF1T-v9",
    "outputId": "b3f905e8-800c-4441-ef2e-1d044c1d2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing myAggregatorForKeyCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile myAggregatorForKeyCount.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def generateLongCountToken(id):\n",
    "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            line = line[:-1]\n",
    "            fields = line.split()\n",
    "            s = fields[4].split('[')[0]\n",
    "            print(generateLongCountToken(s))\n",
    "            line = sys.stdin.readline()\n",
    "    except \"end of file\":\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b2S9K8FWDMz"
   },
   "source": [
    "Set permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:42.425183Z",
     "iopub.status.busy": "2025-07-13T15:34:42.424793Z",
     "iopub.status.idle": "2025-07-13T15:34:42.535499Z",
     "shell.execute_reply": "2025-07-13T15:34:42.534750Z"
    },
    "id": "35DP8K2_WDYO"
   },
   "outputs": [],
   "source": [
    "!chmod 700 myAggregatorForKeyCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9M8lgxMVRYz"
   },
   "source": [
    "Test the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:42.537792Z",
     "iopub.status.busy": "2025-07-13T15:34:42.537373Z",
     "iopub.status.idle": "2025-07-13T15:34:42.666162Z",
     "shell.execute_reply": "2025-07-13T15:34:42.665451Z"
    },
    "id": "k-R7VNoTVRjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:logrotate:\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOEpMFvsVRtM"
   },
   "source": [
    "Run the MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:42.668409Z",
     "iopub.status.busy": "2025-07-13T15:34:42.668200Z",
     "iopub.status.idle": "2025-07-13T15:34:46.998498Z",
     "shell.execute_reply": "2025-07-13T15:34:46.997823Z"
    },
    "id": "XwxHJ7yyVR34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_aggregate': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:44,479 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [myAggregatorForKeyCount.py] [] /tmp/streamjob6364205998257741820.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:44,995 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,070 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,070 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,080 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,287 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,302 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,450 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1880963874_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,450 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,594 INFO mapred.LocalDistributedCacheManager: Localized file:/home/runner/work/big_data/big_data/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local1880963874_0001_962a766c-34f3-47b2-88ac-39aad90e394f/myAggregatorForKeyCount.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,638 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,639 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,639 INFO mapreduce.Job: Running job: job_local1880963874_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,640 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,650 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,650 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,691 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,693 INFO mapred.LocalJobRunner: Starting task: attempt_local1880963874_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,710 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,710 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,723 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,729 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,745 INFO mapred.MapTask: numReduceTasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,766 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,766 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,766 INFO mapred.MapTask: soft limit at 83886080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,766 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,766 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,769 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,778 INFO streaming.PipeMapRed: PipeMapRed exec [/home/runner/work/big_data/big_data/./myAggregatorForKeyCount.py]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,783 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,784 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,785 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,785 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,785 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,786 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,786 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,787 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,787 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,787 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,787 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,788 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,853 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,853 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,855 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,865 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,868 INFO streaming.PipeMapRed: Records R/W=1201/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,884 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,890 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,893 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,893 INFO mapred.MapTask: Starting flush of map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,893 INFO mapred.MapTask: Spilling map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,893 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,893 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,917 INFO mapred.MapTask: Finished spill 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,925 INFO mapred.Task: Task:attempt_local1880963874_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,929 INFO mapred.LocalJobRunner: Records R/W=1201/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,929 INFO mapred.Task: Task 'attempt_local1880963874_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,934 INFO mapred.Task: Final Counters for attempt_local1880963874_0001_m_000000_0: Counters: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=720546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=48923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=199229440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,934 INFO mapred.LocalJobRunner: Finishing task: attempt_local1880963874_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,934 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,936 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,937 INFO mapred.LocalJobRunner: Starting task: attempt_local1880963874_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,942 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,942 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,943 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,945 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d14c165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,946 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,958 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2936012800, maxSingleShuffleLimit=734003200, mergeThreshold=1937768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,959 INFO reduce.EventFetcher: attempt_local1880963874_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,977 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1880963874_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,979 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local1880963874_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,980 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,981 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,982 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,982 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,986 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,986 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,987 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,988 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,988 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,988 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,989 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:45,989 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,038 INFO mapred.Task: Task:attempt_local1880963874_0001_r_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,040 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,040 INFO mapred.Task: Task attempt_local1880963874_0001_r_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,051 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1880963874_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,051 INFO mapred.LocalJobRunner: reduce > reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,051 INFO mapred.Task: Task 'attempt_local1880963874_0001_r_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,055 INFO mapred.Task: Final Counters for attempt_local1880963874_0001_r_000000_0: Counters: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=2655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=721328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=199229440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,055 INFO mapred.LocalJobRunner: Finishing task: attempt_local1880963874_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,056 INFO mapred.LocalJobRunner: reduce task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,642 INFO mapreduce.Job: Job job_local1880963874_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,643 INFO mapreduce.Job:  map 100% reduce 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,644 INFO mapreduce.Job: Job job_local1880963874_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,649 INFO mapreduce.Job: Counters: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=3714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=1441874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=432970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=48923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=398458880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 15:34:46,649 INFO streaming.StreamJob: Output directory: output_aggregate\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "chmod +x myAggregatorForKeyCount.py\n",
    "\n",
    "hdfs dfs -rm -r output_aggregate\n",
    "\n",
    "mapred streaming \\\n",
    "  -input input \\\n",
    "  -output output_aggregate \\\n",
    "  -mapper myAggregatorForKeyCount.py \\\n",
    "  -reducer aggregate \\\n",
    "  -file myAggregatorForKeyCount.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkuYUkh5W0Je"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:47.000991Z",
     "iopub.status.busy": "2025-07-13T15:34:47.000755Z",
     "iopub.status.idle": "2025-07-13T15:34:49.894938Z",
     "shell.execute_reply": "2025-07-13T15:34:49.894260Z"
    },
    "id": "ET3KCfX1UC2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2025-07-13 15:34 output_aggregate/_SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup        326 2025-07-13 15:34 output_aggregate/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bluetooth:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cups:\t12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftpd\t916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm(pam_unix)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm-binary\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpm\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcid\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irqbalance:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel:\t76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klogind\t46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login(pam_unix)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrotate:\t43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named\t16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfslock:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portmap:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpc.statd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpcidmapd:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snmpd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshd(pam_unix)\t677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su(pam_unix)\t172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sysctl:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslog:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslogd\t7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udev\t8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xinetd\t2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls output_aggregate\n",
    "hdfs dfs -cat output_aggregate/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj9qz8wSa1w0"
   },
   "source": [
    "Pretty-print table of aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:49.897605Z",
     "iopub.status.busy": "2025-07-13T15:34:49.897193Z",
     "iopub.status.idle": "2025-07-13T15:34:51.366008Z",
     "shell.execute_reply": "2025-07-13T15:34:51.365162Z"
    },
    "id": "Y8IYl4hAZhZm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftpd                 916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshd(pam_unix)       677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su(pam_unix)         172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel:              76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klogind              46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrotate:           43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named                16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cups:                12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udev                 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslogd              7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bluetooth:           2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm(pam_unix)        2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpm                  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login(pam_unix)      2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network:             2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslog:              2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xinetd               2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--                   1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm-binary           1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcid                 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irqbalance:          1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfslock:             1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portmap:             1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:              1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc:                  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpc.statd            1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpcidmapd:           1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpd                 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snmpd                1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sysctl:              1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
    "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
    "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF6-Z5RotAcO"
   },
   "source": [
    "# Stop cluster\n",
    "\n",
    "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:51.368546Z",
     "iopub.status.busy": "2025-07-13T15:34:51.368116Z",
     "iopub.status.idle": "2025-07-13T15:34:58.217906Z",
     "shell.execute_reply": "2025-07-13T15:34:58.217195Z"
    },
    "id": "IoIYG5NlsIMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping datanodes\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping secondary namenodes [fv-az1277-496]\r\n"
     ]
    }
   ],
   "source": [
    "!./hadoop-3.4.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGj96_e2ccZw"
   },
   "source": [
    "Stop the `sshd` daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T15:34:58.221018Z",
     "iopub.status.busy": "2025-07-13T15:34:58.220560Z",
     "iopub.status.idle": "2025-07-13T15:34:58.358811Z",
     "shell.execute_reply": "2025-07-13T15:34:58.357942Z"
    },
    "id": "FUvKMpy6chQ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping ssh (via systemctl): ssh.service\u001b[0;1;38;5;185mStopping 'ssh.service', but its triggering units are still active:\u001b[0m\r",
      "\r\n",
      "\u001b[0;1;38;5;185mssh.socket\u001b[0m\r",
      "\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "!/etc/init.d/ssh stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5N7tb0HSbZB"
   },
   "source": [
    "# Concluding remarks\n",
    "\n",
    "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
    "\n",
    "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
    "\n",
    "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
    "\n",
    "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
    "\n",
    "If you found this notebook helpful, consider exploring:\n",
    " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
    " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
    " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYyg1O7ysUs6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qFfOrktMPq8M",
    "IF6-Z5RotAcO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
