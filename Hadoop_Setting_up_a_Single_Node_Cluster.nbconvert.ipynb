{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Hadoop_Setting_up_a_Single_Node_Cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEF3qldGPj3T"
   },
   "source": [
    "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
    "\n",
    "# HDFS and MapReduce on a single-node Hadoop cluster\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
    "\n",
    "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
    "\n",
    "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
    "\n",
    "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
    "\n",
    "Finally, we'll proceed to shut down the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "hGm3LhVEWXr9"
   },
   "source": [
    "**TABLE OF CONTENTS**\n",
    "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
    "\n",
    " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
    "\n",
    " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
    "\n",
    "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
    "\n",
    " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
    "\n",
    " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
    "\n",
    " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
    "\n",
    " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
    "\n",
    "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
    "\n",
    "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
    "\n",
    "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
    "\n",
    "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
    "\n",
    "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
    "\n",
    "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
    "\n",
    "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
    "\n",
    "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
    "\n",
    "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
    "\n",
    "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
    "\n",
    "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
    "\n",
    "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
    "\n",
    "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUuQjW2oNMcJ"
   },
   "source": [
    "# Prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFfOrktMPq8M"
   },
   "source": [
    "## Check the available Java version\n",
    " Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only). See: https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuWqBiV89ryq"
   },
   "source": [
    "Check if Java version is one of `8`, `11`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:55:27.058421Z",
     "iopub.status.busy": "2025-07-12T14:55:27.057934Z",
     "iopub.status.idle": "2025-07-12T14:55:28.153186Z",
     "shell.execute_reply": "2025-07-12T14:55:28.152492Z"
    },
    "id": "C7X0EZaMPrsD",
    "outputId": "2bf18811-0898-4d12-ac7c-20558aec8437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.27\" 2025-04-15\r\n",
      "OpenJDK Runtime Environment Temurin-11.0.27+6 (build 11.0.27+6)\r\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.27+6 (build 11.0.27+6, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:55:28.155885Z",
     "iopub.status.busy": "2025-07-12T14:55:28.155516Z",
     "iopub.status.idle": "2025-07-12T14:55:28.233389Z",
     "shell.execute_reply": "2025-07-12T14:55:28.232647Z"
    },
    "id": "lABuOV124G4x",
    "outputId": "05c1947a-8370-4813-89b7-76cb6984d4cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version is one of 8, 11 ✓\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
    "if [[ $JAVA_MAJOR_VERSION -eq 8 || $JAVA_MAJOR_VERSION -eq 11 ]]\n",
    " then\n",
    " echo \"Java version is one of 8, 11 ✓\"\n",
    " fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWROofISgKKW"
   },
   "source": [
    "Find the variable for the environment variable `JAVA_HOME`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH4AGbkLP3iK"
   },
   "source": [
    "Find the path for the environment variable `JAVA_HOME`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:55:28.235906Z",
     "iopub.status.busy": "2025-07-12T14:55:28.235481Z",
     "iopub.status.idle": "2025-07-12T14:55:28.348906Z",
     "shell.execute_reply": "2025-07-12T14:55:28.348185Z"
    },
    "id": "mCmk5GOqv0Y-",
    "outputId": "4efba03e-3fc7-4214-c37b-dd238a840f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/temurin-11-jdk-amd64/bin/java\r\n"
     ]
    }
   ],
   "source": [
    "!readlink -f $(which java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGHKH3Vu9Nwl"
   },
   "source": [
    "Extract JAVA_HOME from the Java path by removing the `bin/java` part in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:55:28.351544Z",
     "iopub.status.busy": "2025-07-12T14:55:28.351111Z",
     "iopub.status.idle": "2025-07-12T14:55:28.374107Z",
     "shell.execute_reply": "2025-07-12T14:55:28.373481Z"
    },
    "id": "Dd7en2Cv68ce",
    "outputId": "40430612-b7e6-4c82-99ca-3a348ba9d99a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/temurin-11-jdk-amd64\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
    "echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE7kSYSXQYLf"
   },
   "source": [
    "## Download core Hadoop\n",
    "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
    "\n",
    "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:55:28.376491Z",
     "iopub.status.busy": "2025-07-12T14:55:28.376060Z",
     "iopub.status.idle": "2025-07-12T14:56:02.557204Z",
     "shell.execute_reply": "2025-07-12T14:56:02.556541Z"
    },
    "id": "54LqS5Rkgyli",
    "outputId": "6ab71854-2018-4711-fda9-2e268cf6024c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-12 14:55:28--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\r\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 965537117 (921M) [application/x-gzip]\r\n",
      "Saving to: ‘hadoop-3.4.0.tar.gz’\r\n",
      "\r\n",
      "\r",
      "hadoop-3.4.0.tar.gz   0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz   5%[>                   ]  55.16M   276MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  13%[=>                  ] 121.49M   304MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  20%[===>                ] 185.04M   308MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  26%[====>               ] 243.07M   304MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  33%[=====>              ] 307.59M   308MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  40%[=======>            ] 373.72M   311MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  47%[========>           ] 434.98M   311MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  54%[=========>          ] 500.00M   312MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  60%[===========>        ] 554.63M   308MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  66%[============>       ] 614.97M   307MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  72%[=============>      ] 670.80M   305MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  79%[==============>     ] 728.42M   303MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  85%[================>   ] 791.73M   304MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  92%[=================>  ] 848.50M   303MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz  98%[==================> ] 908.92M   303MB/s    eta 0s     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "hadoop-3.4.0.tar.gz 100%[===================>] 920.81M   302MB/s    in 3.0s    \r\n",
      "\r\n",
      "2025-07-12 14:56:02 (302 MB/s) - ‘hadoop-3.4.0.tar.gz’ saved [965537117/965537117]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um2CARkgg22j"
   },
   "source": [
    "Uncompress archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:02.559800Z",
     "iopub.status.busy": "2025-07-12T14:56:02.559394Z",
     "iopub.status.idle": "2025-07-12T14:56:11.920348Z",
     "shell.execute_reply": "2025-07-12T14:56:11.919473Z"
    },
    "id": "C17WYI0mQRE8"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \"hadoop-3.4.0\" ]; then\n",
    "  tar xzf hadoop-3.4.0.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGI4TNXPamMr"
   },
   "source": [
    "### Verify the downloaded file\n",
    "\n",
    "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATofMJRXhJ4K"
   },
   "source": [
    "Download sha512 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:11.923027Z",
     "iopub.status.busy": "2025-07-12T14:56:11.922764Z",
     "iopub.status.idle": "2025-07-12T14:56:12.373644Z",
     "shell.execute_reply": "2025-07-12T14:56:12.372855Z"
    },
    "id": "NhTinHLqCrFQ",
    "outputId": "7d7e0b60-0ed0-4239-c91b-a2b6daa25315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-12 14:56:11--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512\r\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\r\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\r\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 160 [text/plain]\r\n",
      "Saving to: ‘hadoop-3.4.0.tar.gz.sha512’\r\n",
      "\r\n",
      "\r",
      "          hadoop-3.   0%[                    ]       0  --.-KB/s               \r",
      "hadoop-3.4.0.tar.gz 100%[===================>]     160  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2025-07-12 14:56:12 (4.89 MB/s) - ‘hadoop-3.4.0.tar.gz.sha512’ saved [160/160]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eL8FxjalhFAn"
   },
   "source": [
    "Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:12.375959Z",
     "iopub.status.busy": "2025-07-12T14:56:12.375748Z",
     "iopub.status.idle": "2025-07-12T14:56:13.755755Z",
     "shell.execute_reply": "2025-07-12T14:56:13.754989Z"
    },
    "id": "zL302M1OhFMH",
    "outputId": "a070f00d-3860-4a51-e33c-a86c20688ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "A=$(sha512sum hadoop-3.4.0.tar.gz | cut - -d' ' -f1)\n",
    "B=$(cut hadoop-3.4.0.tar.gz.sha512 -d' ' -f4)\n",
    "printf \"%s\\n%s\\n\" $A $B\n",
    "[[ $A == $B ]] && echo \"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlgP1ytnRtUK"
   },
   "source": [
    "## Configure `PATH`\n",
    "\n",
    "Add the Hadoop folder to the `PATH` environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:13.758141Z",
     "iopub.status.busy": "2025-07-12T14:56:13.757731Z",
     "iopub.status.idle": "2025-07-12T14:56:13.868229Z",
     "shell.execute_reply": "2025-07-12T14:56:13.867488Z"
    },
    "id": "49xx-zSxIdxa",
    "outputId": "9403498c-8e5e-4401-df95-44de81e11c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:13.870568Z",
     "iopub.status.busy": "2025-07-12T14:56:13.870357Z",
     "iopub.status.idle": "2025-07-12T14:56:13.874089Z",
     "shell.execute_reply": "2025-07-12T14:56:13.873581Z"
    },
    "id": "6V03we10Igek"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.0')\n",
    "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
    "#os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:13.875989Z",
     "iopub.status.busy": "2025-07-12T14:56:13.875792Z",
     "iopub.status.idle": "2025-07-12T14:56:13.879405Z",
     "shell.execute_reply": "2025-07-12T14:56:13.878824Z"
    },
    "id": "Aif21X1ONvwH",
    "outputId": "729bbd33-1098-431e-f31b-a0dd06b35b9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'LANG': 'C.UTF-8', 'PATH': '/home/runner/work/big_data/big_data/hadoop-3.4.0/bin:/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'MAIL': '/var/mail/root', 'LOGNAME': 'root', 'USER': 'root', 'HOME': '/root', 'SHELL': '/bin/bash', 'TERM': 'xterm-color', 'SUDO_COMMAND': '/usr/bin/env PATH=/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin jupyter nbconvert --log-level=ERROR --to notebook --execute Hadoop_Setting_up_a_Single_Node_Cluster.ipynb', 'SUDO_USER': 'runner', 'SUDO_UID': '1001', 'SUDO_GID': '118', 'ImageVersion': '20250710.1.0', 'ImageOS': 'ubuntu24', 'ACCEPT_EULA': 'Y', 'XDG_CONFIG_HOME': '/home/runner/.config', 'AGENT_TOOLSDIRECTORY': '/opt/hostedtoolcache', 'RUNNER_TOOL_CACHE': '/opt/hostedtoolcache', 'ACTIONS_RUNNER_ACTION_ARCHIVE_CACHE': '/opt/actionarchivecache', 'AZURE_EXTENSION_DIR': '/opt/az/azcliextensions', 'SWIFT_PATH': '/usr/share/swift/usr/bin', 'DOTNET_SKIP_FIRST_TIME_EXPERIENCE': '1', 'DOTNET_NOLOGO': '1', 'DOTNET_MULTILEVEL_LOOKUP': '0', 'EDGEWEBDRIVER': '/usr/local/share/edge_driver', 'GECKOWEBDRIVER': '/usr/local/share/gecko_driver', 'CHROME_BIN': '/usr/bin/google-chrome', 'CHROMEWEBDRIVER': '/usr/local/share/chromedriver-linux64', 'BOOTSTRAP_HASKELL_NONINTERACTIVE': '1', 'GHCUP_INSTALL_BASE_PREFIX': '/usr/local', 'JAVA_HOME_8_X64': '/usr/lib/jvm/temurin-8-jdk-amd64', 'JAVA_HOME_11_X64': '/usr/lib/jvm/temurin-11-jdk-amd64', 'JAVA_HOME': '/usr/lib/jvm/temurin-17-jdk-amd64', 'JAVA_HOME_17_X64': '/usr/lib/jvm/temurin-17-jdk-amd64', 'JAVA_HOME_21_X64': '/usr/lib/jvm/temurin-21-jdk-amd64', 'ANT_HOME': '/usr/share/ant', 'GRADLE_HOME': '/usr/share/gradle-8.14.3', 'CONDA': '/usr/share/miniconda', 'NVM_DIR': '/home/runner/.nvm', 'SELENIUM_JAR_PATH': '/usr/share/java/selenium-server.jar', 'VCPKG_INSTALLATION_ROOT': '/usr/local/share/vcpkg', 'DEBIAN_FRONTEND': 'noninteractive', 'ANDROID_SDK_ROOT': '/usr/local/lib/android/sdk', 'ANDROID_HOME': '/usr/local/lib/android/sdk', 'ANDROID_NDK': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_HOME': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_ROOT': '/usr/local/lib/android/sdk/ndk/27.2.12479018', 'ANDROID_NDK_LATEST_HOME': '/usr/local/lib/android/sdk/ndk/28.2.13676358', 'PIPX_BIN_DIR': '/opt/pipx_bin', 'PIPX_HOME': '/opt/pipx', 'GOROOT_1_22_X64': '/opt/hostedtoolcache/go/1.22.12/x64', 'GOROOT_1_23_X64': '/opt/hostedtoolcache/go/1.23.10/x64', 'GOROOT_1_24_X64': '/opt/hostedtoolcache/go/1.24.4/x64', 'HOMEBREW_NO_AUTO_UPDATE': '1', 'HOMEBREW_CLEANUP_PERIODIC_FULL_DAYS': '3650', 'RUNNER_USER': 'runner', 'DEPLOYMENT_BASEPATH': '/opt/runner', 'PERFLOG_LOCATION_SETTING': 'RUNNER_PERFLOG', 'POWERSHELL_DISTRIBUTION_CHANNEL': 'GitHub-Actions-ubuntu24', 'XDG_RUNTIME_DIR': '/run/user/1001', 'JPY_PARENT_PID': '2528', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'HADOOP_HOME': '/home/runner/work/big_data/big_data/hadoop-3.4.0'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:13.881339Z",
     "iopub.status.busy": "2025-07-12T14:56:13.881120Z",
     "iopub.status.idle": "2025-07-12T14:56:13.991028Z",
     "shell.execute_reply": "2025-07-12T14:56:13.990247Z"
    },
    "id": "GcM-idgZQqfV",
    "outputId": "44c3158a-e8e8-4283-cce3-466f9a31a79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/runner/work/big_data/big_data/hadoop-3.4.0/bin:/opt/hostedtoolcache/Java_Temurin-Hotspot_jdk/11.0.27-6/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64/bin:/opt/hostedtoolcache/Python/3.8.18/x64:/snap/bin:/home/runner/.local/bin:/opt/pipx_bin:/home/runner/.cargo/bin:/home/runner/.config/composer/vendor/bin:/usr/local/.ghcup/bin:/home/runner/.dotnet/tools:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLmxLQeJSb4A"
   },
   "source": [
    "## Configure `core-site.xml` and `hdfs-site.xml`\n",
    "\n",
    "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
    "\n",
    "**`etc/hadoop/core-site.xml`**\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "**`etc/hadoop/hdfs-site.xml`**\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:13.993520Z",
     "iopub.status.busy": "2025-07-12T14:56:13.993268Z",
     "iopub.status.idle": "2025-07-12T14:56:14.003141Z",
     "shell.execute_reply": "2025-07-12T14:56:14.002459Z"
    },
    "id": "_n2d2lqXSLU1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"<configuration> \\n\\\n",
    "    <property> \\n\\\n",
    "        <name>fs.defaultFS</name> \\n\\\n",
    "        <value>hdfs://localhost:9000</value> \\n\\\n",
    "    </property> \\n\\\n",
    "</configuration>\" >hadoop-3.4.0/etc/hadoop/core-site.xml\n",
    "\n",
    "echo -e \"<configuration> \\n\\\n",
    "    <property> \\n\\\n",
    "        <name>dfs.replication</name> \\n\\\n",
    "        <value>1</value> \\n\\\n",
    "    </property> \\n\\\n",
    "</configuration>\" >hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mdkNb-Cg9HW"
   },
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:14.005507Z",
     "iopub.status.busy": "2025-07-12T14:56:14.005108Z",
     "iopub.status.idle": "2025-07-12T14:56:14.116289Z",
     "shell.execute_reply": "2025-07-12T14:56:14.115498Z"
    },
    "id": "-ISxE4Gqg_LG",
    "outputId": "a69ca821-a48b-4979-e663-3bdad46c3219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<configuration> \r\n",
      "    <property> \r\n",
      "        <name>dfs.replication</name> \r\n",
      "        <value>1</value> \r\n",
      "    </property> \r\n",
      "</configuration>\r\n"
     ]
    }
   ],
   "source": [
    "cat hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXbSKFyeMqr2"
   },
   "source": [
    "## Set environment variables\n",
    "\n",
    "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.0/sbin`).\n",
    "```\n",
    "export HDFS_NAMENODE_USER=root\n",
    "export HDFS_DATANODE_USER=root\n",
    "export HDFS_SECONDARYNAMENODE_USER=root\n",
    "export YARN_RESOURCEMANAGER_USER=root\n",
    "export YARN_NODEMANAGER_USER=root\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:14.118884Z",
     "iopub.status.busy": "2025-07-12T14:56:14.118663Z",
     "iopub.status.idle": "2025-07-12T14:56:14.131551Z",
     "shell.execute_reply": "2025-07-12T14:56:14.130915Z"
    },
    "id": "2_vn-TGyPe9V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: warning: behavior of -n is non-portable and may change in future; use --update=none instead\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cp -n hadoop-3.4.0/etc/hadoop/hadoop-env.sh hadoop-3.4.0/etc/hadoop/hadoop-env.sh.org\n",
    "cat <<😃 >hadoop-3.4.0/etc/hadoop/hadoop-env.sh\n",
    "export HDFS_NAMENODE_USER=root\n",
    "export HDFS_DATANODE_USER=root\n",
    "export HDFS_SECONDARYNAMENODE_USER=root\n",
    "export YARN_RESOURCEMANAGER_USER=root\n",
    "export YARN_NODEMANAGER_USER=root\n",
    "😃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2-Fdp73cF0V"
   },
   "source": [
    "## Setup localhost access via SSH key\n",
    "\n",
    "We are going to allow passphraseless access to `localhost` with a secure key.\n",
    "\n",
    "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Uxmv3RdUwiF"
   },
   "source": [
    "\n",
    "### Install `openssh` and start server\n",
    "\n",
    "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
    "\n",
    "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:14.134049Z",
     "iopub.status.busy": "2025-07-12T14:56:14.133650Z",
     "iopub.status.idle": "2025-07-12T14:56:15.885528Z",
     "shell.execute_reply": "2025-07-12T14:56:15.884835Z"
    },
    "id": "yOxz683FNuYH",
    "outputId": "06b37eb9-a0ec-49ab-b1f0-22b3688cf080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 file:/etc/apt/apt-mirrors.txt Mirrorlist [144 B]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:2 http://azure.archive.ubuntu.com/ubuntu noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:6 https://packages.microsoft.com/repos/azure-cli noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:7 https://packages.microsoft.com/ubuntu/24.04/prod noble InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:3 http://azure.archive.ubuntu.com/ubuntu noble-updates InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:4 http://azure.archive.ubuntu.com/ubuntu noble-backports InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:5 http://azure.archive.ubuntu.com/ubuntu noble-security InRelease\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dependency tree...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading state information...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openssh-server is already the newest version (1:9.6p1-3ubuntu13.12).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting ssh (via systemctl): ssh.service.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt-get update\n",
    "apt-get -y install openssh-server\n",
    "echo 'StrictHostKeyChecking no' >> /etc/ssh/ssh_config\n",
    "/etc/init.d/ssh restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYKoSlaENuyG"
   },
   "source": [
    "### Generate key\n",
    "Generate an SSH key that does not require a password.\n",
    "\n",
    "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
    "\n",
    "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:15.888072Z",
     "iopub.status.busy": "2025-07-12T14:56:15.887590Z",
     "iopub.status.idle": "2025-07-12T14:56:16.876404Z",
     "shell.execute_reply": "2025-07-12T14:56:16.875636Z"
    },
    "id": "YHOjUaxHSsQD",
    "outputId": "475e9552-4f57-4108-f4a0-c632b52a58dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private rsa key pair.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your identification has been saved in /root/.ssh/id_rsa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your public key has been saved in /root/.ssh/id_rsa.pub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key fingerprint is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA256:wHtRSTgrFaNXVU8mgfCnsIuqslEWuvcVDQv4TbKzSSk root@fv-az1719-822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key's randomart image is:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---[RSA 3072]----+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        o+=+.o+.o|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     o .++...  = |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    o *.=o. . . .|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   . o.@.= o o   |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  . E B.S o .    |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   + o = o .     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  o . o o .      |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  .o . o         |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  .o..o          |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm $HOME/.ssh/id_rsa\n",
    "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "chmod 0600 ~/.ssh/authorized_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwA6rKpScnVi"
   },
   "source": [
    "### Check SSH connection to localhost\n",
    "\n",
    "The following command should output \"hi!\" if the connection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:16.879008Z",
     "iopub.status.busy": "2025-07-12T14:56:16.878608Z",
     "iopub.status.idle": "2025-07-12T14:56:19.451299Z",
     "shell.execute_reply": "2025-07-12T14:56:19.450475Z"
    },
    "id": "hqIRVxcfce0F",
    "outputId": "a6a130c9-1717-4b74-a93a-f945c7fdd911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi 👋\r\n"
     ]
    }
   ],
   "source": [
    "!ssh localhost \"echo hi 👋\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V68C4cDySyek"
   },
   "source": [
    "# Launch a single-node Hadoop cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTDPwnVlSbHS"
   },
   "source": [
    "## Initialize the namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:19.454216Z",
     "iopub.status.busy": "2025-07-12T14:56:19.453962Z",
     "iopub.status.idle": "2025-07-12T14:56:24.018343Z",
     "shell.execute_reply": "2025-07-12T14:56:24.017504Z"
    },
    "id": "Q-aicnKKLVKQ",
    "outputId": "4778340e-d7ea-472b-97ce-631e91e6e850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: /home/runner/work/big_data/big_data/hadoop-3.4.0/logs does not exist. Creating.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:21,683 INFO namenode.NameNode: STARTUP_MSG: \r\n",
      "/************************************************************\r\n",
      "STARTUP_MSG: Starting NameNode\r\n",
      "STARTUP_MSG:   host = fv-az1719-822/10.1.0.58\r\n",
      "STARTUP_MSG:   args = [-format, -nonInteractive]\r\n",
      "STARTUP_MSG:   version = 3.4.0\r\n",
      "STARTUP_MSG:   classpath = /home/runner/work/big_data/big_data/hadoop-3.4.0/etc/hadoop:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-common-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-compress-1.24.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-client-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-codec-1.15.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-core-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsch-0.1.55.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsp-api-2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-client-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/token-provider-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-server-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-core-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/metrics-core-3.2.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/httpcore-4.4.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-math3-3.6.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/httpclient-4.5.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-cli-1.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/failureaccess-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/re2j-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsr305-3.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-config-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-text-1.10.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-logging-1.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/reload4j-1.2.22.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-framework-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-identity-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/gson-2.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-core-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/stax2-api-4.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerb-admin-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/checker-qual-2.5.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-net-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-json-1.20.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-server-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/commons-io-2.14.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jline-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jettison-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/dnsjava-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-registry-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-kms-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/common/hadoop-nfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-framework-5.2.0.ja"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/gson-2.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jline-3.9.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/codemodel-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/asm-tree-9.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.inject-1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jna-5.2.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/fst-2.50.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/asm-commons-9.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/guice-4.2.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/objenesis-2.6.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar:/home/runner/work/big_data/big_data/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar\r\n",
      "STARTUP_MSG:   build = git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760; compiled by 'root' on 2024-03-04T06:35Z\r\n",
      "STARTUP_MSG:   java = 17.0.15\r\n",
      "************************************************************/\r\n",
      "2025-07-12 14:56:21,701 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:21,812 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,233 INFO namenode.NameNode: Formatting using clusterid: CID-9633ae23-f315-49be-af65-a073f3706c03\r\n",
      "2025-07-12 14:56:23,253 INFO namenode.FSEditLog: Edit logging is async:true\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,281 INFO namenode.FSNamesystem: KeyProvider: null\r\n",
      "2025-07-12 14:56:23,282 INFO namenode.FSNamesystem: fsLock is fair: true\r\n",
      "2025-07-12 14:56:23,282 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,413 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\r\n",
      "2025-07-12 14:56:23,413 INFO namenode.FSNamesystem: supergroup             = supergroup\r\n",
      "2025-07-12 14:56:23,413 INFO namenode.FSNamesystem: isPermissionEnabled    = true\r\n",
      "2025-07-12 14:56:23,413 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\r\n",
      "2025-07-12 14:56:23,413 INFO namenode.FSNamesystem: HA Enabled: false\r\n",
      "2025-07-12 14:56:23,460 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,539 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\r\n",
      "2025-07-12 14:56:23,546 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\r\n",
      "2025-07-12 14:56:23,546 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\r\n",
      "2025-07-12 14:56:23,548 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\r\n",
      "2025-07-12 14:56:23,549 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jul 12 14:56:23\r\n",
      "2025-07-12 14:56:23,550 INFO util.GSet: Computing capacity for map BlocksMap\r\n",
      "2025-07-12 14:56:23,550 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-12 14:56:23,551 INFO util.GSet: 2.0% max memory 3.9 GB = 80 MB\r\n",
      "2025-07-12 14:56:23,551 INFO util.GSet: capacity      = 2^23 = 8388608 entries\r\n",
      "2025-07-12 14:56:23,560 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\r\n",
      "2025-07-12 14:56:23,560 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,564 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\r\n",
      "2025-07-12 14:56:23,564 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\r\n",
      "2025-07-12 14:56:23,564 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\r\n",
      "2025-07-12 14:56:23,564 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\r\n",
      "2025-07-12 14:56:23,564 INFO blockmanagement.BlockManager: defaultReplication         = 1\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: maxReplication             = 512\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: minReplication             = 1\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\r\n",
      "2025-07-12 14:56:23,565 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\r\n",
      "2025-07-12 14:56:23,596 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\r\n",
      "2025-07-12 14:56:23,596 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-12 14:56:23,596 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-12 14:56:23,596 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\r\n",
      "2025-07-12 14:56:23,602 INFO util.GSet: Computing capacity for map INodeMap\r\n",
      "2025-07-12 14:56:23,602 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-12 14:56:23,602 INFO util.GSet: 1.0% max memory 3.9 GB = 40 MB\r\n",
      "2025-07-12 14:56:23,602 INFO util.GSet: capacity      = 2^22 = 4194304 entries\r\n",
      "2025-07-12 14:56:23,604 INFO namenode.FSDirectory: ACLs enabled? true\r\n",
      "2025-07-12 14:56:23,604 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\r\n",
      "2025-07-12 14:56:23,604 INFO namenode.FSDirectory: XAttrs enabled? true\r\n",
      "2025-07-12 14:56:23,604 INFO namenode.NameNode: Caching file names occurring more than 10 times\r\n",
      "2025-07-12 14:56:23,608 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\r\n",
      "2025-07-12 14:56:23,608 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\r\n",
      "2025-07-12 14:56:23,609 INFO snapshot.SnapshotManager: SkipList is disabled\r\n",
      "2025-07-12 14:56:23,611 INFO util.GSet: Computing capacity for map cachedBlocks\r\n",
      "2025-07-12 14:56:23,611 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-12 14:56:23,612 INFO util.GSet: 0.25% max memory 3.9 GB = 10 MB\r\n",
      "2025-07-12 14:56:23,612 INFO util.GSet: capacity      = 2^20 = 1048576 entries\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,618 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\r\n",
      "2025-07-12 14:56:23,618 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\r\n",
      "2025-07-12 14:56:23,618 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\r\n",
      "2025-07-12 14:56:23,621 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\r\n",
      "2025-07-12 14:56:23,621 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\r\n",
      "2025-07-12 14:56:23,622 INFO util.GSet: Computing capacity for map NameNodeRetryCache\r\n",
      "2025-07-12 14:56:23,622 INFO util.GSet: VM type       = 64-bit\r\n",
      "2025-07-12 14:56:23,623 INFO util.GSet: 0.029999999329447746% max memory 3.9 GB = 1.2 MB\r\n",
      "2025-07-12 14:56:23,623 INFO util.GSet: capacity      = 2^17 = 131072 entries\r\n",
      "2025-07-12 14:56:23,645 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1021452147-10.1.0.58-1752332183640\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,707 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,746 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,819 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\r\n",
      "2025-07-12 14:56:23,832 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\r\n",
      "2025-07-12 14:56:23,835 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\r\n",
      "2025-07-12 14:56:23,855 INFO namenode.FSNamesystem: Stopping services started for active state\r\n",
      "2025-07-12 14:56:23,855 INFO namenode.FSNamesystem: Stopping services started for standby state\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:23,869 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\r\n",
      "2025-07-12 14:56:23,869 INFO namenode.NameNode: SHUTDOWN_MSG: \r\n",
      "/************************************************************\r\n",
      "SHUTDOWN_MSG: Shutting down NameNode at fv-az1719-822/10.1.0.58\r\n",
      "************************************************************/\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs namenode -format -nonInteractive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMrEiLB_VAeR"
   },
   "source": [
    "## Start cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:24.021009Z",
     "iopub.status.busy": "2025-07-12T14:56:24.020787Z",
     "iopub.status.idle": "2025-07-12T14:56:34.008778Z",
     "shell.execute_reply": "2025-07-12T14:56:34.008057Z"
    },
    "id": "FXHowFfFEwAF",
    "outputId": "deb79696-2f24-410a-90e1-e09111c68b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting datanodes\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting secondary namenodes [fv-az1719-822]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fv-az1719-822: Warning: Permanently added 'fv-az1719-822' (ED25519) to the list of known hosts.\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!$HADOOP_HOME/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:34.011296Z",
     "iopub.status.busy": "2025-07-12T14:56:34.011056Z",
     "iopub.status.idle": "2025-07-12T14:56:35.498617Z",
     "shell.execute_reply": "2025-07-12T14:56:35.497831Z"
    },
    "id": "pOHsiWv9or7B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namenode is not in safe mode.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Check if HDFS is in safe mode\n",
    "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
    "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
    "  hdfs dfsadmin -safemode leave\n",
    "else\n",
    "  echo \"Namenode is not in safe mode.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKRRbwDFv3ZQ"
   },
   "source": [
    "# Run some simple HDFS commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:35.501128Z",
     "iopub.status.busy": "2025-07-12T14:56:35.500757Z",
     "iopub.status.idle": "2025-07-12T14:56:43.498405Z",
     "shell.execute_reply": "2025-07-12T14:56:43.497702Z"
    },
    "id": "73wuvOJTxX4O",
    "outputId": "25357ddf-891c-4639-8cc2-a6ce862b96dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup         20 2025-07-12 14:56 my_dir/mnist_test.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# create directory \"my_dir\" in HDFS home\n",
    "hdfs dfs -mkdir /user\n",
    "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
    "hdfs dfs -mkdir my_dir\n",
    "\n",
    "# if sampls_data does not exist, create it (so that the notebook can run also outside of Colab)\n",
    "mkdir -p sample_data\n",
    "touch sample_data/mnist_test.csv\n",
    "\n",
    "# Check if the file is empty and fill it if needed\n",
    "if [ ! -s sample_data/mnist_test.csv ]; then\n",
    "  echo -e \"0 1 2 3 4\\n5 6 7 8 9\" > sample_data/mnist_test.csv\n",
    "fi\n",
    "\n",
    "\n",
    "# upload file mnist_test.csv to my_dir\n",
    "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
    "\n",
    "# show contents of directory my_dir\n",
    "hdfs dfs -ls -h my_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3KBe4R65bl1"
   },
   "source": [
    "# Run some simple MapReduce jobs\n",
    "\n",
    "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
    "\n",
    "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVJA-3jSATGV"
   },
   "source": [
    "## Simplest MapReduce job\n",
    "\n",
    "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
    "\n",
    "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
    "\n",
    "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ICcKO2jcHl"
   },
   "source": [
    "Now run the MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:43.501062Z",
     "iopub.status.busy": "2025-07-12T14:56:43.500660Z",
     "iopub.status.idle": "2025-07-12T14:56:50.280046Z",
     "shell.execute_reply": "2025-07-12T14:56:50.279363Z"
    },
    "id": "VDuQYWGi5b7J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_simplest': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "namenode is running as process 3373.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,096 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,181 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,181 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,190 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,408 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:46,425 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,821 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1475239370_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,821 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,914 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,915 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,915 INFO mapreduce.Job: Running job: job_local1475239370_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,916 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,919 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,919 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,943 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,946 INFO mapred.LocalJobRunner: Starting task: attempt_local1475239370_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,960 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,961 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,973 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,977 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:48,996 INFO mapred.MapTask: numReduceTasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,013 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,014 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,014 INFO mapred.MapTask: soft limit at 83886080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,014 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,014 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,016 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,017 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,021 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,023 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,023 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,023 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,024 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,024 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,025 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,025 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,025 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,025 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,026 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,026 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,111 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,113 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,120 INFO streaming.PipeMapRed: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,120 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,122 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,122 INFO mapred.MapTask: Starting flush of map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,122 INFO mapred.MapTask: Spilling map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,122 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,122 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,127 INFO mapred.MapTask: Finished spill 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,136 INFO mapred.Task: Task:attempt_local1475239370_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,139 INFO mapred.LocalJobRunner: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,139 INFO mapred.Task: Task 'attempt_local1475239370_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,144 INFO mapred.Task: Final Counters for attempt_local1475239370_0001_m_000000_0: Counters: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=141986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=860083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=239075328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,144 INFO mapred.LocalJobRunner: Finishing task: attempt_local1475239370_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,145 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,148 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,148 INFO mapred.LocalJobRunner: Starting task: attempt_local1475239370_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,154 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,154 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,155 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,157 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1e5faf6e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,158 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,169 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2936012800, maxSingleShuffleLimit=734003200, mergeThreshold=1937768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,171 INFO reduce.EventFetcher: attempt_local1475239370_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,209 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1475239370_0001_m_000000_0 decomp: 28 len: 32 to MEMORY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,213 INFO reduce.InMemoryMapOutput: Read 28 bytes from map-output for attempt_local1475239370_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,214 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 28, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,220 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,221 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,221 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,225 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,225 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,226 INFO reduce.MergeManagerImpl: Merged 1 segments, 28 bytes to disk to satisfy reduce memory limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,229 INFO reduce.MergeManagerImpl: Merging 1 files, 32 bytes from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,230 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,230 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,232 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,234 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,234 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,238 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,239 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,276 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,277 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,278 INFO streaming.PipeMapRed: Records R/W=2/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,279 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,319 INFO mapred.Task: Task:attempt_local1475239370_0001_r_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,322 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,322 INFO mapred.Task: Task attempt_local1475239370_0001_r_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,332 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1475239370_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,333 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,333 INFO mapred.Task: Task 'attempt_local1475239370_0001_r_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,334 INFO mapred.Task: Final Counters for attempt_local1475239370_0001_r_000000_0: Counters: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=142082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=860115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=239075328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,334 INFO mapred.LocalJobRunner: Finishing task: attempt_local1475239370_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,334 INFO mapred.LocalJobRunner: reduce task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,922 INFO mapreduce.Job: Job job_local1475239370_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,923 INFO mapreduce.Job:  map 100% reduce 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,924 INFO mapreduce.Job: Job job_local1475239370_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,930 INFO mapreduce.Job: Counters: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=284068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=1720198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=478150656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:49,930 INFO streaming.StreamJob: Output directory: output_simplest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
    "mapred streaming \\\n",
    "  -input my_dir \\\n",
    "  -output output_simplest \\\n",
    "  -mapper /bin/cat \\\n",
    "  -reducer /usr/bin/wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiZ6FH2gFfE5"
   },
   "source": [
    "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHPEoIIWFubx"
   },
   "source": [
    "Check the output of the MapReduce job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:50.282812Z",
     "iopub.status.busy": "2025-07-12T14:56:50.282390Z",
     "iopub.status.idle": "2025-07-12T14:56:51.924342Z",
     "shell.execute_reply": "2025-07-12T14:56:51.923599Z"
    },
    "id": "rB7FXYTbwNzm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      10      22\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output_simplest/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDObCPW2F39S"
   },
   "source": [
    "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbosNo0TD3oH"
   },
   "source": [
    "## Another MapReduce example: filter a log file\n",
    "\n",
    "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
    "\n",
    "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
    "\n",
    "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVdUuulwGzq5"
   },
   "source": [
    "Download the logfile `Linux_2k.log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:51.926804Z",
     "iopub.status.busy": "2025-07-12T14:56:51.926608Z",
     "iopub.status.idle": "2025-07-12T14:56:52.159335Z",
     "shell.execute_reply": "2025-07-12T14:56:52.158569Z"
    },
    "id": "yJIm4SPZFPxy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-12 14:56:51--  https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 216485 (211K) [text/plain]\r\n",
      "Saving to: ‘Linux_2k.log’\r\n",
      "\r\n",
      "\r",
      "Linux_2k.log          0%[                    ]       0  --.-KB/s               \r",
      "Linux_2k.log        100%[===================>] 211.41K  --.-KB/s    in 0.01s   \r\n",
      "\r\n",
      "2025-07-12 14:56:52 (17.2 MB/s) - ‘Linux_2k.log’ saved [216485/216485]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:52.162024Z",
     "iopub.status.busy": "2025-07-12T14:56:52.161578Z",
     "iopub.status.idle": "2025-07-12T14:56:55.072250Z",
     "shell.execute_reply": "2025-07-12T14:56:55.071365Z"
    },
    "id": "M1WgyQE3MYWI"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -mkdir input || true\n",
    "hdfs dfs -put Linux_2k.log input/ || true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILUOCdzEH3Gm"
   },
   "source": [
    "Define the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:55.075396Z",
     "iopub.status.busy": "2025-07-12T14:56:55.074954Z",
     "iopub.status.idle": "2025-07-12T14:56:55.079557Z",
     "shell.execute_reply": "2025-07-12T14:56:55.079027Z"
    },
    "id": "4-rraIUdfdj0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # split the line into words\n",
    "    line = line.strip()\n",
    "    fields = line.split()\n",
    "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
    "      print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8AxdFFPIuDo"
   },
   "source": [
    "Test the script (after setting the correct permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:55.081721Z",
     "iopub.status.busy": "2025-07-12T14:56:55.081372Z",
     "iopub.status.idle": "2025-07-12T14:56:55.192161Z",
     "shell.execute_reply": "2025-07-12T14:56:55.191430Z"
    },
    "id": "QwOk_y7egbGM"
   },
   "outputs": [],
   "source": [
    "!chmod 700 mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhv95VzfIAnz"
   },
   "source": [
    "Look at the first 10 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:55.194679Z",
     "iopub.status.busy": "2025-07-12T14:56:55.194173Z",
     "iopub.status.idle": "2025-07-12T14:56:55.305929Z",
     "shell.execute_reply": "2025-07-12T14:56:55.305193Z"
    },
    "id": "9qf1dFqIKgoJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r",
      "\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r",
      "\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -10 Linux_2k.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQ09Y1AqR6Fy"
   },
   "source": [
    "Test the mapper in the shell (not using MapReduce):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:55.308235Z",
     "iopub.status.busy": "2025-07-12T14:56:55.308029Z",
     "iopub.status.idle": "2025-07-12T14:56:55.437369Z",
     "shell.execute_reply": "2025-07-12T14:56:55.436713Z"
    },
    "id": "TJ0kDRsigCC2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\r\n",
      "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\r\n",
      "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\r\n",
      "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\r\n",
      "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\r\n",
      "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\r\n",
      "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\r\n",
      "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\r\n",
      "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\r\n",
      "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\r\n",
      "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\r\n",
      "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\r\n",
      "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\r\n"
     ]
    }
   ],
   "source": [
    "!head -100 Linux_2k.log| ./mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXbgh5g7OraF"
   },
   "source": [
    "Now run the MapReduce job on the pseudo-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:55.439749Z",
     "iopub.status.busy": "2025-07-12T14:56:55.439245Z",
     "iopub.status.idle": "2025-07-12T14:56:59.819984Z",
     "shell.execute_reply": "2025-07-12T14:56:59.819246Z"
    },
    "id": "G7SEzMC2OqWW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_filter': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:57,233 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [mapper.py] [] /tmp/streamjob1385876954389327095.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:57,787 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:57,863 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:57,863 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:57,873 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,078 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,090 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,238 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1740120820_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,238 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,394 INFO mapred.LocalDistributedCacheManager: Localized file:/home/runner/work/big_data/big_data/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1740120820_0001_0e9317d1-9ede-4df5-b81e-550ff5d14e33/mapper.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,459 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,460 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,460 INFO mapreduce.Job: Running job: job_local1740120820_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,461 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,466 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,466 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,505 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,507 INFO mapred.LocalJobRunner: Starting task: attempt_local1740120820_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,524 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,524 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,544 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,549 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,566 INFO mapred.MapTask: numReduceTasks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,610 INFO streaming.PipeMapRed: PipeMapRed exec [/home/runner/work/big_data/big_data/./mapper.py]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,615 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,617 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,617 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,618 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,618 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,618 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,619 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,619 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,619 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,620 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,620 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,621 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,669 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,669 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,671 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,679 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,682 INFO streaming.PipeMapRed: Records R/W=1201/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,692 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,699 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,702 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,737 INFO mapred.Task: Task:attempt_local1740120820_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,740 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,740 INFO mapred.Task: Task attempt_local1740120820_0001_m_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,749 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1740120820_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,750 INFO mapred.LocalJobRunner: Records R/W=1201/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,750 INFO mapred.Task: Task 'attempt_local1740120820_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,755 INFO mapred.Task: Final Counters for attempt_local1740120820_0001_m_000000_0: Counters: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=719156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=111149056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,755 INFO mapred.LocalJobRunner: Finishing task: attempt_local1740120820_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:58,755 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:59,463 INFO mapreduce.Job: Job job_local1740120820_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:59,464 INFO mapreduce.Job:  map 100% reduce 0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:59,466 INFO mapreduce.Job: Job job_local1740120820_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:59,470 INFO mapreduce.Job: Counters: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=719156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=111149056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=85436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:56:59,470 INFO streaming.StreamJob: Output directory: output_filter\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "hdfs dfs -rm -r output_filter\n",
    "\n",
    "mapred streaming \\\n",
    "  -file mapper.py \\\n",
    "  -input input \\\n",
    "  -output output_filter \\\n",
    "  -mapper mapper.py \\\n",
    "  -reducer NONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuZJ2ACzSTJi"
   },
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:56:59.822442Z",
     "iopub.status.busy": "2025-07-12T14:56:59.822111Z",
     "iopub.status.idle": "2025-07-12T14:57:01.297038Z",
     "shell.execute_reply": "2025-07-12T14:57:01.296275Z"
    },
    "id": "RhLA5HZEhfmT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 root supergroup          0 2025-07-12 14:56 output_filter/_SUCCESS\r\n",
      "-rw-r--r--   1 root supergroup      85436 2025-07-12 14:56 output_filter/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls output_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:01.299636Z",
     "iopub.status.busy": "2025-07-12T14:57:01.299197Z",
     "iopub.status.idle": "2025-07-12T14:57:02.934552Z",
     "shell.execute_reply": "2025-07-12T14:57:02.933780Z"
    },
    "id": "Ffi4RvXnPH14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\r\n",
      "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat output_filter/part-00000 |head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sam22f-YT1xR"
   },
   "source": [
    "## Aggregate data with MapReduce\n",
    "\n",
    "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:02.937492Z",
     "iopub.status.busy": "2025-07-12T14:57:02.936850Z",
     "iopub.status.idle": "2025-07-12T14:57:02.941414Z",
     "shell.execute_reply": "2025-07-12T14:57:02.940877Z"
    },
    "id": "fMKEqUF1T-v9",
    "outputId": "b3f905e8-800c-4441-ef2e-1d044c1d2b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing myAggregatorForKeyCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile myAggregatorForKeyCount.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "\n",
    "def generateLongCountToken(id):\n",
    "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
    "\n",
    "def main(argv):\n",
    "    line = sys.stdin.readline()\n",
    "    try:\n",
    "        while line:\n",
    "            line = line[:-1]\n",
    "            fields = line.split()\n",
    "            s = fields[4].split('[')[0]\n",
    "            print(generateLongCountToken(s))\n",
    "            line = sys.stdin.readline()\n",
    "    except \"end of file\":\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b2S9K8FWDMz"
   },
   "source": [
    "Set permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:02.943607Z",
     "iopub.status.busy": "2025-07-12T14:57:02.943134Z",
     "iopub.status.idle": "2025-07-12T14:57:03.054082Z",
     "shell.execute_reply": "2025-07-12T14:57:03.053258Z"
    },
    "id": "35DP8K2_WDYO"
   },
   "outputs": [],
   "source": [
    "!chmod 700 myAggregatorForKeyCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9M8lgxMVRYz"
   },
   "source": [
    "Test the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:03.056501Z",
     "iopub.status.busy": "2025-07-12T14:57:03.056086Z",
     "iopub.status.idle": "2025-07-12T14:57:03.184274Z",
     "shell.execute_reply": "2025-07-12T14:57:03.183487Z"
    },
    "id": "k-R7VNoTVRjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:logrotate:\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:su(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n",
      "LongValueSum:sshd(pam_unix)\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOEpMFvsVRtM"
   },
   "source": [
    "Run the MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:03.186848Z",
     "iopub.status.busy": "2025-07-12T14:57:03.186427Z",
     "iopub.status.idle": "2025-07-12T14:57:07.742035Z",
     "shell.execute_reply": "2025-07-12T14:57:07.741362Z"
    },
    "id": "XwxHJ7yyVR34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `output_aggregate': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:05,125 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "packageJobJar: [myAggregatorForKeyCount.py] [] /tmp/streamjob1220146221685080510.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:05,744 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:05,824 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:05,824 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:05,834 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,036 INFO mapred.FileInputFormat: Total input files to process : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,047 INFO mapreduce.JobSubmitter: number of splits:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,179 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1979674862_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,179 INFO mapreduce.JobSubmitter: Executing with tokens: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,305 INFO mapred.LocalDistributedCacheManager: Localized file:/home/runner/work/big_data/big_data/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local1979674862_0001_d211987a-3a0f-454b-9641-96c3e154fe85/myAggregatorForKeyCount.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,371 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,372 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,373 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,374 INFO mapreduce.Job: Running job: job_local1979674862_0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,378 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,378 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,437 INFO mapred.LocalJobRunner: Waiting for map tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,439 INFO mapred.LocalJobRunner: Starting task: attempt_local1979674862_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,457 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,457 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,469 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,474 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,490 INFO mapred.MapTask: numReduceTasks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,510 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,511 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,511 INFO mapred.MapTask: soft limit at 83886080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,511 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,511 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,514 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,520 INFO streaming.PipeMapRed: PipeMapRed exec [/home/runner/work/big_data/big_data/./myAggregatorForKeyCount.py]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,525 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,526 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,527 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,527 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,527 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,527 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,528 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,529 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,529 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,529 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,530 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,530 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,606 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,607 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,609 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,617 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,622 INFO streaming.PipeMapRed: Records R/W=1696/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,633 INFO streaming.PipeMapRed: MRErrorThread done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,641 INFO streaming.PipeMapRed: mapRedFinished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,644 INFO mapred.LocalJobRunner: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,644 INFO mapred.MapTask: Starting flush of map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,644 INFO mapred.MapTask: Spilling map output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,644 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,644 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,677 INFO mapred.MapTask: Finished spill 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,687 INFO mapred.Task: Task:attempt_local1979674862_0001_m_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,691 INFO mapred.LocalJobRunner: Records R/W=1696/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,691 INFO mapred.Task: Task 'attempt_local1979674862_0001_m_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,696 INFO mapred.Task: Final Counters for attempt_local1979674862_0001_m_000000_0: Counters: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=1111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=720598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=48923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=218103808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,697 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979674862_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,697 INFO mapred.LocalJobRunner: map task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,702 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,702 INFO mapred.LocalJobRunner: Starting task: attempt_local1979674862_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,713 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,713 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,714 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,715 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d14c165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,721 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,735 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2936012800, maxSingleShuffleLimit=734003200, mergeThreshold=1937768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,737 INFO reduce.EventFetcher: attempt_local1979674862_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,758 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1979674862_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,761 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local1979674862_0001_m_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,763 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,764 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,765 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,765 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,769 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,769 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,771 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,771 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,772 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,772 INFO mapred.Merger: Merging 1 sorted segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,772 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,773 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,822 INFO mapred.Task: Task:attempt_local1979674862_0001_r_000000_0 is done. And is in the process of committing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,824 INFO mapred.LocalJobRunner: 1 / 1 copied.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,824 INFO mapred.Task: Task attempt_local1979674862_0001_r_000000_0 is allowed to commit now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,835 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1979674862_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,835 INFO mapred.LocalJobRunner: reduce > reduce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,835 INFO mapred.Task: Task 'attempt_local1979674862_0001_r_000000_0' done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,836 INFO mapred.Task: Final Counters for attempt_local1979674862_0001_r_000000_0: Counters: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=721380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=218103808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,836 INFO mapred.LocalJobRunner: Finishing task: attempt_local1979674862_0001_r_000000_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:06,836 INFO mapred.LocalJobRunner: reduce task executor complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:07,383 INFO mapreduce.Job: Job job_local1979674862_0001 running in uber mode : false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:07,384 INFO mapreduce.Job:  map 100% reduce 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:07,385 INFO mapreduce.Job: Job job_local1979674862_0001 completed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:07,391 INFO mapreduce.Job: Counters: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile System Counters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes read=3818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of bytes written=1441978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFILE: Number of write operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read=432970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of read operations=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of large read operations=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of write operations=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tHDFS: Number of bytes read erasure-coded=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tMap-Reduce Framework\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output bytes=48923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMap output materialized bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tInput split bytes=102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine input records=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCombine output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input groups=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce shuffle bytes=782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce input records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tReduce output records=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tSpilled Records=60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tShuffled Maps =1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFailed Shuffles=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tMerged Map outputs=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tGC time elapsed (ms)=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tTotal committed heap usage (bytes)=436207616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tShuffle Errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBAD_ID=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tCONNECTION=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tIO_ERROR=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_LENGTH=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_MAP=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tWRONG_REDUCE=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Input Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Read=216485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tFile Output Format Counters \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tBytes Written=326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:57:07,391 INFO streaming.StreamJob: Output directory: output_aggregate\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "chmod +x myAggregatorForKeyCount.py\n",
    "\n",
    "hdfs dfs -rm -r output_aggregate\n",
    "\n",
    "mapred streaming \\\n",
    "  -input input \\\n",
    "  -output output_aggregate \\\n",
    "  -mapper myAggregatorForKeyCount.py \\\n",
    "  -reducer aggregate \\\n",
    "  -file myAggregatorForKeyCount.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkuYUkh5W0Je"
   },
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:07.744514Z",
     "iopub.status.busy": "2025-07-12T14:57:07.744266Z",
     "iopub.status.idle": "2025-07-12T14:57:10.683537Z",
     "shell.execute_reply": "2025-07-12T14:57:10.682802Z"
    },
    "id": "ET3KCfX1UC2u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup          0 2025-07-12 14:57 output_aggregate/_SUCCESS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup        326 2025-07-12 14:57 output_aggregate/part-00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bluetooth:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cups:\t12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftpd\t916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm(pam_unix)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm-binary\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpm\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcid\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irqbalance:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel:\t76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klogind\t46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login(pam_unix)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrotate:\t43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named\t16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfslock:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portmap:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpc.statd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpcidmapd:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snmpd\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshd(pam_unix)\t677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su(pam_unix)\t172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sysctl:\t1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslog:\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslogd\t7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udev\t8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xinetd\t2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls output_aggregate\n",
    "hdfs dfs -cat output_aggregate/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj9qz8wSa1w0"
   },
   "source": [
    "Pretty-print table of aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:10.686109Z",
     "iopub.status.busy": "2025-07-12T14:57:10.685735Z",
     "iopub.status.idle": "2025-07-12T14:57:12.289707Z",
     "shell.execute_reply": "2025-07-12T14:57:12.288915Z"
    },
    "id": "Y8IYl4hAZhZm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftpd                 916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshd(pam_unix)       677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su(pam_unix)         172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel:              76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klogind              46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logrotate:           43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named                16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cups:                12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "udev                 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslogd              7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bluetooth:           2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm(pam_unix)        2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpm                  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login(pam_unix)      2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network:             2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syslog:              2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xinetd               2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--                   1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gdm-binary           1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcid                 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irqbalance:          1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfslock:             1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portmap:             1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random:              1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc:                  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpc.statd            1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpcidmapd:           1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdpd                 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snmpd                1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sysctl:              1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
    "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
    "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF6-Z5RotAcO"
   },
   "source": [
    "# Stop cluster\n",
    "\n",
    "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:12.293167Z",
     "iopub.status.busy": "2025-07-12T14:57:12.292948Z",
     "iopub.status.idle": "2025-07-12T14:57:19.278968Z",
     "shell.execute_reply": "2025-07-12T14:57:19.278231Z"
    },
    "id": "IoIYG5NlsIMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping namenodes on [localhost]\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping datanodes\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping secondary namenodes [fv-az1719-822]\r\n"
     ]
    }
   ],
   "source": [
    "!./hadoop-3.4.0/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGj96_e2ccZw"
   },
   "source": [
    "Stop the `sshd` daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T14:57:19.281916Z",
     "iopub.status.busy": "2025-07-12T14:57:19.281414Z",
     "iopub.status.idle": "2025-07-12T14:57:19.419396Z",
     "shell.execute_reply": "2025-07-12T14:57:19.418659Z"
    },
    "id": "FUvKMpy6chQ5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping ssh (via systemctl): ssh.service\u001b[0;1;38;5;185mStopping 'ssh.service', but its triggering units are still active:\u001b[0m\r",
      "\r\n",
      "\u001b[0;1;38;5;185mssh.socket\u001b[0m\r",
      "\r\n",
      ".\r\n"
     ]
    }
   ],
   "source": [
    "!/etc/init.d/ssh stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5N7tb0HSbZB"
   },
   "source": [
    "# Concluding remarks\n",
    "\n",
    "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
    "\n",
    "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
    "\n",
    "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
    "\n",
    "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
    "\n",
    "If you found this notebook helpful, consider exploring:\n",
    " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
    " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
    " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYyg1O7ysUs6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "qFfOrktMPq8M",
    "IF6-Z5RotAcO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
