{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ukIg5iRf3yoD",
        "2QpF7XoRzry7",
        "brQkWtmMi786",
        "31Q0HgWMK19o",
        "VFtQ9QQgj98S",
        "sml8t6rVM17l",
        "BhhgV7kNYT7c",
        "XhJQeLeT99Tz",
        "3d8C0ay3Xa-d",
        "gFMWSBKBX7Yl",
        "92wZzJmSfK35",
        "Ua-nBgvmsJpA",
        "1DAjlH9ltWXj",
        "uH60mvkDzzFk",
        "Jnx75guCFmZk",
        "H-QTevQSJjJM"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMbZOXMsB6DHZXU0SM0itCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/SLURM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\" alt=\"Logo Big Data for Beginners\"></div></a>\n",
        "# SLURM Demonstration: Unleashing Parallel Computing\n",
        "\n",
        "<div width=\"110\"><img src=\"https://github.com/groda/big_data/blob/master/Slurm_logo.svg.png?raw=true\" align=left width=\"90\" style=\"margin:30px\" alt=\"SLURM logo\"/></div>\n",
        "\n",
        "\n",
        " **SLURM** is an acronym for **S**imple **L**inux **U**tility for **R**esource **M**anagement. The name reflects its original design goal of being a straightforward yet powerful tool for managing Linux cluster resources.\n",
        "\n",
        "This demonstration walks you through setting up, configuring, and using SLURM on a single Ubuntu virtual machine (VM), providing a hands-on introduction to its features for high-performance computing (HPC) environments.\n",
        "\n",
        "We‚Äôll explore how to set up and harness SLURM‚Äôs job scheduling capabilities, whether you‚Äôre using Google Colab‚Äôs cloud environment or your own virtual machine (VM). From submitting jobs to verifying parallel execution, you‚Äôll learn to leverage SLURM for efficient computation‚Äîperfect for your single-node Docker setup or beyond.\n",
        "\n",
        "To make navigation easier:\n",
        "- **üöÄ Critical Steps**: Sections marked with the üöÄ emoji highlight essential tasks, such as installation and configuration, that are crucial for setting up and running SLURM. Follow these steps carefully to ensure a successful deployment.\n",
        "- **üìù Side Notes**: Sections marked with the üìù emoji contain optional, supplementary information, such as historical context or additional tips. These can be skipped if you‚Äôre focused on the core setup process but are expandable for deeper insights.\n",
        "\n",
        "Get ready to dive into the power of SLURM for parallel computing and supercharge your workflows!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukIg5iRf3yoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [üöÄ Install the SLURM Packages](#scrollTo=2QpF7XoRzry7)\n",
        "- [üìù Side Note: What is slurm-wlm?](#scrollTo=brQkWtmMi786)\n",
        "- [üìù Side Note: A Brief History of SLURM](#scrollTo=31Q0HgWMK19o)\n",
        "- [üìù Side Note: Debian's sid and SLURM](#scrollTo=VFtQ9QQgj98S)\n",
        "- [üìù Side Note: Nerdy Names, Serious Systems](#scrollTo=_Side_Note_Nerdy_Names_Serious_Systems)\n",
        "- [üöÄ Configuration](#scrollTo=BhhgV7kNYT7c)\n",
        "- [üìù Configure using slurm-wlm-configurator.html](#scrollTo=XhJQeLeT99Tz)\n",
        "- [üöÄ Generate a `munge` key](#scrollTo=3d8C0ay3Xa-d)\n",
        "- [üöÄ Create Spool Directories](#scrollTo=fgqo9xAZXxYS)\n",
        "- [üöÄ Start the Services](#scrollTo=gFMWSBKBX7Yl)\n",
        "- [üöÄ Verify Cluster Status](#scrollTo=92wZzJmSfK35)\n",
        "- [üìù Useful Commands for Debugging](#scrollTo=rNeusBTLcqve)\n",
        "- [üöÄ Run a simple job](#scrollTo=Ua-nBgvmsJpA)\n",
        "- [üöÄ Did the Job Run in Parallel?](#scrollTo=1DAjlH9ltWXj)\n",
        "- [üöÄ Single-task parallel Python with `multiprocessing` and `--cpus-per-task`](#scrollTo=uH60mvkDzzFk)\n",
        "- [üöÄ Run multiple tasks with `srun`](#scrollTo=Jnx75guCFmZk)\n",
        "- [üöÄ SLURM Job Arrays](#scrollTo=H-QTevQSJjJM)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "W5_sjZ6PEAoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Install the SLURM Packages\n",
        "\n",
        "This might take a few seconds ...\n",
        "<img src=\"data:image/gif;base64,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\" alt=\"Running cat\" />"
      ],
      "metadata": {
        "id": "2QpF7XoRzry7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2r8fGUyR-7P",
        "outputId": "e145e4f8-c519-4351-cd03-8c0e0b413612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Wa\r                                                                               \rHit:3 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.92.23)] [Wa\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "slurm-wlm is already the newest version (21.08.5-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 104 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update && sudo sudo apt-get install slurm-wlm -y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the `scontrol version` command to find out the installed SLURM version."
      ],
      "metadata": {
        "id": "dFUVddWILGUA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad27e1d6",
        "outputId": "95788feb-55f3-4815-bdc3-a72ad0e93844"
      },
      "source": [
        "!scontrol version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slurm-wlm 21.08.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Side Note: What is `slurm-wlm`?"
      ],
      "metadata": {
        "id": "brQkWtmMi786"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`slurm-wlm` is the Debian package for SLURM and it is an unofficial distribution from **SchedMD**'s perspective (SchedMD is the company that develops and maintains the open-source Slurm Workload Manager).\n",
        "\n",
        "According to the [Slurm Quick Start Administrator Guide](https://slurm.schedmd.com/quickstart_admin.html#quick_start), SchedMD explicitly notes:\n",
        "\n",
        "> **NOTE**: Some Linux distributions may have unofficial Slurm packages available in software repositories. SchedMD does not maintain or recommend these packages.\n",
        "\n",
        "Some of the reasons why SchedMD does not recommend inofficial builds might be:\n",
        "\n",
        "1. **SchedMD Does Not Maintain It**: The `slurm-wlm` package is maintained by the Debian community, not by SchedMD, the official developers of Slurm. Debian maintainers build and package Slurm for inclusion in their repositories, which may include modifications or configurations that differ from SchedMD's official releases.\n",
        "\n",
        "2. **Potential for Issues**: Unofficial packages may not always align with SchedMD's latest recommendations, configurations, or patches. This can lead to compatibility issues, missing features, or differences in behavior compared to SchedMD's official builds.\n",
        "\n",
        "3. **SchedMD's Recommendation**: SchedMD recommends building Slurm from source or using RPM/DEB packages built directly from their official tarballs (e.g., using `rpmbuild` or `debuild` as described in the guide). This ensures full control over the build process, dependencies, and configuration, tailored to the user's specific cluster needs.\n",
        "\n",
        "Despite SchedMD's recommendation against unofficial packages, users might choose Debian's `slurm-wlm` for convenience, especially in environments already integrated with Debian's package management system (`apt`). Benefits include:\n",
        "\n",
        "- **Ease of Installation**: Installing `slurm-wlm` via `apt` is simpler than building from source or creating custom packages.\n",
        "- **Dependency Management**: Debian's package manager automatically handles dependencies like `munge`, `mysql`, or other required libraries.\n",
        "- **System Integration**: The package is configured to work with Debian's conventions (e.g., systemd services, file paths like `/etc/slurm/` for configuration).\n",
        "\n",
        "However, users should be cautious:\n",
        "\n",
        "- **Version Lag**: Debian's `sid` repository may not always have the latest Slurm version. For example, as of October 12, 2025, `slurm-wlm` is at version `24.11.5-4`, which is recent but may lag behind SchedMD's latest releases.\n",
        "- **Customizations**: Debian's package may include patches or configurations not endorsed by SchedMD, potentially affecting behavior.\n",
        "- **Support**: Issues with Debian's package would need to be addressed through Debian's bug tracker rather than SchedMD's support channels.\n"
      ],
      "metadata": {
        "id": "UUXFHKA34Us5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Side Note: A Brief History of SLURM\n"
      ],
      "metadata": {
        "id": "31Q0HgWMK19o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slurm was first developed in 2001 at **Lawrence Livermore National Laboratory (LLNL)** to manage large Linux clusters for high-performance computing (HPC). The first formal publication of Slurm appeared in a 2003 paper by Yoo, A.B., Jette, M.A., and Grondona, M., titled \"SLURM: Simple Linux Utility for Resource Management,\" presented at the Workshop on Job Scheduling Strategies for Parallel Processing (published in Lecture Notes in Computer Science, pp. 44‚Äì60, Springer Berlin Heidelberg). This paper introduced Slurm‚Äôs design principles, emphasizing its lightweight architecture and adaptability and it is available at [https://www.osti.gov/servlets/purl/15002533](https://www.osti.gov/servlets/purl/15002533).\n",
        "\n",
        "Slurm was designed as a lightweight, scalable alternative to existing resource managers. Over the years, SLURM gained popularity in the HPC community, with significant adoption by supercomputing centers. Key milestones include the introduction of the REST API in 2020 and support for JWT authentication in 2023.\n",
        "\n",
        "In terms of scheduling, SLURM has evolved from initially only supporting FIFO to:\n",
        "- **Multifactor Priority Scheduling**, where multiple factors such as quality of service, age, size, _fairshare_, are taken into account when prioritizing jobs\n",
        "- **Backfill Scheduling**, allowing smaller, shorter jobs to ‚Äúfill in‚Äù gaps in the schedule while waiting for larger jobs to acquire sufficient resources\n",
        "- **Preemption**, where higher-priority jobs can interrupt or suspend lower-priority jobs to access resources immediately\n",
        "- **Advanced Resource Allocation**: SLURM can allocate exactly the resources a job needs‚ÄîCPUs, GPUs, memory, licenses, or even specific nodes‚Äîdown to the socket or core level\n",
        "- **Burst Buffer Support**: fast intermediate storage (e.g., NVMe SSDs) placed between RAM and parallel filesystem to speed up I/O-heavy jobs without flooding slow storage\n",
        "- **Cloud & Elastic Computing**: automatically spin up/down cloud nodes (AWS, Azure, GCP) when on-prem resources are full\n",
        "- **Container Support**: run jobs inside Docker, Singularity/Apptainer, or Podman containers directly via SLURM to ensures reproducible, portable, secure software environments\n",
        "\n",
        "Today, SLURM powers some of the world's largest supercomputers.\n",
        "\n"
      ],
      "metadata": {
        "id": "8vErIxs2gPYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Side Note: Debian's `sid` and SLURM"
      ],
      "metadata": {
        "id": "VFtQ9QQgj98S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debian's versioning includes **stable** (e.g., `bookworm`), **testing** (e.g., `trixie`), and **unstable** (`sid`) branches. The `sid` branch, named after the mischievous character Sid from Toy Story (1995, Pixar Animation Studios), who breaks toys and creates chaos, reflects its role as Debian's cutting-edge, unstable repository where packages are continuously updated. As noted in Debian's documentation, `sid` is a development playground, prone to breakages but offering the latest software versions, like `slurm-wlm` version `24.11.5-4` as of October 2025.\n",
        "\n",
        "Users might choose `slurm-wlm` from `sid` for its recent SLURM features, such as enhanced GPU autodetection (`AutoDetect=nvml` for NVIDIA GPUs) or REST API improvements (`slurmrestd`), which may not yet be in `bookworm` (e.g., `23.02.x`). Its integration with Debian‚Äôs `apt` simplifies installation and dependency management (e.g., `munge`, `mysql`). However, `sid`‚Äôs instability‚Äîakin to Sid‚Äôs destructive antics in Toy Story‚Äîmakes it risky for production HPC clusters, as updates can introduce bugs or dependency conflicts. For testing or research environments needing the latest SLURM capabilities, `sid` is appealing, but SchedMD recommends building from source for reliability and official support."
      ],
      "metadata": {
        "id": "ADJNkIxFkBbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Side Note: Nerdy Names, Serious Systems\n",
        "\n"
      ],
      "metadata": {
        "id": "sml8t6rVM17l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While SLURM stands for Simple Linux Utility for Resource Management it also nods to *Slurm*, the soft drink in *Futurama*.\n",
        "\n",
        "I notice more and more how deeply tech culture is shaped by the tastes and humor of a specific group ‚Äî mostly young, male ‚Äúnerds.‚Äù Debian versions named after *Toy Story* characters, SLURM taking its name from a beverage in *Futurama* ‚Äî these choices aren‚Äôt random. They reveal a shared subculture that has long defined open-source and DevOps communities.\n",
        "\n",
        "What started as playful references among enthusiasts has become a kind of tradition ‚Äî a way of signaling identity and belonging. The same spirit shows up in projects like **Borg** (_Star Trek_) or **Python** (_Monty Python‚Äôs Flying Circus_). It‚Äôs a mix of wit, nostalgia, and a sense of ‚Äúif you get the joke, you‚Äôre one of us.‚Äù\n",
        "\n",
        "It encodes a certain kind of taste and belonging: if you recognize *Toy Story*, *Futurama*, *Star Wars*, or *Monty Python* jokes, you‚Äôre in the club. If not, you‚Äôre subtly reminded that the culture wasn‚Äôt built with you in mind. It‚Äôs not malicious, but it is a reflection of a narrow demographic shaping the collective tone of open-source and DevOps culture. And that tone often persists even as the field itself has become more diverse and mature.\n",
        "\n",
        "There‚Äôs an interesting contrast here: these projects embody serious technical excellence, yet their cultural expression is stuck in a kind of perpetual adolescence ‚Äî like an in-joke from a college dorm that somehow became global infrastructure.\n",
        "\n",
        "This tendency, however, is mostly a relic of the early days of computer science up through the 1990s, when the field was dominated by a small, highly homogeneous group of enthusiasts steeped in science fiction, comics, and arcade culture. Modern technologies increasingly break away from this ‚Äúnerd comics‚Äù culture, opting instead for names that are descriptive, abstract, or evocative. Examples include *Terraform*, *Docker*, *Kubernetes*, *Airflow*, *Figma*, *Notion*, and *Snowflake* ‚Äî names that signal function, creativity, or metaphor rather than relying on insider pop-culture knowledge.\n",
        "\n",
        "Perhaps what could be wished for is simply more freedom to break away from that established mold. Tech doesn‚Äôt have to lose its sense of humor to grow up a little. It can remain creative and irreverent, while exploring names, metaphors, and references that reflect the wider, richer world of ideas around it."
      ],
      "metadata": {
        "id": "Ipvf135-L1Wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Configuration\n",
        "\n",
        "Configuration instructions are provided in `/usr/share/doc/slurmctld/README.Debian`."
      ],
      "metadata": {
        "id": "BhhgV7kNYT7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /usr/share/doc/slurmctld/README.Debian"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVdSILtfYJGk",
        "outputId": "48ef03ec-d195-4271-af0e-6d68a01dd056"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration Instructions\n",
            "==========================\n",
            "In order to use SLURM you need a proper configuration file for your\n",
            "cluster that need to be stored under /etc/slurm/slurm.conf on every\n",
            "node. You can point your browser to\n",
            "file:///usr/share/doc/slurmctld/slurm-wlm-configurator.html for an\n",
            "automatic configuration tool. Please leave red fields untouched and\n",
            "change green field to fit your cluster configuration.\n",
            "\n",
            "You can also find a simple sample configuration that provides a\n",
            "control machine to run the Slurm's central management daemon and\n",
            "a single node for job execution under\n",
            "/usr/share/doc/slurmctld/examples/slurm.conf.simple.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to create a file `slurm.conf`.\n",
        "\n",
        "For convenience, create a symbolic link to the folder `/etc/slurm` so that the file can be opened from the left pane in Google Colab's Jupyterhub interface."
      ],
      "metadata": {
        "id": "oDJ51P8xVhOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo ln -sf /etc/slurm ./"
      ],
      "metadata": {
        "id": "vsmGSDV_YcNS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 CPUs, 7923 MB RAM):"
      ],
      "metadata": {
        "id": "LHWqyIilbDmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile temp_slurm.conf\n",
        "# Minimal slurm.conf for single-node testing\n",
        "ClusterName=mylocalcluster\n",
        "SlurmctldHost=localhost\n",
        "AuthType=auth/munge\n",
        "MpiDefault=none\n",
        "ProctrackType=proctrack/linuxproc\n",
        "ReturnToService=2\n",
        "SlurmctldPidFile=/var/run/slurmctld.pid\n",
        "SlurmctldPort=6817\n",
        "SlurmdPidFile=/var/run/slurmd.pid\n",
        "SlurmdPort=6818\n",
        "StateSaveLocation=/var/spool/slurmctld\n",
        "SlurmdSpoolDir=/var/spool/slurmd\n",
        "SlurmUser=slurm\n",
        "SlurmdLogFile=/var/log/slurmd.log\n",
        "SlurmctldLogFile=/var/log/slurmctld.log\n",
        "# Node and partition configuration\n",
        "NodeName=localhost CPUs=2 RealMemory=7923 State=UNKNOWN\n",
        "PartitionName=LocalQ Nodes=localhost Default=YES MaxTime=INFINITE State=UP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wlRLDmLVPNj",
        "outputId": "03b0ee21-7912-46f0-f02a-478acf550cc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing temp_slurm.conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Use sudo to move temp_slurm.conf to the protected system directory\n",
        "!sudo mv temp_slurm.conf /etc/slurm/slurm.conf"
      ],
      "metadata": {
        "id": "g7FVOMHcjoSR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify your config: NodeName, ControlMachine, and PartitionName.Nodes must match either `localhost` or `hostname -s`."
      ],
      "metadata": {
        "id": "wWB0pjcMWzi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Configure using `slurm-wlm-configurator.html`\n",
        "\n",
        "Instead of editing the configuration file `slurm.conf` you could also start a minimal Web app. This is an overkill for the current demonstration but it might be interesting to see all possible SLURM configuration parameters."
      ],
      "metadata": {
        "id": "XhJQeLeT99Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin by creating a minimal Flask Web application."
      ],
      "metadata": {
        "id": "krQCXb9HMwil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_flask.py\n",
        "from flask import Flask, Response\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def serve_configurator():\n",
        "   try:\n",
        "       with open('/usr/share/doc/slurmctld/slurm-wlm-configurator.html', 'r') as f:\n",
        "           html_content = f.read()\n",
        "       return Response(html_content, mimetype='text/html')\n",
        "   except FileNotFoundError:\n",
        "       return \"Error: slurm-wlm-configurator.html not found\", 404\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL87RRKqt2Ot",
        "outputId": "b5091462-9f18-4c77-e13e-c0ededb154e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_flask.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Check if the process is running and terminate it\n",
        "if 'process' in locals() and process.poll() is None:\n",
        "    print(\"Terminating existing Flask process...\")\n",
        "    process.terminate()\n",
        "\n",
        "with open('flask.out', \"w\") as stdout_file, open('flask.err', \"w\") as stderr_file:\n",
        "    process = subprocess.Popen(\n",
        "        [\"python\", \"run_flask.py\"],\n",
        "        stdout=stdout_file,\n",
        "        stderr=stderr_file,\n",
        "        preexec_fn=os.setsid  # Start the process in a new session\n",
        "    )"
      ],
      "metadata": {
        "id": "pmr_NqNSt3VS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Serve the Web app through `output` (only works if in Google Colab). Click on the link below."
      ],
      "metadata": {
        "id": "zX4Dj9P5Cwbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# true if running on Google Colab\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_window(5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "MdU_M04muIQ0",
        "outputId": "14e74475-c19e-4e44-dd2b-8f4ca768edf3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(5000, \"/\", \"https://localhost:5000/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hostname -s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EsfKH2VW2L4",
        "outputId": "92d922d1-e758-4fad-adc3-4595231ea030"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b710c5a06c09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Generate a `munge` key\n",
        "\n",
        "MUNGE stands for **M**ac-based **U**ser **N**ame **G**roup **E**xpiration and it is a lightweight, high-performance authentication system used by SLURM to securely verify user identity (UID/GID) and message integrity across cluster nodes using a shared symmetric key and time-limited credentials. It ensures fast, trusted communication between SLURM daemons and clients with minimal overhead."
      ],
      "metadata": {
        "id": "3d8C0ay3Xa-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a secure, random 1024-byte shared key for MUNGE authentication."
      ],
      "metadata": {
        "id": "gvm489ZVbj1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024 >/dev/null 2>&1\n",
        "sudo chown munge:munge /etc/munge/munge.key\n",
        "sudo chmod 400 /etc/munge/munge.key\n"
      ],
      "metadata": {
        "id": "2HPJoAE5Xpyy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Create Spool Directories"
      ],
      "metadata": {
        "id": "fgqo9xAZXxYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo mkdir -p /var/spool/slurmctld /var/spool/slurmd /var/lib/munge\n",
        "sudo chown slurm:slurm /var/spool/slurm{ctld,d} /var/lib/munge\n",
        "sudo chown munge:munge /var/lib/munge\n",
        "sudo chmod 755 /var/spool/slurm* /var/lib/munge"
      ],
      "metadata": {
        "id": "VhO4SsqMX0yP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### üìå Recap: installation\n",
        "> - On Ubuntu, install the unofficial Debian package for SLURM with the command `apt install slurm-wlm -y`\n",
        "> - A minimal configuration requires creating a `slurm.conf` file, a `munge` key, and spool directories."
      ],
      "metadata": {
        "id": "Ra4_XsQsncaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Start the Services"
      ],
      "metadata": {
        "id": "gFMWSBKBX7Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip the next step if you are launching the services for the first time."
      ],
      "metadata": {
        "id": "ufPA8L4hqQ7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo service munge stop\n",
        "sudo service slurmctld stop\n",
        "sudo service slurmd stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAbDbtEWX_kP",
        "outputId": "a7a95fe7-d20f-4532-9797-52058a866636"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping MUNGE munged\n",
            "   ...done.\n",
            " * Stopping slurm central management daemon slurmctld\n",
            "   ...done.\n",
            " * Stopping slurm compute node daemon slurmd\n",
            "   ...done.\n",
            "slurmd is stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo service munge start\n",
        "sudo service slurmctld start\n",
        "sudo service slurmd start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXZ71G39ZKke",
        "outputId": "04a7fb33-4795-48bb-dff4-90005c51b6e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting MUNGE munged\n",
            "   ...done.\n",
            " * Starting slurm central management daemon slurmctld\n",
            "   ...done.\n",
            " * Starting slurm compute node daemon slurmd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Verify Cluster Status"
      ],
      "metadata": {
        "id": "92wZzJmSfK35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWWFjgBTfMal",
        "outputId": "54e4ddb0-bc1a-4274-ede4-7d3a9134cb26"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
            "LocalQ*      up   infinite      1   idle localhost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see something like\n",
        "```\n",
        "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
        "LocalQ*      up   infinite      1   idle localhost\n",
        "```\n",
        "\n",
        "This is a 1-node cluster with no time limit for jobs."
      ],
      "metadata": {
        "id": "Ij97YCdMNLiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the node is not idle, set it:"
      ],
      "metadata": {
        "id": "oIXkkbNAfPSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo scontrol update NodeName=6d91971d1d4a State=IDLE"
      ],
      "metadata": {
        "id": "kMpX-uFQfadO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Useful Commands for Debugging\n",
        "\n",
        "To look for errors you can use:\n",
        "\n",
        "- `tail /var/log/slurmctld.log` to view the end of the `slurmctld` log file.\n",
        "- `grep \"error\" /var/log/slurmctld.log` to search for specific terms like \"error\" in the log file.\n",
        "- `sudo -u slurm /usr/sbin/slurmctld -D -vvv` for streaming output to the console\n",
        "\n"
      ],
      "metadata": {
        "id": "rNeusBTLcqve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep \"error\" /var/log/slurmctld.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkg0m1F5aIoy",
        "outputId": "8c21e13c-dfa5-40cc-9bc6-eeb3dbe665b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-03-01T18:01:05.547] error: chdir(/var/log): Permission denied\n",
            "[2026-03-01T18:01:05.547] error: Configured MailProg is invalid\n",
            "[2026-03-01T18:01:05.559] error: Could not open node state file /var/spool/slurmctld/node_state: No such file or directory\n",
            "[2026-03-01T18:01:05.559] error: NOTE: Trying backup state save file. Information may be lost!\n",
            "[2026-03-01T18:01:05.560] error: Could not open job state file /var/spool/slurmctld/job_state: No such file or directory\n",
            "[2026-03-01T18:01:05.560] error: NOTE: Trying backup state save file. Jobs may be lost!\n",
            "[2026-03-01T18:01:05.560] error: Could not open reservation state file /var/spool/slurmctld/resv_state: No such file or directory\n",
            "[2026-03-01T18:01:05.560] error: NOTE: Trying backup state save file. Reservations may be lost\n",
            "[2026-03-01T18:01:05.560] error: Could not open trigger state file /var/spool/slurmctld/trigger_state: No such file or directory\n",
            "[2026-03-01T18:01:05.560] error: NOTE: Trying backup state save file. Triggers may be lost!\n",
            "[2026-03-01T18:02:20.830] error: Could not open job state file /var/spool/slurmctld/job_state: No such file or directory\n",
            "[2026-03-01T18:02:20.830] error: NOTE: Trying backup state save file. Jobs may be lost!\n",
            "[2026-03-01T18:02:22.918] error: chdir(/var/log): Permission denied\n",
            "[2026-03-01T18:02:22.918] error: Configured MailProg is invalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## üìå Recap: start SLURM\n",
        "> - Three services need to be started: `munge`, `slurmctld`, and `slurmd`.\n",
        "> - Use `sinfo` to verify the cluster status."
      ],
      "metadata": {
        "id": "dOevZO30pBK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Run a simple job\n",
        "\n",
        "To run a SLURM job, you generally embed _SLURM directives_ inside a shell script. This script tells SLURM what resources you need and what commands to execute.\n",
        "\n",
        "The script must start with #!/bin/bash or another valid shell.\n",
        "\n",
        "All SLURM directives start with `#SBATCH` and must appear at the top of the script before any commands. For instance the following directives\n",
        "```\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=LocalQ\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=2\n",
        "#SBATCH --mem=100M\n",
        "```\n",
        "tell SLURM:\n",
        "- to run the job in partition (queue) `LocalQ` (we chose this name in the configuration file);\n",
        "- to run one task (one single process) with $2$ CPU cores for each task;\n",
        "- to request $100$ megabytes of RAM‚ÄîSLURM will reserve this amount and prevent your job from exceeding it. Depending on how memory constraints are configured, if your program uses more memory than requested, it may be terminated.\n",
        "\n",
        "After the `#SBATCH` directives, you can write normal shell commands.\n",
        "\n",
        "‚ö†Ô∏è Be careful not insert blank lines between the shebang (`#!/bin/bash`) and the directives (`#SBATCH`) otherwise SLURM will ignore them!"
      ],
      "metadata": {
        "id": "Ua-nBgvmsJpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a bash script that computes the sum of the first $100$ numbers."
      ],
      "metadata": {
        "id": "L7a4-uIH9ODT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_job.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=LocalQ\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=2\n",
        "#SBATCH --mem=100M\n",
        "\n",
        "echo \"Job started on $(hostname) at $(date)\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "echo \"Job ID: $SLURM_JOB_ID\"\n",
        "echo \"Calculating sum of numbers 1 to 100...\"\n",
        "sum=0\n",
        "for i in {1..100}; do\n",
        "    sum=$((sum + i))\n",
        "done\n",
        "echo \"Sum: $sum\"\n",
        "sleep 5\n",
        "echo \"Job finished at $(date)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqr1tRXvkd5x",
        "outputId": "c66be1ca-93f5-488e-8e19-ca2dbef9bbac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple_job.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "chmod +x simple_job.sh\n",
        "sbatch simple_job.sh\n",
        "sleep 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RGthhyJk-cI",
        "outputId": "61bce7d5-cd35-4c8d-c5a1-0ae6a39dd6f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls slurm-*.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_M2u0yJlQAI",
        "outputId": "82267399-3b30-42ea-f035-405927607c62"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slurm-1.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat slurm-1.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BNaJMn8lTOs",
        "outputId": "cb4071c6-83f3-4197-f9d8-2ccfd05bf345"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job started on b710c5a06c09 at Sun Mar  1 06:02:25 PM UTC 2026\n",
            "Running on node: localhost\n",
            "Job ID: 1\n",
            "Calculating sum of numbers 1 to 100...\n",
            "Sum: 5050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the result is correct"
      ],
      "metadata": {
        "id": "Kdr7QnootMiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "100*101/2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy9TZElhs__A",
        "outputId": "72141f14-ed2e-40b9-b5ba-7c447abaaec5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5050.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Did the Job Run in Parallel?"
      ],
      "metadata": {
        "id": "1DAjlH9ltWXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scontrol show job 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbfx_M89t89I",
        "outputId": "32ef9266-b6d1-40ed-928a-a72ff435f1ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobId=1 JobName=simple_job.sh\n",
            "   UserId=root(0) GroupId=root(0) MCS_label=N/A\n",
            "   Priority=4294901759 Nice=0 Account=(null) QOS=(null)\n",
            "   JobState=RUNNING Reason=None Dependency=(null)\n",
            "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
            "   RunTime=00:00:03 TimeLimit=UNLIMITED TimeMin=N/A\n",
            "   SubmitTime=2026-03-01T18:02:23 EligibleTime=2026-03-01T18:02:23\n",
            "   AccrueTime=2026-03-01T18:02:23\n",
            "   StartTime=2026-03-01T18:02:25 EndTime=Unknown Deadline=N/A\n",
            "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2026-03-01T18:02:25 Scheduler=Main\n",
            "   Partition=LocalQ AllocNode:Sid=b710c5a06c09:3447\n",
            "   ReqNodeList=(null) ExcNodeList=(null)\n",
            "   NodeList=localhost\n",
            "   BatchHost=localhost\n",
            "   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n",
            "   TRES=cpu=2,node=1,billing=2\n",
            "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
            "   MinCPUsNode=2 MinMemoryNode=100M MinTmpDiskNode=0\n",
            "   Features=(null) DelayBoot=00:00:00\n",
            "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
            "   Command=/content/simple_job.sh\n",
            "   WorkDir=/content\n",
            "   StdErr=/content/slurm-1.out\n",
            "   StdIn=/dev/null\n",
            "   StdOut=/content/slurm-1.out\n",
            "   Power=\n",
            "   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line\n",
        "```\n",
        "NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ...\n",
        "```\n",
        "confirms that SLURM reserved 2 CPUs for our job.\n",
        "\n",
        "But this doesn't mean that the job used the two CPUs for parallel computation since our script `simple_job.sh` is a sequential Bash loop (`for i in {1..100}`) that runs on a single process/thread.\n",
        "\n",
        "The option `--cpus-per-task=2` means that SLURM allocates 2 CPUs to the job and that other jobs can‚Äôt use them, but the script‚Äôs computation remains single-threaded."
      ],
      "metadata": {
        "id": "G19ZE50zuZbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## üìå Recap: SLURM doesn't parallelize your code for you\n",
        "> SLURM just hooks you up with resources like CPUs or machines. If your program‚Äôs single-threaded, it‚Äôll stick to one CPU unless you make it parallel.\n"
      ],
      "metadata": {
        "id": "CXpc7Ko5cH4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The `scontrol show job` command primarily shows information about jobs that are currently running or have recently completed and are still held in SLURM's internal state. This information is not retained indefinitely.\n",
        "\n",
        "For long-term retention of job information, SLURM uses an accounting database that needs to be installed extra and can be queried with `sacct`). The retention period for job information in the accounting database is determined by the SLURM accounting configuration."
      ],
      "metadata": {
        "id": "dPWDC-e1OXVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ  Single-task parallel Python with `multiprocessing` and `--cpus-per-task`"
      ],
      "metadata": {
        "id": "uH60mvkDzzFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us modify the script so that it runs in parallel on the two CPUs. In order to achieve this, let us use Python's `multiprocessing` library. Note that we still need to wrap our code inside a shell script."
      ],
      "metadata": {
        "id": "gNiHqJACv4ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_job_parallel.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=LocalQ\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=2\n",
        "#SBATCH --mem=100M\n",
        "\n",
        "echo \"Job started on $(hostname) at $(date)\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "echo \"Job ID: $SLURM_JOB_ID\"\n",
        "echo \"Calculating sum of numbers 1 to 100 in parallel with Python...\"\n",
        "\n",
        "python3 -c \"\n",
        "from multiprocessing import Pool\n",
        "def add(x): return x\n",
        "with Pool(processes=2) as pool:\n",
        "    result = sum(pool.map(add, range(1, 101)))\n",
        "print(f'Sum: {result}')\n",
        "\"\n",
        "\n",
        "sleep 10\n",
        "echo \"Job finished at $(date)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NweDRLhSv8vu",
        "outputId": "05aaf378-4af9-4f26-cc15-fb43ccb1493d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple_job_parallel.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --out id\n",
        "sbatch simple_job_parallel.sh"
      ],
      "metadata": {
        "id": "8RcwsrF_vn8n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjp6H6rF9wq8",
        "outputId": "9181ad08-f982-4e1d-ddca-3070e41bb66b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_ID = id.split()[-1]"
      ],
      "metadata": {
        "id": "11ZA-QSy9xWZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat slurm-{JOB_ID}.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUno8Yd2wqZ8",
        "outputId": "1404f81b-8813-4a64-941c-176d92f4c20f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: slurm-2.out: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us introduce some logging to view the parallel processing."
      ],
      "metadata": {
        "id": "zhqUzxy-3NMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_job_parallel.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=\"Run in Parallel\"  # name of the job\n",
        "#SBATCH --partition=LocalQ\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=2\n",
        "#SBATCH --mem=100M\n",
        "#SBATCH --time=00:02:00               # HH:MM:SS format\n",
        "\n",
        "# Ensure psutil is installed\n",
        "if ! python3 -c \"import psutil\" 2>/dev/null; then\n",
        "    sudo apt update && sudo apt install -y python3-psutil\n",
        "fi\n",
        "\n",
        "echo \"Job started on $(hostname) at $(date)\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "echo \"Job ID: $SLURM_JOB_ID\"\n",
        "echo \"Calculating sum of numbers from 1 to 100 in parallel with Python...\"\n",
        "\n",
        "python3 -c \"\n",
        "import multiprocessing as mp\n",
        "import psutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "def add(x):\n",
        "    # Log PID and CPU affinity\n",
        "    process = psutil.Process()\n",
        "    cpu_affinity = process.cpu_affinity()\n",
        "    start_time = time.ctime(process.create_time())\n",
        "    print(f'Process PID={os.getpid()}, CPU affinity={cpu_affinity}, Start time={start_time}, x={x}')\n",
        "    return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    with mp.Pool(processes=2) as pool:\n",
        "        result = sum(pool.map(add, range(1, 101)))\n",
        "    print(f'Sum of numbers: {result}')\n",
        "\"\n",
        "\n",
        "sleep 10\n",
        "echo \"Job finished at $(date)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DggIOqU23T0O",
        "outputId": "cd293ce0-1c09-4850-e3bc-0064be4070fe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting simple_job_parallel.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --out id\n",
        "sbatch simple_job_parallel.sh"
      ],
      "metadata": {
        "id": "sybAWfLO3m7k"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86D15H5s3oOX",
        "outputId": "da8351a4-27f6-46c5-911a-7aff11dada62"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** the number of processes (defined in `Pool(processes=2)`) should not exceed the CPUs reserved in SLURM (`--cpus-per-task=2`), otherwise some processes might have to wait while the maximum $2$ CPUs available simultaneosly are busy. Running more parallel processes than the number of CPUs reserved (via `--cpus-per-task`) is called _oversubscription_. The job might still complete if there is enough available memory, but it will be slowed down."
      ],
      "metadata": {
        "id": "cO0sv3_sxmam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_ID = id.split()[-1]"
      ],
      "metadata": {
        "id": "zbE1yMeh8hXE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Wait for the job to finish\n",
        "while True:\n",
        "    result = !squeue -j {JOB_ID} -h\n",
        "    if not result:  # squeue returns empty if job is not in queue\n",
        "        print(f\"Job {JOB_ID} finished.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Waiting for job {JOB_ID} to finish...\")\n",
        "        print(\"\\n\".join(result))\n",
        "        time.sleep(5) # Wait for 5 seconds before checking again\n",
        "\n",
        "# After the job is finished, you can view the output files\n",
        "!cat slurm-{JOB_ID}.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV-5kRp68P-5",
        "outputId": "0236ffc8-3352-41fc-aed2-f1e63ef1792b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for job 3 to finish...\n",
            "                 3    LocalQ Run in P     root PD       0:00      1 (Resources)\n",
            "Waiting for job 3 to finish...\n",
            "                 3    LocalQ Run in P     root PD       0:00      1 (Resources)\n",
            "Waiting for job 3 to finish...\n",
            "                 3    LocalQ Run in P     root PD       0:00      1 (Resources)\n",
            "Waiting for job 3 to finish...\n",
            "                 3    LocalQ Run in P     root  R       0:03      1 localhost\n",
            "Waiting for job 3 to finish...\n",
            "                 3    LocalQ Run in P     root  R       0:08      1 localhost\n",
            "Job 3 finished.\n",
            "Job started on b710c5a06c09 at Sun Mar  1 06:02:41 PM UTC 2026\n",
            "Running on node: localhost\n",
            "Job ID: 3\n",
            "Calculating sum of numbers from 1 to 100 in parallel with Python...\n",
            "Sum of numbers: 5050\n",
            "Job finished at Sun Mar  1 06:02:51 PM UTC 2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that two processes were spawned:"
      ],
      "metadata": {
        "id": "3Zs60Rso4r3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2 slurm-{JOB_ID}.out |grep PID| sort | uniq"
      ],
      "metadata": {
        "id": "_aQNVDH9GbE_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see which numbers were picked by each process:"
      ],
      "metadata": {
        "id": "u1r7TNigGcGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2,12 slurm-{JOB_ID}.out |grep PID| sort | uniq -f1"
      ],
      "metadata": {
        "id": "TLSAq_b3GQUw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scontrol show job {JOB_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBz1pHcNzKm2",
        "outputId": "9f57f041-2549-49eb-e211-45a1a114a94c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobId=3 JobName=Run in Parallel\n",
            "   UserId=root(0) GroupId=root(0) MCS_label=N/A\n",
            "   Priority=4294901757 Nice=0 Account=(null) QOS=(null)\n",
            "   JobState=COMPLETED Reason=None Dependency=(null)\n",
            "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
            "   RunTime=00:00:10 TimeLimit=00:02:00 TimeMin=N/A\n",
            "   SubmitTime=2026-03-01T18:02:29 EligibleTime=2026-03-01T18:02:29\n",
            "   AccrueTime=2026-03-01T18:02:29\n",
            "   StartTime=2026-03-01T18:02:41 EndTime=2026-03-01T18:02:51 Deadline=N/A\n",
            "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2026-03-01T18:02:41 Scheduler=Main\n",
            "   Partition=LocalQ AllocNode:Sid=b710c5a06c09:3447\n",
            "   ReqNodeList=(null) ExcNodeList=(null)\n",
            "   NodeList=localhost\n",
            "   BatchHost=localhost\n",
            "   NumNodes=1 NumCPUs=2 NumTasks=1 CPUs/Task=2 ReqB:S:C:T=0:0:*:*\n",
            "   TRES=cpu=2,node=1,billing=2\n",
            "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
            "   MinCPUsNode=2 MinMemoryNode=100M MinTmpDiskNode=0\n",
            "   Features=(null) DelayBoot=00:00:00\n",
            "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
            "   Command=/content/simple_job_parallel.sh\n",
            "   WorkDir=/content\n",
            "   StdErr=/content/slurm-3.out\n",
            "   StdIn=/dev/null\n",
            "   StdOut=/content/slurm-3.out\n",
            "   Power=\n",
            "   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Python's `multiprocessing.Pool.map()` automatically splits the computation into chunks (works for **any iterable**, not just numbers):\n",
        "* `range(1, 101)` ‚Üí 100 items: 1, 2, 3, ‚Ä¶, 100\n",
        "* `Pool(processes=2)` ‚Üí creates **2 worker processes**\n",
        "* `pool.map(add, range(1, 101))` splits the 100 items and for 2 processes, it might assign roughly:\n",
        "    | Process  | Items  |\n",
        "    | -------- | ------ |\n",
        "    | Worker 1 | 1‚Äì50   |\n",
        "    | Worker 2 | 51‚Äì100 |\n",
        "\n",
        "```\n",
        "    range(1, 101)  ‚Üí  [1, 2, 3, ..., 100]\n",
        "\n",
        "    Pool(processes=2)\n",
        "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "    ‚îÇ Worker 1      ‚îÇ   ‚îÇ Worker 2      ‚îÇ\n",
        "    ‚îÇ 1, 2, 3 ...50 ‚îÇ   ‚îÇ 51, 52 ...100 ‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "    Each worker applies add(x) to its chunk\n",
        "    Results are collected and combined ‚Üí sum = 5050\n",
        "```\n"
      ],
      "metadata": {
        "id": "CH5gYIBO82Fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Run multiple tasks with `srun`"
      ],
      "metadata": {
        "id": "Jnx75guCFmZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_job_srun.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=\"srun job\"  # name of the job\n",
        "#SBATCH --ntasks=2             # number of tasks\n",
        "#SBATCH --cpus-per-task=1      # we just have 2 CPUs on COlab\n",
        "#SBATCH --mem=100M\n",
        "#SBATCH --time=00:02:00        # HH:MM:SS format\n",
        "\n",
        "# Ensure psutil is installed\n",
        "if ! python3 -c \"import psutil\" 2>/dev/null; then\n",
        "    sudo apt update && sudo apt install -y python3-psutil\n",
        "fi\n",
        "\n",
        "echo \"Job started on $(hostname) at $(date)\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "echo \"Job ID: $SLURM_JOB_ID\"\n",
        "echo \"Calculating sum of numbers from 1 to 100 with srun ...\"\n",
        "\n",
        "python3 -c \"\n",
        "import psutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "def add(x):\n",
        "    # Log PID and CPU affinity\n",
        "    process = psutil.Process()\n",
        "    cpu_affinity = process.cpu_affinity()\n",
        "    start_time = time.ctime(process.create_time())\n",
        "    print(f'Process PID={os.getpid()}, CPU affinity={cpu_affinity}, Start time={start_time}, x={x}')\n",
        "    return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = sum(map(add, range(1, 101)))\n",
        "    print(f'Sum of numbers: {result}')\n",
        "\"\n",
        "\n",
        "sleep 10\n",
        "echo \"Job finished at $(date)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VDa3EgH6qF9",
        "outputId": "2f7429c6-017a-4041-c09a-4ce0396b30d1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple_job_srun.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --out id\n",
        "sbatch simple_job_srun.sh"
      ],
      "metadata": {
        "id": "l_rfl0ZoB9lq"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5697eafc-d7f2-46cf-8015-22aaa5c2f482",
        "id": "5UVwLORdB9lr"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_ID = id.split()[-1]"
      ],
      "metadata": {
        "id": "dDRlp4NxIItS"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afacaaa3",
        "outputId": "6d317e16-1c95-41a7-f2ab-0d33f798a702"
      },
      "source": [
        "import time\n",
        "\n",
        "# Wait for the job to finish with a timeout\n",
        "timeout = 30 # seconds\n",
        "start_time = time.time()\n",
        "\n",
        "while True:\n",
        "    result = !squeue -j {JOB_ID} -h\n",
        "    if not result:  # squeue returns empty if job is not in queue\n",
        "        print(f\"Job {JOB_ID} finished.\")\n",
        "        break\n",
        "    elif time.time() - start_time > timeout:\n",
        "        print(f\"Timeout ({timeout} seconds) reached. Job {JOB_ID} may still be running or have failed.\")\n",
        "        break\n",
        "    else:\n",
        "        print(f\"Waiting for job {JOB_ID} to finish...\")\n",
        "        print(\"\\n\".join(result))\n",
        "        time.sleep(5) # Wait for 5 seconds before checking again\n",
        "\n",
        "# After the job is finished (or timeout), you can attempt to view the output files\n",
        "!cat slurm-{JOB_ID}.out"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for job 4 to finish...\n",
            "                 4    LocalQ srun job     root PD       0:00      1 (None)\n",
            "Waiting for job 4 to finish...\n",
            "                 4    LocalQ srun job     root  R       0:05      1 localhost\n",
            "Waiting for job 4 to finish...\n",
            "                 4    LocalQ srun job     root  R       0:10      1 localhost\n",
            "Job 4 finished.\n",
            "Job started on b710c5a06c09 at Sun Mar  1 06:02:56 PM UTC 2026\n",
            "Running on node: localhost\n",
            "Job ID: 4\n",
            "Calculating sum of numbers from 1 to 100 with srun ...\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=1\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=2\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=3\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=4\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=5\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=6\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=7\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=8\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=9\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=10\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=11\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=12\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=13\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=14\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=15\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=16\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=17\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=18\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=19\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=20\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=21\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=22\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=23\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=24\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=25\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=26\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=27\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=28\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=29\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=30\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=31\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=32\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=33\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=34\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=35\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=36\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=37\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=38\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=39\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=40\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=41\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=42\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=43\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=44\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=45\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=46\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=47\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=48\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=49\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=50\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=51\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=52\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=53\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=54\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=55\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=56\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=57\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=58\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=59\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=60\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=61\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=62\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=63\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=64\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=65\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=66\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=67\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=68\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=69\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=70\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=71\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=72\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=73\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=74\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=75\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=76\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=77\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=78\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=79\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=80\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=81\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=82\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=83\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=84\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=85\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=86\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=87\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=88\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=89\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=90\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=91\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=92\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=93\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=94\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=95\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=96\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=97\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=98\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=99\n",
            "Process PID=4582, CPU affinity=[0, 1], Start time=Sun Mar  1 18:02:55 2026, x=100\n",
            "Sum of numbers: 5050\n",
            "Job finished at Sun Mar  1 06:03:06 PM UTC 2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But in this case we only ran one process"
      ],
      "metadata": {
        "id": "Uhk51XnqCOXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2 slurm-{JOB_ID}.out |grep PID| sort | uniq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRu-PXfqCDRq",
        "outputId": "4964eff7-009d-4362-e6f9-62263e858c27"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID=4582,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2,12 slurm-{JOB_ID}.out |grep PID| sort | uniq -f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxwVhGWBJaiS",
        "outputId": "ff9a6817-b5c3-43ff-9832-4973adee0a25"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID=4582, 2026,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though $2$ CPUs were reserved, `srun` did not automatically split the job into two tasks, but instead ran the job on one CPU while keeping the second one reserved (and idle)."
      ],
      "metadata": {
        "id": "52PiOd20JlaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scontrol show job {JOB_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL98l_IeCHSC",
        "outputId": "e32b6c3e-ca0a-421b-83a1-55d0c8763e07"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobId=4 JobName=srun job\n",
            "   UserId=root(0) GroupId=root(0) MCS_label=N/A\n",
            "   Priority=4294901756 Nice=0 Account=(null) QOS=(null)\n",
            "   JobState=COMPLETED Reason=None Dependency=(null)\n",
            "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
            "   RunTime=00:00:11 TimeLimit=00:02:00 TimeMin=N/A\n",
            "   SubmitTime=2026-03-01T18:02:55 EligibleTime=2026-03-01T18:02:55\n",
            "   AccrueTime=2026-03-01T18:02:55\n",
            "   StartTime=2026-03-01T18:02:55 EndTime=2026-03-01T18:03:06 Deadline=N/A\n",
            "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2026-03-01T18:02:55 Scheduler=Main\n",
            "   Partition=LocalQ AllocNode:Sid=b710c5a06c09:3447\n",
            "   ReqNodeList=(null) ExcNodeList=(null)\n",
            "   NodeList=localhost\n",
            "   BatchHost=localhost\n",
            "   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
            "   TRES=cpu=2,node=1,billing=2\n",
            "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
            "   MinCPUsNode=1 MinMemoryNode=100M MinTmpDiskNode=0\n",
            "   Features=(null) DelayBoot=00:00:00\n",
            "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
            "   Command=/content/simple_job_srun.sh\n",
            "   WorkDir=/content\n",
            "   StdErr=/content/slurm-4.out\n",
            "   StdIn=/dev/null\n",
            "   StdOut=/content/slurm-4.out\n",
            "   Power=\n",
            "   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `srun` it's up to us to specify how to split the job."
      ],
      "metadata": {
        "id": "rKWvJ3qjJ69E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile simple_job_srun_manual_split.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=\"srun job\"  # name of the job\n",
        "#SBATCH --ntasks=2             # number of tasks\n",
        "#SBATCH --cpus-per-task=1      # we just have 2 CPUs on COlab\n",
        "#SBATCH --mem=100M\n",
        "#SBATCH --time=00:02:00        # HH:MM:SS format\n",
        "\n",
        "# Ensure psutil is installed\n",
        "if ! python3 -c \"import psutil\" 2>/dev/null; then\n",
        "    sudo apt update && sudo apt install -y python3-psutil\n",
        "fi\n",
        "\n",
        "echo \"Job started on $(hostname) at $(date)\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "echo \"Job ID: $SLURM_JOB_ID\"\n",
        "echo \"Calculating sum of numbers from 1 to 100 with srun ...\"\n",
        "\n",
        "# Launch Python for each SLURM task\n",
        "srun python3 - <<EOF\n",
        "import psutil\n",
        "import os\n",
        "import time\n",
        "\n",
        "def add(x):\n",
        "    # Log PID and CPU affinity\n",
        "    process = psutil.Process()\n",
        "    cpu_affinity = process.cpu_affinity()\n",
        "    start_time = time.ctime(process.create_time())\n",
        "    print(f'Process PID={os.getpid()}, CPU affinity={cpu_affinity}, Start time={start_time}, x={x}')\n",
        "    return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # SLURM environment variables\n",
        "    task_id = int(os.environ.get(\"SLURM_PROCID\", 0))\n",
        "    ntasks = int(os.environ.get(\"SLURM_NTASKS\", 1))\n",
        "\n",
        "    # Split the range across tasks\n",
        "    total = 100\n",
        "    chunk_size = total // ntasks\n",
        "    start = task_id * chunk_size + 1\n",
        "    end = (task_id + 1) * chunk_size if task_id != ntasks - 1 else total\n",
        "\n",
        "    # Compute partial sum\n",
        "    result = sum(map(add, range(start, end + 1)))\n",
        "    print(f\"Task {task_id}: sum({start}..{end}) = {result}\")\n",
        "\n",
        "EOF\n",
        "\n",
        "sleep 10\n",
        "echo \"Job finished at $(date)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbm1MZpGKGJ7",
        "outputId": "af80edfa-5b3d-4e84-da40-93564b450273"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple_job_srun_manual_split.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --out id\n",
        "sbatch simple_job_srun_manual_split.sh"
      ],
      "metadata": {
        "id": "jVwyexBvPSlo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d4605b-3b22-4a5f-ac7b-1e0d9cd70c84",
        "id": "R31dwxDUPSlo"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_ID = id.split()[-1]"
      ],
      "metadata": {
        "id": "8tAubOxOPSlo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scontrol show job {JOB_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUsNuDUEPiqO",
        "outputId": "18145c4a-97c9-4bde-b1dc-85d349019331"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JobId=5 JobName=srun job\n",
            "   UserId=root(0) GroupId=root(0) MCS_label=N/A\n",
            "   Priority=4294901755 Nice=0 Account=(null) QOS=(null)\n",
            "   JobState=PENDING Reason=None Dependency=(null)\n",
            "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
            "   RunTime=00:00:00 TimeLimit=00:02:00 TimeMin=N/A\n",
            "   SubmitTime=2026-03-01T18:03:11 EligibleTime=2026-03-01T18:03:11\n",
            "   AccrueTime=Unknown\n",
            "   StartTime=Unknown EndTime=Unknown Deadline=N/A\n",
            "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2026-03-01T18:03:11 Scheduler=Main\n",
            "   Partition=LocalQ AllocNode:Sid=b710c5a06c09:3447\n",
            "   ReqNodeList=(null) ExcNodeList=(null)\n",
            "   NodeList=(null)\n",
            "   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n",
            "   TRES=cpu=2,mem=100M,node=1,billing=2\n",
            "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
            "   MinCPUsNode=1 MinMemoryNode=100M MinTmpDiskNode=0\n",
            "   Features=(null) DelayBoot=00:00:00\n",
            "   OverSubscribe=NO Contiguous=0 Licenses=(null) Network=(null)\n",
            "   Command=/content/simple_job_srun_manual_split.sh\n",
            "   WorkDir=/content\n",
            "   StdErr=/content/slurm-5.out\n",
            "   StdIn=/dev/null\n",
            "   StdOut=/content/slurm-5.out\n",
            "   Power=\n",
            "   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sleep 5\n",
        "!cat slurm-{JOB_ID}.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As0ynV5aPsMH",
        "outputId": "606c0b4b-81ee-4de0-cb5a-aa3c2b6e83ef"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job started on b710c5a06c09 at Sun Mar  1 06:03:12 PM UTC 2026\n",
            "Running on node: localhost\n",
            "Job ID: 5\n",
            "Calculating sum of numbers from 1 to 100 with srun ...\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=51\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=52\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=53\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=54\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=55\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=56\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=57\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=58\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=59\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=60\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=61\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=62\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=63\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=64\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=65\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=66\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=67\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=68\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=69\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=70\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=71\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=72\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=73\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=74\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=75\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=76\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=77\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=78\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=79\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=80\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=81\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=82\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=83\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=84\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=85\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=86\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=87\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=88\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=89\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=90\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=91\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=92\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=93\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=94\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=95\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=96\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=97\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=98\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=99\n",
            "Process PID=4723, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=100\n",
            "Task 1: sum(51..100) = 3775\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=1\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=2\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=3\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=4\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=5\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=6\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=7\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=8\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=9\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=10\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=11\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=12\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=13\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=14\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=15\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=16\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=17\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=18\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=19\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=20\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=21\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=22\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=23\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=24\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=25\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=26\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=27\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=28\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=29\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=30\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=31\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=32\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=33\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=34\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=35\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=36\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=37\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=38\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=39\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=40\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=41\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=42\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=43\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=44\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=45\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=46\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=47\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=48\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=49\n",
            "Process PID=4722, CPU affinity=[0, 1], Start time=Sun Mar  1 18:03:11 2026, x=50\n",
            "Task 0: sum(1..50) = 1275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the computation was split into two tasks, each task had its own process and the two processes ran simultaneously."
      ],
      "metadata": {
        "id": "r3DgF1VERch8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2 slurm-{JOB_ID}.out |grep PID| sort | uniq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0c8e3f-8f0e-45f8-a667-d21eca3efba3",
        "id": "7ykU5BrbPSlp"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID=4722,\n",
            "PID=4723,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cut -d' ' -f2,12 slurm-{JOB_ID}.out |grep PID| sort | uniq -f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e70b9e7-7af0-48e0-d454-2e520f86ca18",
        "id": "92z9_UunPSlp"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PID=4722, 2026,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## üìå Recap: how to run a SLURM job\n",
        ">\n",
        "> * Start your script with the shebang `#!/bin/bash`\n",
        "> * Use SBATCH directives to request resources: `#SBATCH --option=value`\n",
        "> * Do **not** insert empty lines between the shebang and SBATCH directives\n",
        "> * Most common directives: `--job-name`, `--output`, `--error`, `--partition`, `--ntasks`, `--cpus-per-task`, `--mem`, `--time`\n",
        "> * Write your commands **after the directives**, e.g., `python my_script.py`\n",
        "> * Run $2$ tasks in parallel with `--ntasks=1`, `--cpus-per-task=2`, and Python's `multiprocessing` with `Pool(2)`\n",
        "> * Use `srun` for parallel execution with `--ntasks` ‚â•$1$ but take care of splitting the computation"
      ],
      "metadata": {
        "id": "WImegEuy5qAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ SLURM Job Arrays"
      ],
      "metadata": {
        "id": "H-QTevQSJjJM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3348bc9"
      },
      "source": [
        "Here is a simple script to demonstrate SLURM job arrays. Each task in the array will print its assigned `SLURM_ARRAY_TASK_ID`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca75c1ce",
        "outputId": "f30ac62e-826a-4ad6-a192-f4b45f24a30f"
      },
      "source": [
        "%%writefile simple_array_job.sh\n",
        "#!/bin/bash\n",
        "#SBATCH --partition=LocalQ\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=1\n",
        "#SBATCH --output=array_output_%A_%a.out\n",
        "#SBATCH --array=0-3  # This creates a job array with tasks 0, 1, 2, and 3\n",
        "\n",
        "echo \"This is array task $SLURM_ARRAY_TASK_ID of job $SLURM_JOB_ID\"\n",
        "echo \"Running on node: $SLURM_NODELIST\"\n",
        "sleep 5 # Simulate some work"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing simple_array_job.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c171181"
      },
      "source": [
        "# Make the script executable\n",
        "!chmod +x simple_array_job.sh"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --out id\n",
        "sbatch simple_array_job.sh"
      ],
      "metadata": {
        "id": "xUPZcDTAOkzZ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo {id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e4bce70-20e3-49a5-cecf-f4b07be1ecc8",
        "id": "xaK_nI1VKZZp"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submitted batch job 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_ID = id.split()[-1]"
      ],
      "metadata": {
        "id": "A51WVhrIKZZp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eafbd97",
        "outputId": "8d79092b-62ff-4d15-f517-919bd2cc03be"
      },
      "source": [
        "# Check the job status (optional)\n",
        "!squeue"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "           6_[0-3]    LocalQ simple_a     root PD       0:00      1 (Resources)\n",
            "                 5    LocalQ srun job     root  R       0:05      1 localhost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the jobs are not yet finished, you will get an error when you try to view the output files.\n",
        "\n",
        "```\n",
        "cat: 'array_output_*.out': No such file or directory\n",
        "```\n"
      ],
      "metadata": {
        "id": "vW9pkCTdH0Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat array_output_*.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-m6lyZDLNXh",
        "outputId": "64ab87ee-4708-4176-a226-70556626f1bf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: 'array_output_*.out': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the jobs are finished, you can view the output files."
      ],
      "metadata": {
        "id": "pzdhw7n9H7JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Wait for all jobs to finish\n",
        "while True:\n",
        "    result = !squeue -u $(id -u)\n",
        "    # Check if the output contains more than just the header\n",
        "    if len(result) <= 1:\n",
        "        print(\"All jobs finished.\")\n",
        "        break\n",
        "    else:\n",
        "        print(\"Waiting for jobs to finish...\")\n",
        "        print(\"\\n\".join(result))\n",
        "        time.sleep(5) # Wait for 5 seconds before checking again\n",
        "\n",
        "# After the jobs are finished, you can view the output files\n",
        "!cat array_output_*.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtuEXWDmLHHF",
        "outputId": "ddf96230-4d1a-4058-90ac-3f06d5e2a112"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for jobs to finish...\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "           6_[0-3]    LocalQ simple_a     root PD       0:00      1 (Resources)\n",
            "                 5    LocalQ srun job     root  R       0:05      1 localhost\n",
            "Waiting for jobs to finish...\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "           6_[1-3]    LocalQ simple_a     root PD       0:00      1 (Resources)\n",
            "               6_0    LocalQ simple_a     root  R       0:00      1 localhost\n",
            "Waiting for jobs to finish...\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "           6_[2-3]    LocalQ simple_a     root PD       0:00      1 (Resources)\n",
            "               6_1    LocalQ simple_a     root  R       0:00      1 localhost\n",
            "Waiting for jobs to finish...\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "               6_3    LocalQ simple_a     root PD       0:00      1 (Resources)\n",
            "               6_2    LocalQ simple_a     root  R       0:00      1 localhost\n",
            "Waiting for jobs to finish...\n",
            "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
            "               6_3    LocalQ simple_a     root  R       0:00      1 localhost\n",
            "All jobs finished.\n",
            "This is array task 0 of job 7\n",
            "Running on node: localhost\n",
            "This is array task 1 of job 8\n",
            "Running on node: localhost\n",
            "This is array task 2 of job 9\n",
            "Running on node: localhost\n",
            "This is array task 3 of job 6\n",
            "Running on node: localhost\n"
          ]
        }
      ]
    }
  ]
}