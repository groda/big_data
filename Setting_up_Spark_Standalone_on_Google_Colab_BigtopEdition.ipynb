{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DeF-OtwDrXVY",
        "SCoxn5xDimni",
        "JaaWPSEqsAyO",
        "m8hiZOGUr2FD",
        "7WND_44npS_d",
        "n8q4BIe316Ae",
        "ewBl-o1aH5hh",
        "LoPO2VrqEVCk",
        "AZuz6m48UyoO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Setting_up_Spark_Standalone_on_Google_Colab_BigtopEdition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\" alt=\"Logo Big Data for Beginners\"></div></a>\n",
        "# Install and run Spark in standalone modeâ€”Apache Bigtop edition <div><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></div>\n",
        "\n",
        "<br>\n",
        "\n",
        "We will install Apache Spark on a single machine (the virtual machine hosting this notebook) in _standalone mode_, meaning it will run without any cluster manager like YARN, Mesos, or Kubernetes. For more information, see the [types of cluster managers supported by Spark](https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)).\n",
        "\n",
        "We're following the official [Spark Standalone documentation](https://spark.apache.org/docs/latest/spark-standalone.html), using Apache Bigtop's Spark distribution, which conveniently packages Spark's start scripts as services.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Before running this notebook, you may want to update the Bigtop version (currently version `3.3.0` [with Hadoop 3.3.5](https://bigtop.apache.org/release-notes.html), see also the [full list of releases](https://bigtop.apache.org/download.html))."
      ],
      "metadata": {
        "id": "DeF-OtwDrXVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A side note:"
      ],
      "metadata": {
        "id": "Ayk0pz16sxMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://spark.apache.org/\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></a>\n",
        "<a href=\"https://bigtop.apache.org/\"><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"></a>\n",
        "\n",
        "\n",
        "I recently discovered [a website](https://www.apache.org/logos/) where you can find all Apache project logos, including Spark, with transparent backgrounds. Itâ€™s a great resource for anyone needing these assets for presentations or documentation. <p>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uribYroGLPlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "Set `JAVA_HOME` to Java 17. Paths work in Colab and in Github's `ubuntu-22.4` runner."
      ],
      "metadata": {
        "id": "vkRc2nNl6_pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'\n",
        "else:\n",
        "  # https://github.com/actions/runner-images/blob/main/images/ubuntu/Ubuntu2204-Readme.md#java\n",
        "  os.environ['JAVA_HOME'] = os.environ['JAVA_HOME_17_X64']\n"
      ],
      "metadata": {
        "id": "kZqcAB7A6_2X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxzRLcrntHfo",
        "outputId": "5f3f2694-13ac-4b13-ffcc-5ae4f9947cef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark from Bigtop repository"
      ],
      "metadata": {
        "id": "8XOex6ixsSml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** since the current underlying Machine in Colab runs Ubuntu `22.04`, our choice of Bigtop versions is limited to at most `3.3.0`, since only Ubuntu `24.04` is supported since Bigtop `3.4.0`.\n",
        "\n",
        "**Note 2:** Bigtop `3.3.0` includes Spark `3.3.4` (here is the [list of all libraries included in the release](https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+3.3.0+Release). Spark `3.3.4` [runs on Java 8/11/17](https://archive.apache.org/dist/spark/docs/3.3.4), so we do not need to install anything in Colab because it comes with Java 17 pre-installed."
      ],
      "metadata": {
        "id": "jp6rE6PTpgYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -rs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ-nBr7Ipgl3",
        "outputId": "86c45743-5657-4576-a6f0-09adbfe33ff9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Bigtop and Spark packages"
      ],
      "metadata": {
        "id": "oShuNuhgtJwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# 1. Use sudo to write the repo list\n",
        "echo \"Adding Bigtop repository...\"\n",
        "curl -s https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/$(lsb_release -is | tr '[:upper:]' '[:lower:]')-$(lsb_release -rs)/bigtop.list | sudo tee /etc/apt/sources.list.d/bigtop-3.3.0.list\n",
        "\n",
        "# 2. Add the GPG key (Updated to modern trusted.gpg.d method)\n",
        "echo \"Adding Bigtop GPG key...\"\n",
        "wget -qO - https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/GPG-KEY-bigtop | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/bigtop.gpg\n",
        "\n",
        "# 3. Use sudo for apt update\n",
        "echo \"Updating package cache...\"\n",
        "sudo apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFoSvMF9rlW",
        "outputId": "08b3c925-4c3c-47bd-82f0-597fa3679fbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Bigtop repository...\n",
            "deb http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/$(ARCH) bigtop contrib\n",
            "Adding Bigtop GPG key...\n",
            "Updating package cache...\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo 'List all available packages that match \"bigtop\"'\n",
        "sudo apt-get -qq update && sudo apt search bigtop\n",
        "\n",
        "echo 'List all available packages that match \"spark\"'\n",
        "sudo apt search spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNfPDnHqBE4p",
        "outputId": "ea117a5f-2547-4c1d-bc59-db7e3e1eb84c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all available packages that match \"bigtop\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "bigtop-groovy/stable,now 2.5.4-1 all [installed,automatic]\n",
            "  An agile and dynamic language for the Java Virtual Machine\n",
            "\n",
            "bigtop-jsvc/stable,now 1.2.4-1 amd64 [installed,automatic]\n",
            "  Application to launch java daemon\n",
            "\n",
            "bigtop-utils/stable,now 3.3.0-1 all [installed]\n",
            "  Collection of useful tools for Bigtop\n",
            "\n",
            "List all available packages that match \"spark\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "alluxio/stable 2.9.3-1 all\n",
            "  Reliable file sharing at memory speed across cluster frameworks\n",
            "\n",
            "libjs-jquery.sparkline/jammy 2.1.2-3 all\n",
            "  library for jQuery to generate sparklines\n",
            "\n",
            "libsparkline-php/jammy 0.2-7 all\n",
            "  sparkline graphing library for php\n",
            "\n",
            "livy/stable 0.8.0-1 all\n",
            "  Livy is an open source REST interface for interacting with Apache Spark from anywhere.\n",
            "\n",
            "node-sparkles/jammy 1.0.1-2 all\n",
            "  Namespaced global event emitter\n",
            "\n",
            "nspark/jammy 1.7.8B2+git20210317.cb30779-2 amd64\n",
            "  Unarchiver for Spark and ArcFS files\n",
            "\n",
            "pcp-export-pcp2spark/jammy 5.3.6-1build1 amd64\n",
            "  Tool for exporting data from PCP to Apache Spark\n",
            "\n",
            "python3-sahara-plugin-spark/jammy 7.0.0-0ubuntu1 all\n",
            "  OpenStack data processing cluster as a service - Spark plugin\n",
            "\n",
            "python3-sparkpost/jammy 1.3.10-1 all\n",
            "  SparkPost Python API client (Python 3)\n",
            "\n",
            "r-cran-analysispipelines/jammy 1.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'analysisPipelines' (Compose Interoperable Analysis Pipelines & Put Them inProduction)\n",
            "\n",
            "r-cran-apache.sedona/jammy 1.8.1-1.ca2204.1 all\n",
            "  CRAN Package 'apache.sedona' (R Interface for Apache Sedona)\n",
            "\n",
            "r-cran-catalog/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'catalog' (Access the 'Spark Catalog' API via 'sparklyr')\n",
            "\n",
            "r-cran-databaseconnector/jammy 7.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'DatabaseConnector' (Connecting to Various Database Platforms)\n",
            "\n",
            "r-cran-fabricqueryr/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'fabricQueryR' (Query Data in 'Microsoft Fabric')\n",
            "\n",
            "r-cran-geospark/jammy 0.3.1-1.ca2204.1 all\n",
            "  CRAN Package 'geospark' (Bring Local Sf to Spark)\n",
            "\n",
            "r-cran-ggspark/jammy 0.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'ggspark' ('ggplot2' Functions to Create Tufte Style Sparklines)\n",
            "\n",
            "r-cran-graphframes/jammy 0.1.2-1.ca2204.1 all\n",
            "  CRAN Package 'graphframes' (Interface for 'GraphFrames')\n",
            "\n",
            "r-cran-hatchr/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'hatchR' (Predict Fish Hatch and Emergence Timing)\n",
            "\n",
            "r-cran-ibmdbr/jammy 1.51.0-1.ca2204.1 all\n",
            "  CRAN Package 'ibmdbR' (IBM in-Database Analytics for R)\n",
            "\n",
            "r-cran-ltxsparklines/jammy 1.1.3-1.ca2204.1 all\n",
            "  CRAN Package 'ltxsparklines' (Lightweight Sparklines for a LaTeX Document)\n",
            "\n",
            "r-cran-microplot/jammy 1.0-47-1.ca2204.1 all\n",
            "  CRAN Package 'microplot' (Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel')\n",
            "\n",
            "r-cran-notebookutils/jammy 1.6.2-1.ca2204.1 all\n",
            "  CRAN Package 'notebookutils' (Dummy R APIs Used in 'Azure Synapse Analytics' for LocalDevelopments)\n",
            "\n",
            "r-cran-oenokpm/jammy 2.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'OenoKPM' (Modeling the Kinetics of Carbon Dioxide Production in AlcoholicFermentation)\n",
            "\n",
            "r-cran-omoponspark/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'OmopOnSpark' (Using a Common Data Model on 'Spark')\n",
            "\n",
            "r-cran-parsnip/jammy 1.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'parsnip' (A Common API to Modeling and Analysis Functions)\n",
            "\n",
            "r-cran-paws.analytics/jammy 0.9.0-1.ca2204.1 all\n",
            "  CRAN Package 'paws.analytics' ('Amazon Web Services' Analytics Services)\n",
            "\n",
            "r-cran-pmmltransformations/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'pmmlTransformations' (Transforms Input Data from a PMML Perspective)\n",
            "\n",
            "r-cran-pointblank/jammy 0.12.3-1.ca2204.1 all\n",
            "  CRAN Package 'pointblank' (Data Validation and Organization of Metadata for Local andRemote Tables)\n",
            "\n",
            "r-cran-pysparklyr/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'pysparklyr' (Provides a 'PySpark' Back-End for the 'sparklyr' Package)\n",
            "\n",
            "r-cran-reactablefmtr/jammy 2.0.0-1.ca2204.1 all\n",
            "  CRAN Package 'reactablefmtr' (Streamlined Table Styling and Formatting for Reactable)\n",
            "\n",
            "r-cran-rquery/jammy 1.4.99-1.ca2204.1 all\n",
            "  CRAN Package 'rquery' (Relational Query Generator for Data Manipulation at Scale)\n",
            "\n",
            "r-cran-rsparkling/jammy 0.2.19-1.ca2204.1 all\n",
            "  CRAN Package 'rsparkling' (R Interface for H2O Sparkling Water)\n",
            "\n",
            "r-cran-s3.resourcer/jammy 1.1.2-1.ca2204.1 all\n",
            "  CRAN Package 's3.resourcer' (S3 Resource Resolver)\n",
            "\n",
            "r-cran-shinyml/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'shinyML' (Compare Supervised Machine Learning Models Using Shiny App)\n",
            "\n",
            "r-cran-skimr/jammy 2.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'skimr' (Compact and Flexible Summaries of Data)\n",
            "\n",
            "r-cran-spark.sas7bdat/jammy 1.4-1.ca2204.1 all\n",
            "  CRAN Package 'spark.sas7bdat' (Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkavro/jammy 0.3.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkavro' (Load Avro file into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkbq/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkbq' (Google 'BigQuery' Support for 'sparklyr')\n",
            "\n",
            "r-cran-sparkhail/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkhail' (A 'Sparklyr' Extension for 'Hail')\n",
            "\n",
            "r-cran-sparkline/jammy 2.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkline' ('jQuery' Sparkline 'htmlwidget')\n",
            "\n",
            "r-cran-sparklyr/jammy 1.9.3-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr' (R Interface to Apache Spark)\n",
            "\n",
            "r-cran-sparklyr.flint/jammy 0.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.flint' (Sparklyr Extension for 'Flint')\n",
            "\n",
            "r-cran-sparklyr.nested/jammy 0.0.4-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.nested' (A 'sparklyr' Extension for Nested Data)\n",
            "\n",
            "r-cran-sparktex/jammy 0.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparktex' (Generate LaTeX sparklines in R)\n",
            "\n",
            "r-cran-sparktf/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparktf' (Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark')\n",
            "\n",
            "r-cran-sparkwarc/jammy 0.1.6-1.ca2204.1 amd64\n",
            "  CRAN Package 'sparkwarc' (Load WARC Files into Apache Spark)\n",
            "\n",
            "r-cran-sparkxgb/jammy 0.2.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkxgb' (Interface for 'XGBoost' on 'Apache Spark')\n",
            "\n",
            "r-cran-sqlrender/jammy 1.19.4-1.ca2204.1 all\n",
            "  CRAN Package 'SqlRender' (Rendering Parameterized SQL and Translation to Dialects)\n",
            "\n",
            "r-cran-stddiff.spark/jammy 1.0-1.ca2204.1 all\n",
            "  CRAN Package 'stddiff.spark' (Calculate the Standardized Difference for Numeric, Binary andCategory Variables in Apache Spark)\n",
            "\n",
            "r-cran-svg/jammy 1.0.0-1.ca2204.1 amd64\n",
            "  CRAN Package 'SVG' (Spatially Variable Genes Detection Methods for SpatialTranscriptomics)\n",
            "\n",
            "r-cran-tidier/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'tidier' (Enhanced 'mutate')\n",
            "\n",
            "r-cran-timevizpro/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'TimeVizPro' (Dynamic Data Explorer: Visualize and Forecast with 'TimeVizPro')\n",
            "\n",
            "r-cran-variantspark/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'variantspark' (A 'Sparklyr' Extension for 'VariantSpark')\n",
            "\n",
            "spark-core/stable,now 3.3.4-1 all [installed]\n",
            "  Lightning-Fast Cluster Computing\n",
            "\n",
            "spark-datanucleus/stable 3.3.4-1 all\n",
            "  DataNucleus libraries for Apache Spark\n",
            "\n",
            "spark-external/stable 3.3.4-1 all\n",
            "  External libraries for Apache Spark\n",
            "\n",
            "spark-history-server/stable 3.3.4-1 all\n",
            "  History server for Apache Spark\n",
            "\n",
            "spark-master/stable,now 3.3.4-1 all [installed]\n",
            "  Server for Spark master\n",
            "\n",
            "spark-python/stable 3.3.4-1 all\n",
            "  Python client for Spark\n",
            "\n",
            "spark-sparkr/stable 3.3.4-1 all\n",
            "  R package for Apache Spark\n",
            "\n",
            "spark-thriftserver/stable 3.3.4-1 all\n",
            "  Thrift server for Spark SQL\n",
            "\n",
            "spark-worker/stable,now 3.3.4-1 all [installed]\n",
            "  Server for Spark worker\n",
            "\n",
            "spark-yarn-shuffle/stable 3.3.4-1 all\n",
            "  Spark YARN Shuffle Service\n",
            "\n",
            "sparkleshare/jammy 3.28+git20190525+cf446c0-3 all\n",
            "  distributed collaboration and sharing tool\n",
            "\n",
            "zeppelin/stable 0.11.0-1 all\n",
            "  Web-based notebook for data analysts\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the essential packages\n",
        "\n",
        "In order to run a Spark job, we need the core libraries as well as the Spark master and Spark worker. Master and worker in this case are going to run both on the same machine, the localhost.\n",
        "\n",
        "The package `bigtop-utils` will be used to start  the services."
      ],
      "metadata": {
        "id": "j8_0tUQ8tesJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-core spark-master spark-worker bigtop-utils; do\n",
        "  echo \"ðŸ› ï¸ Installing $p\"\n",
        "  sudo apt install -qq -y $p\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCNjbZUe-69F",
        "outputId": "2b326415-f36c-48ac-a716-fda2b8d14a34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ› ï¸ Installing spark-core\n",
            "spark-core is already the newest version (3.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n",
            "ðŸ› ï¸ Installing spark-master\n",
            "spark-master is already the newest version (3.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n",
            "ðŸ› ï¸ Installing spark-worker\n",
            "spark-worker is already the newest version (3.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n",
            "ðŸ› ï¸ Installing bigtop-utils\n",
            "bigtop-utils is already the newest version (3.3.0-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note;** in a future version of this notebook we are going to use an alternative to `apt` for installing packages in order to avoid the warning\n",
        "\n",
        "```\n",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "IFxPmikEu0An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Spark services\n",
        "\n",
        "Thanks to the Bigtop's utilities, we can now start the Spark master and Spark worker as services. Normally one would use `systemctl` but since this is not allowed on Colab, we are going to resort to `service`."
      ],
      "metadata": {
        "id": "3jx-t17PvHGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-master spark-worker; do\n",
        "  echo \"Starting $p\"\n",
        "  # systemctl start $p\n",
        "  sudo service $p start\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6lMkulx-X2I",
        "outputId": "06ed3484-be59-4e1c-a123-1fea0b7d5a4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting spark-master\n",
            " * Starting Spark master (spark-master): \n",
            " * Spark master is running\n",
            "Starting spark-worker\n",
            " * Starting Spark worker (spark-worker): \n",
            " * Spark worker is running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the `pi` example"
      ],
      "metadata": {
        "id": "6erQkYR6v7nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step may take some time.\n",
        "\n",
        "We'll run the `SparkPi` demo from the examples included in the Spark distribution, which are packaged in the `spark-examples*.jar` file.\n",
        "\n",
        "We'll submit the job using [`spark-submit`](https://spark.apache.org/docs/latest/submitting-applications.html), and the output will be an approximation of Ï€ (for more details, see the [official Spark examples](https://spark.apache.org/examples.html).\n",
        "\n",
        "\n",
        "The following code defines the variable `$EXAMPLE_JAR`, which points to the archive containing all the examples from the Spark distribution.\n",
        "\n",
        "The following command submits the SparkPi application (located in the `org.apache.spark.examples.SparkPi` class) to the Spark master at `spark://${HOSTNAME}:7077` using `spark-submit`:\n",
        "\n",
        "```\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100\n",
        "```\n",
        "\n",
        "In this example, the number $100$ represents the number of iterations used to compute an approximation of Ï€ by calculating the ratio of points inside versus outside the unit circle."
      ],
      "metadata": {
        "id": "DyfvrPtVVCfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRKc5yAHoEsH",
        "outputId": "7dab65b9-3e40-4757-811b-daabdd193447"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/spark/examples/jars/spark-examples.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export EXAMPLES_JAR=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100"
      ],
      "metadata": {
        "id": "a4COXhlrR1mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb97f1e-1f4b-4e1e-a799-676614d5b47e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/02/22 20:08:41 INFO SparkContext: Running Spark version 3.3.4\n",
            "26/02/22 20:08:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/02/22 20:08:41 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 20:08:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "26/02/22 20:08:41 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 20:08:41 INFO SparkContext: Submitted application: Spark Pi\n",
            "26/02/22 20:08:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "26/02/22 20:08:41 INFO ResourceProfile: Limiting resource is cpu\n",
            "26/02/22 20:08:41 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "26/02/22 20:08:41 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 20:08:41 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 20:08:41 INFO SecurityManager: Changing view acls groups to: \n",
            "26/02/22 20:08:41 INFO SecurityManager: Changing modify acls groups to: \n",
            "26/02/22 20:08:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "26/02/22 20:08:41 INFO Utils: Successfully started service 'sparkDriver' on port 42337.\n",
            "26/02/22 20:08:41 INFO SparkEnv: Registering MapOutputTracker\n",
            "26/02/22 20:08:42 INFO SparkEnv: Registering BlockManagerMaster\n",
            "26/02/22 20:08:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "26/02/22 20:08:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "26/02/22 20:08:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "26/02/22 20:08:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5411ecb9-befc-4f1d-9458-5352adb3ad7c\n",
            "26/02/22 20:08:42 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "26/02/22 20:08:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "26/02/22 20:08:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "26/02/22 20:08:42 INFO SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.3.4.jar at spark://090e06d941c5:42337/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771790921012\n",
            "26/02/22 20:08:42 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://090e06d941c5:7077...\n",
            "26/02/22 20:08:42 INFO TransportClientFactory: Successfully created connection to 090e06d941c5/172.28.0.12:7077 after 58 ms (0 ms spent in bootstraps)\n",
            "26/02/22 20:08:42 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260222200842-0002\n",
            "26/02/22 20:08:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260222200842-0002/0 on worker-20260222192519-172.28.0.12-7078 (172.28.0.12:7078) with 2 core(s)\n",
            "26/02/22 20:08:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20260222200842-0002/0 on hostPort 172.28.0.12:7078 with 2 core(s), 1024.0 MiB RAM\n",
            "26/02/22 20:08:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37361.\n",
            "26/02/22 20:08:43 INFO NettyBlockTransferService: Server created on 090e06d941c5:37361\n",
            "26/02/22 20:08:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "26/02/22 20:08:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 090e06d941c5, 37361, None)\n",
            "26/02/22 20:08:43 INFO BlockManagerMasterEndpoint: Registering block manager 090e06d941c5:37361 with 434.4 MiB RAM, BlockManagerId(driver, 090e06d941c5, 37361, None)\n",
            "26/02/22 20:08:43 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260222200842-0002/0 is now RUNNING\n",
            "26/02/22 20:08:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 090e06d941c5, 37361, None)\n",
            "26/02/22 20:08:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 090e06d941c5, 37361, None)\n",
            "26/02/22 20:08:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
            "26/02/22 20:08:46 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
            "26/02/22 20:08:46 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 100 output partitions\n",
            "26/02/22 20:08:46 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
            "26/02/22 20:08:46 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:08:46 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:08:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
            "26/02/22 20:08:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)\n",
            "26/02/22 20:08:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)\n",
            "26/02/22 20:08:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 090e06d941c5:37361 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "26/02/22 20:08:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509\n",
            "26/02/22 20:08:47 INFO DAGScheduler: Submitting 100 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
            "26/02/22 20:08:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 100 tasks resource profile 0\n",
            "26/02/22 20:08:50 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:43260) with ID 0,  ResourceProfileId 0\n",
            "26/02/22 20:08:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.28.0.12:36447 with 434.4 MiB RAM, BlockManagerId(0, 172.28.0.12, 36447, None)\n",
            "26/02/22 20:08:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.28.0.12, executor 0, partition 0, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.28.0.12, executor 0, partition 1, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.28.0.12:36447 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.28.0.12, executor 0, partition 2, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.28.0.12, executor 0, partition 3, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1340 ms on 172.28.0.12 (executor 0) (1/100)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1293 ms on 172.28.0.12 (executor 0) (2/100)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.28.0.12, executor 0, partition 4, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 154 ms on 172.28.0.12 (executor 0) (3/100)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.28.0.12, executor 0, partition 5, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 180 ms on 172.28.0.12 (executor 0) (4/100)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.28.0.12, executor 0, partition 6, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 161 ms on 172.28.0.12 (executor 0) (5/100)\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.28.0.12, executor 0, partition 7, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:52 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 127 ms on 172.28.0.12 (executor 0) (6/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.28.0.12, executor 0, partition 8, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 83 ms on 172.28.0.12 (executor 0) (7/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.28.0.12, executor 0, partition 9, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 105 ms on 172.28.0.12 (executor 0) (8/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.28.0.12, executor 0, partition 10, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 137 ms on 172.28.0.12 (executor 0) (9/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.28.0.12, executor 0, partition 11, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 184 ms on 172.28.0.12 (executor 0) (10/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.28.0.12, executor 0, partition 12, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 146 ms on 172.28.0.12 (executor 0) (11/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.28.0.12, executor 0, partition 13, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 108 ms on 172.28.0.12 (executor 0) (12/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.28.0.12, executor 0, partition 14, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 100 ms on 172.28.0.12 (executor 0) (13/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.28.0.12, executor 0, partition 15, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 137 ms on 172.28.0.12 (executor 0) (14/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.28.0.12, executor 0, partition 16, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 84 ms on 172.28.0.12 (executor 0) (15/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.28.0.12, executor 0, partition 17, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 105 ms on 172.28.0.12 (executor 0) (16/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.28.0.12, executor 0, partition 18, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 73 ms on 172.28.0.12 (executor 0) (17/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.28.0.12, executor 0, partition 19, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 83 ms on 172.28.0.12 (executor 0) (18/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 65 ms on 172.28.0.12 (executor 0) (19/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.28.0.12, executor 0, partition 20, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.28.0.12, executor 0, partition 21, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 66 ms on 172.28.0.12 (executor 0) (20/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.28.0.12, executor 0, partition 22, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 74 ms on 172.28.0.12 (executor 0) (21/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.28.0.12, executor 0, partition 23, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 64 ms on 172.28.0.12 (executor 0) (22/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.28.0.12, executor 0, partition 24, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 45 ms on 172.28.0.12 (executor 0) (23/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.28.0.12, executor 0, partition 25, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 78 ms on 172.28.0.12 (executor 0) (24/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.28.0.12, executor 0, partition 26, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 56 ms on 172.28.0.12 (executor 0) (25/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.28.0.12, executor 0, partition 27, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 65 ms on 172.28.0.12 (executor 0) (26/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.28.0.12, executor 0, partition 28, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 59 ms on 172.28.0.12 (executor 0) (27/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.28.0.12, executor 0, partition 29, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 119 ms on 172.28.0.12 (executor 0) (28/100)\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.28.0.12, executor 0, partition 30, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:53 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 132 ms on 172.28.0.12 (executor 0) (29/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.28.0.12, executor 0, partition 31, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 50 ms on 172.28.0.12 (executor 0) (30/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.28.0.12, executor 0, partition 32, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 100 ms on 172.28.0.12 (executor 0) (31/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.28.0.12, executor 0, partition 33, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.28.0.12, executor 0, partition 34, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 67 ms on 172.28.0.12 (executor 0) (32/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 90 ms on 172.28.0.12 (executor 0) (33/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.28.0.12, executor 0, partition 35, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 51 ms on 172.28.0.12 (executor 0) (34/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.28.0.12, executor 0, partition 36, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 57 ms on 172.28.0.12 (executor 0) (35/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.28.0.12, executor 0, partition 37, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 55 ms on 172.28.0.12 (executor 0) (36/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.28.0.12, executor 0, partition 38, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 61 ms on 172.28.0.12 (executor 0) (37/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.28.0.12, executor 0, partition 39, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 58 ms on 172.28.0.12 (executor 0) (38/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.28.0.12, executor 0, partition 40, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 63 ms on 172.28.0.12 (executor 0) (39/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.28.0.12, executor 0, partition 41, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 46 ms on 172.28.0.12 (executor 0) (40/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.28.0.12, executor 0, partition 42, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 63 ms on 172.28.0.12 (executor 0) (41/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.28.0.12, executor 0, partition 43, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 71 ms on 172.28.0.12 (executor 0) (42/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.28.0.12, executor 0, partition 44, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 52 ms on 172.28.0.12 (executor 0) (43/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.28.0.12, executor 0, partition 45, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.28.0.12, executor 0, partition 46, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 78 ms on 172.28.0.12 (executor 0) (44/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 63 ms on 172.28.0.12 (executor 0) (45/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.28.0.12, executor 0, partition 47, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 78 ms on 172.28.0.12 (executor 0) (46/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.28.0.12, executor 0, partition 48, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 80 ms on 172.28.0.12 (executor 0) (47/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.28.0.12, executor 0, partition 49, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 65 ms on 172.28.0.12 (executor 0) (48/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.28.0.12, executor 0, partition 50, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 65 ms on 172.28.0.12 (executor 0) (49/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.28.0.12, executor 0, partition 51, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 67 ms on 172.28.0.12 (executor 0) (50/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.28.0.12, executor 0, partition 52, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 66 ms on 172.28.0.12 (executor 0) (51/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.28.0.12, executor 0, partition 53, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 49 ms on 172.28.0.12 (executor 0) (52/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.28.0.12, executor 0, partition 54, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 60 ms on 172.28.0.12 (executor 0) (53/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.28.0.12, executor 0, partition 55, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 54 ms on 172.28.0.12 (executor 0) (54/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.28.0.12, executor 0, partition 56, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 63 ms on 172.28.0.12 (executor 0) (55/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.28.0.12, executor 0, partition 57, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 60 ms on 172.28.0.12 (executor 0) (56/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.28.0.12, executor 0, partition 58, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 59 ms on 172.28.0.12 (executor 0) (57/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.28.0.12, executor 0, partition 59, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 48 ms on 172.28.0.12 (executor 0) (58/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.28.0.12, executor 0, partition 60, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 46 ms on 172.28.0.12 (executor 0) (59/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.28.0.12, executor 0, partition 61, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 49 ms on 172.28.0.12 (executor 0) (60/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.28.0.12, executor 0, partition 62, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 49 ms on 172.28.0.12 (executor 0) (61/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.28.0.12, executor 0, partition 63, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 47 ms on 172.28.0.12 (executor 0) (62/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.28.0.12, executor 0, partition 64, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 54 ms on 172.28.0.12 (executor 0) (63/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.28.0.12, executor 0, partition 65, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 42 ms on 172.28.0.12 (executor 0) (64/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 56 ms on 172.28.0.12 (executor 0) (65/100)\n",
            "26/02/22 20:08:54 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.28.0.12, executor 0, partition 66, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.28.0.12, executor 0, partition 67, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 46 ms on 172.28.0.12 (executor 0) (66/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.28.0.12, executor 0, partition 68, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 46 ms on 172.28.0.12 (executor 0) (67/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.28.0.12, executor 0, partition 69, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 51 ms on 172.28.0.12 (executor 0) (68/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.28.0.12, executor 0, partition 70, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 50 ms on 172.28.0.12 (executor 0) (69/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.28.0.12, executor 0, partition 71, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 48 ms on 172.28.0.12 (executor 0) (70/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.28.0.12, executor 0, partition 72, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 48 ms on 172.28.0.12 (executor 0) (71/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.28.0.12, executor 0, partition 73, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 64 ms on 172.28.0.12 (executor 0) (72/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.28.0.12, executor 0, partition 74, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 63 ms on 172.28.0.12 (executor 0) (73/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.28.0.12, executor 0, partition 75, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 46 ms on 172.28.0.12 (executor 0) (74/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (172.28.0.12, executor 0, partition 76, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 50 ms on 172.28.0.12 (executor 0) (75/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (172.28.0.12, executor 0, partition 77, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 45 ms on 172.28.0.12 (executor 0) (76/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (172.28.0.12, executor 0, partition 78, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 62 ms on 172.28.0.12 (executor 0) (77/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (172.28.0.12, executor 0, partition 79, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 63 ms on 172.28.0.12 (executor 0) (78/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (172.28.0.12, executor 0, partition 80, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 41 ms on 172.28.0.12 (executor 0) (79/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (172.28.0.12, executor 0, partition 81, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 78 ms on 172.28.0.12 (executor 0) (80/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (172.28.0.12, executor 0, partition 82, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 85 ms on 172.28.0.12 (executor 0) (81/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (172.28.0.12, executor 0, partition 83, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 75 ms on 172.28.0.12 (executor 0) (82/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (172.28.0.12, executor 0, partition 84, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 82 ms on 172.28.0.12 (executor 0) (83/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (172.28.0.12, executor 0, partition 85, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 75 ms on 172.28.0.12 (executor 0) (84/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (172.28.0.12, executor 0, partition 86, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 86 ms on 172.28.0.12 (executor 0) (85/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (172.28.0.12, executor 0, partition 87, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 88 ms on 172.28.0.12 (executor 0) (86/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (172.28.0.12, executor 0, partition 88, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 83 ms on 172.28.0.12 (executor 0) (87/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (172.28.0.12, executor 0, partition 89, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 64 ms on 172.28.0.12 (executor 0) (88/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (172.28.0.12, executor 0, partition 90, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 68 ms on 172.28.0.12 (executor 0) (89/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (172.28.0.12, executor 0, partition 91, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 67 ms on 172.28.0.12 (executor 0) (90/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (172.28.0.12, executor 0, partition 92, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 61 ms on 172.28.0.12 (executor 0) (91/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (172.28.0.12, executor 0, partition 93, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 52 ms on 172.28.0.12 (executor 0) (92/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (172.28.0.12, executor 0, partition 94, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 59 ms on 172.28.0.12 (executor 0) (93/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (172.28.0.12, executor 0, partition 95, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 54 ms on 172.28.0.12 (executor 0) (94/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (172.28.0.12, executor 0, partition 96, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 64 ms on 172.28.0.12 (executor 0) (95/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (172.28.0.12, executor 0, partition 97, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 58 ms on 172.28.0.12 (executor 0) (96/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (172.28.0.12, executor 0, partition 98, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 42 ms on 172.28.0.12 (executor 0) (97/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (172.28.0.12, executor 0, partition 99, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 33 ms on 172.28.0.12 (executor 0) (98/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 62 ms on 172.28.0.12 (executor 0) (99/100)\n",
            "26/02/22 20:08:55 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 37 ms on 172.28.0.12 (executor 0) (100/100)\n",
            "26/02/22 20:08:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "26/02/22 20:08:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 9.684 s\n",
            "26/02/22 20:08:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:08:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "26/02/22 20:08:56 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 9.994217 s\n",
            "Pi is roughly 3.1417483141748312\n",
            "26/02/22 20:08:56 INFO SparkUI: Stopped Spark web UI at http://090e06d941c5:4040\n",
            "26/02/22 20:08:56 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
            "26/02/22 20:08:56 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
            "26/02/22 20:08:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "26/02/22 20:08:56 INFO MemoryStore: MemoryStore cleared\n",
            "26/02/22 20:08:56 INFO BlockManager: BlockManager stopped\n",
            "26/02/22 20:08:56 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "26/02/22 20:08:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "26/02/22 20:08:56 INFO SparkContext: Successfully stopped SparkContext\n",
            "26/02/22 20:08:56 INFO ShutdownHookManager: Shutdown hook called\n",
            "26/02/22 20:08:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-12e09104-d37f-4118-895c-a30b62c94e88\n",
            "26/02/22 20:08:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-d091d5ca-4dca-4d6f-9318-9e4a7a8bf24e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Java Random Forest Regressor example\n",
        "\n",
        "Next, we will run the Java Random Forest Regressor example. Source: [JavaRandomForestRegressorExample.java](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java)."
      ],
      "metadata": {
        "id": "mLsH_Xpdyl97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        " j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        " echo \"Jar file containing examples: $j\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8JwPHSSJCJ8",
        "outputId": "8e0344bf-3d83-474a-aa4f-bdc95a17bd8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jar file containing examples: /usr/lib/spark/examples/jars/spark-examples.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run\n",
        "\n",
        "```\n",
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j\n",
        "```\n",
        "\n",
        "you'll get an error message telling you that the file `/content/data/mllib/sample_libsvm_data.txt` is missing. We are just going to create this file, but first we need to find it!"
      ],
      "metadata": {
        "id": "d1idj6w9y493"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name 'sample_libsvm_data.txt' 2> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS1eO8PtLHwl",
        "outputId": "d867378f-7d85-4d2f-e6c9-f83539ec1da5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/data/mllib/sample_libsvm_data.txt\n",
            "/usr/lib/spark/data/mllib/sample_libsvm_data.txt\n",
            "/content/data/mllib/sample_libsvm_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the datafile to the desired location"
      ],
      "metadata": {
        "id": "ADBuYvjezWZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p data/mllib/\n",
        "cp /usr/lib/spark/data/mllib/sample_libsvm_data.txt data/mllib/sample_libsvm_data.txt"
      ],
      "metadata": {
        "id": "E5vwn1iNMHKQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the JavaRandomForestRegressorExample example."
      ],
      "metadata": {
        "id": "snh_A1DFzj7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuvY9Pt_MUcw",
        "outputId": "4cb69703-d757-4f0f-e900-0769b6bcab39"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.1|  0.0|(692,[100,101,102...|\n",
            "|       0.0|  0.0|(692,[121,122,123...|\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "Root Mean Squared Error (RMSE) on test data = 0.1742631546220357\n",
            "Learned regression forest model:\n",
            "RandomForestRegressionModel: uid=rfr_27bd9836b36c, numTrees=20, numFeatures=692\n",
            "  Tree 0 (weight 1.0):\n",
            "    If (feature 378 <= 126.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 378 > 126.0)\n",
            "     Predict: 1.0\n",
            "  Tree 1 (weight 1.0):\n",
            "    If (feature 517 <= 50.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 517 > 50.0)\n",
            "     Predict: 1.0\n",
            "  Tree 2 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 3 (weight 1.0):\n",
            "    If (feature 378 <= 126.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 378 > 126.0)\n",
            "     Predict: 1.0\n",
            "  Tree 4 (weight 1.0):\n",
            "    If (feature 433 <= 52.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 52.5)\n",
            "     Predict: 1.0\n",
            "  Tree 5 (weight 1.0):\n",
            "    If (feature 490 <= 27.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 27.5)\n",
            "     Predict: 1.0\n",
            "  Tree 6 (weight 1.0):\n",
            "    If (feature 433 <= 52.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 52.5)\n",
            "     Predict: 1.0\n",
            "  Tree 7 (weight 1.0):\n",
            "    If (feature 517 <= 20.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 517 > 20.5)\n",
            "     Predict: 1.0\n",
            "  Tree 8 (weight 1.0):\n",
            "    If (feature 568 <= 73.5)\n",
            "     Predict: 1.0\n",
            "    Else (feature 568 > 73.5)\n",
            "     Predict: 0.0\n",
            "  Tree 9 (weight 1.0):\n",
            "    If (feature 490 <= 27.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 27.5)\n",
            "     Predict: 1.0\n",
            "  Tree 10 (weight 1.0):\n",
            "    If (feature 568 <= 105.0)\n",
            "     Predict: 1.0\n",
            "    Else (feature 568 > 105.0)\n",
            "     Predict: 0.0\n",
            "  Tree 11 (weight 1.0):\n",
            "    If (feature 455 <= 24.5)\n",
            "     If (feature 457 <= 5.0)\n",
            "      Predict: 1.0\n",
            "     Else (feature 457 > 5.0)\n",
            "      Predict: 0.0\n",
            "    Else (feature 455 > 24.5)\n",
            "     Predict: 0.0\n",
            "  Tree 12 (weight 1.0):\n",
            "    If (feature 490 <= 27.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 27.5)\n",
            "     Predict: 1.0\n",
            "  Tree 13 (weight 1.0):\n",
            "    If (feature 490 <= 27.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 27.5)\n",
            "     Predict: 1.0\n",
            "  Tree 14 (weight 1.0):\n",
            "    If (feature 435 <= 42.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 435 > 42.5)\n",
            "     Predict: 1.0\n",
            "  Tree 15 (weight 1.0):\n",
            "    If (feature 433 <= 52.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 52.5)\n",
            "     Predict: 1.0\n",
            "  Tree 16 (weight 1.0):\n",
            "    If (feature 462 <= 63.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 462 > 63.0)\n",
            "     Predict: 1.0\n",
            "  Tree 17 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 18 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 19 (weight 1.0):\n",
            "    If (feature 462 <= 63.0)\n",
            "     Predict: 0.0\n",
            "    Else (feature 462 > 63.0)\n",
            "     Predict: 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "26/02/22 20:09:05 INFO SparkContext: Running Spark version 4.0.2\n",
            "26/02/22 20:09:05 INFO SparkContext: OS info Linux, 6.6.113+, amd64\n",
            "26/02/22 20:09:05 INFO SparkContext: Java version 17.0.17\n",
            "26/02/22 20:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/02/22 20:09:05 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 20:09:05 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "26/02/22 20:09:05 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 20:09:05 INFO SparkContext: Submitted application: JavaRandomForestRegressorExample\n",
            "26/02/22 20:09:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "26/02/22 20:09:05 INFO ResourceProfile: Limiting resource is cpu\n",
            "26/02/22 20:09:05 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "26/02/22 20:09:05 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 20:09:05 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 20:09:05 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 20:09:05 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 20:09:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 20:09:05 INFO Utils: Successfully started service 'sparkDriver' on port 39841.\n",
            "26/02/22 20:09:06 INFO SparkEnv: Registering MapOutputTracker\n",
            "26/02/22 20:09:06 INFO SparkEnv: Registering BlockManagerMaster\n",
            "26/02/22 20:09:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "26/02/22 20:09:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "26/02/22 20:09:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "26/02/22 20:09:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-615bdc4c-5455-496b-a29f-572b1b9cbcb2\n",
            "26/02/22 20:09:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "26/02/22 20:09:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "26/02/22 20:09:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "26/02/22 20:09:06 INFO SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.3.4.jar at spark://090e06d941c5:39841/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771790945001\n",
            "26/02/22 20:09:06 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 20:09:06 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 20:09:06 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 20:09:06 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 20:09:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 20:09:06 INFO Executor: Starting executor ID driver on host 090e06d941c5\n",
            "26/02/22 20:09:06 INFO Executor: OS info Linux, 6.6.113+, amd64\n",
            "26/02/22 20:09:06 INFO Executor: Java version 17.0.17\n",
            "26/02/22 20:09:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "26/02/22 20:09:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@286866cb for default.\n",
            "26/02/22 20:09:06 INFO Executor: Fetching spark://090e06d941c5:39841/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771790945001\n",
            "26/02/22 20:09:07 INFO TransportClientFactory: Successfully created connection to 090e06d941c5/172.28.0.12:39841 after 52 ms (0 ms spent in bootstraps)\n",
            "26/02/22 20:09:07 INFO Utils: Fetching spark://090e06d941c5:39841/jars/spark-examples_2.12-3.3.4.jar to /tmp/spark-4f9f3c3b-cf54-4e04-9a73-9f5f4050a0e6/userFiles-b9743849-e193-475f-b6ec-d39f7c5c42d6/fetchFileTemp6946992673742368388.tmp\n",
            "26/02/22 20:09:07 INFO Executor: Adding file:/tmp/spark-4f9f3c3b-cf54-4e04-9a73-9f5f4050a0e6/userFiles-b9743849-e193-475f-b6ec-d39f7c5c42d6/spark-examples_2.12-3.3.4.jar to class loader default\n",
            "26/02/22 20:09:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38975.\n",
            "26/02/22 20:09:07 INFO NettyBlockTransferService: Server created on 090e06d941c5:38975\n",
            "26/02/22 20:09:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "26/02/22 20:09:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 090e06d941c5, 38975, None)\n",
            "26/02/22 20:09:07 INFO BlockManagerMasterEndpoint: Registering block manager 090e06d941c5:38975 with 434.4 MiB RAM, BlockManagerId(driver, 090e06d941c5, 38975, None)\n",
            "26/02/22 20:09:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 090e06d941c5, 38975, None)\n",
            "26/02/22 20:09:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 090e06d941c5, 38975, None)\n",
            "26/02/22 20:09:09 WARN SparkSession: Failed to load session extension\n",
            "java.lang.NoClassDefFoundError: scala/Serializable\n",
            "\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n",
            "\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n",
            "\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)\n",
            "\tat java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:524)\n",
            "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:427)\n",
            "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:421)\n",
            "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:420)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
            "\tat org.apache.spark.examples.extensions.SessionExtensionsWithLoader.apply(SessionExtensionsWithLoader.scala:28)\n",
            "\tat org.apache.spark.examples.extensions.SessionExtensionsWithLoader.apply(SessionExtensionsWithLoader.scala:24)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$.org$apache$spark$sql$classic$SparkSession$$loadExtensions(SparkSession.scala:1083)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.build(SparkSession.scala:843)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:859)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:732)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:923)\n",
            "\tat org.apache.spark.examples.ml.JavaRandomForestRegressorExample.main(JavaRandomForestRegressorExample.java:39)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "Caused by: java.lang.ClassNotFoundException: scala.Serializable\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
            "\t... 30 more\n",
            "26/02/22 20:09:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "26/02/22 20:09:09 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "26/02/22 20:09:11 INFO InMemoryFileIndex: It took 98 ms to list leaf files for 1 paths.\n",
            "26/02/22 20:09:11 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
            "26/02/22 20:09:11 INFO InMemoryFileIndex: It took 8 ms to list leaf files for 1 paths.\n",
            "26/02/22 20:09:13 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 20:09:13 INFO FileSourceStrategy: Post-Scan Filters: Set(NOT (length(trim(value#0, None)) = 0), NOT StartsWith(trim(value#0, None), #))\n",
            "26/02/22 20:09:14 INFO CodeGenerator: Code generated in 465.723904 ms\n",
            "26/02/22 20:09:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "26/02/22 20:09:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 213.9 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:14 INFO SparkContext: Created broadcast 0 from rdd at MLUtils.scala:126\n",
            "26/02/22 20:09:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 20:09:15 INFO SparkContext: Starting job: reduce at MLUtils.scala:96\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Got job 0 (reduce at MLUtils.scala:96) with 1 output partitions\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at MLUtils.scala:96)\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:94), which has no missing parents\n",
            "26/02/22 20:09:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 27.0 KiB, free 434.1 MiB)\n",
            "26/02/22 20:09:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 434.1 MiB)\n",
            "26/02/22 20:09:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:94) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 20:09:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "26/02/22 20:09:15 INFO CodeGenerator: Code generated in 28.367391 ms\n",
            "26/02/22 20:09:15 INFO CodeGenerator: Code generated in 28.412417 ms\n",
            "26/02/22 20:09:15 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:15 INFO CodeGenerator: Code generated in 27.037799 ms\n",
            "26/02/22 20:09:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1769 bytes result sent to driver\n",
            "26/02/22 20:09:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 696 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:16 INFO DAGScheduler: ResultStage 0 (reduce at MLUtils.scala:96) finished in 959 ms\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:16 INFO TaskSchedulerImpl: Canceling stage 0\n",
            "26/02/22 20:09:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Job 0 finished: reduce at MLUtils.scala:96, took 1030.161404 ms\n",
            "26/02/22 20:09:16 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 20:09:16 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 20:09:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 213.7 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:16 INFO SparkContext: Created broadcast 2 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 20:09:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 20:09:16 INFO SparkContext: Starting job: treeReduce at VectorIndexer.scala:151\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Got job 1 (treeReduce at VectorIndexer.scala:151) with 1 output partitions\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Final stage: ResultStage 1 (treeReduce at VectorIndexer.scala:151)\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:151), which has no missing parents\n",
            "26/02/22 20:09:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.6 KiB, free 434.1 MiB)\n",
            "26/02/22 20:09:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 434.1 MiB)\n",
            "26/02/22 20:09:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:151) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 20:09:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "26/02/22 20:09:16 INFO CodeGenerator: Code generated in 50.512312 ms\n",
            "26/02/22 20:09:16 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:16 INFO CodeGenerator: Code generated in 25.53891 ms\n",
            "26/02/22 20:09:17 INFO CodeGenerator: Code generated in 47.417693 ms\n",
            "26/02/22 20:09:17 INFO CodeGenerator: Code generated in 99.041354 ms\n",
            "26/02/22 20:09:17 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 415401 bytes result sent to driver\n",
            "26/02/22 20:09:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 728 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:17 INFO DAGScheduler: ResultStage 1 (treeReduce at VectorIndexer.scala:151) finished in 813 ms\n",
            "26/02/22 20:09:17 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:17 INFO TaskSchedulerImpl: Canceling stage 1\n",
            "26/02/22 20:09:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "26/02/22 20:09:17 INFO DAGScheduler: Job 1 finished: treeReduce at VectorIndexer.scala:151, took 829.952108 ms\n",
            "26/02/22 20:09:18 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 20:09:18 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 20:09:18 INFO CodeGenerator: Code generated in 115.977117 ms\n",
            "26/02/22 20:09:18 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.7 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 434.2 MiB)\n",
            "26/02/22 20:09:18 INFO SparkContext: Created broadcast 4 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 20:09:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 20:09:18 INFO Instrumentation: [d0c87596] Stage class: RandomForestRegressor\n",
            "26/02/22 20:09:18 INFO Instrumentation: [d0c87596] Stage uid: rfr_27bd9836b36c\n",
            "26/02/22 20:09:18 INFO Instrumentation: [d0c87596] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)\n",
            "26/02/22 20:09:18 INFO Instrumentation: [d0c87596] {\"labelCol\":\"label\",\"featuresCol\":\"indexedFeatures\"}\n",
            "26/02/22 20:09:18 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Got job 2 (take at DecisionTreeMetadata.scala:119) with 1 output partitions\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Final stage: ResultStage 2 (take at DecisionTreeMetadata.scala:119)\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[26] at map at DecisionTreeMetadata.scala:119), which has no missing parents\n",
            "26/02/22 20:09:18 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 128.4 KiB, free 434.0 MiB)\n",
            "26/02/22 20:09:18 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 434.0 MiB)\n",
            "26/02/22 20:09:18 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[26] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 20:09:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "26/02/22 20:09:18 INFO CodeGenerator: Code generated in 82.613155 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 80.486307 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 29.823663 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 68.407468 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 19.485022 ms\n",
            "26/02/22 20:09:19 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 25.100692 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 13.162326 ms\n",
            "26/02/22 20:09:19 INFO CodeGenerator: Code generated in 27.952771 ms\n",
            "26/02/22 20:09:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1908 bytes result sent to driver\n",
            "26/02/22 20:09:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 763 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:19 INFO DAGScheduler: ResultStage 2 (take at DecisionTreeMetadata.scala:119) finished in 805 ms\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Canceling stage 2\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Job 2 finished: take at DecisionTreeMetadata.scala:119, took 815.49023 ms\n",
            "26/02/22 20:09:19 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Got job 3 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Final stage: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125)\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at retag at RandomForest.scala:276), which has no missing parents\n",
            "26/02/22 20:09:19 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 128.2 KiB, free 433.9 MiB)\n",
            "26/02/22 20:09:19 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 433.8 MiB)\n",
            "26/02/22 20:09:19 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at retag at RandomForest.scala:276) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:19 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 20:09:19 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "26/02/22 20:09:19 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:19 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1980 bytes result sent to driver\n",
            "26/02/22 20:09:19 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 234 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:19 INFO DAGScheduler: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125) finished in 258 ms\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Canceling stage 3\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Job 3 finished: aggregate at DecisionTreeMetadata.scala:125, took 265.731849 ms\n",
            "26/02/22 20:09:19 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1056\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Registering RDD 28 (flatMap at RandomForest.scala:1041) as input to shuffle 0\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Got job 4 (collectAsMap at RandomForest.scala:1056) with 1 output partitions\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Final stage: ResultStage 5 (collectAsMap at RandomForest.scala:1056)\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at flatMap at RandomForest.scala:1041), which has no missing parents\n",
            "26/02/22 20:09:19 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 136.0 KiB, free 433.7 MiB)\n",
            "26/02/22 20:09:19 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 42.3 KiB, free 433.7 MiB)\n",
            "26/02/22 20:09:19 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:19 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at flatMap at RandomForest.scala:1041) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:19 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:19 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 20:09:19 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "26/02/22 20:09:20 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 20:09:20 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 20:09:20 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 20:09:20 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 20:09:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 20:09:20 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:20 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2245 bytes result sent to driver\n",
            "26/02/22 20:09:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 865 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:20 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:20 INFO DAGScheduler: ShuffleMapStage 4 (flatMap at RandomForest.scala:1041) finished in 915 ms\n",
            "26/02/22 20:09:20 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 20:09:20 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 20:09:20 INFO DAGScheduler: waiting: HashSet(ResultStage 5)\n",
            "26/02/22 20:09:20 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 20:09:20 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[30] at map at RandomForest.scala:1056), which has no missing parents\n",
            "26/02/22 20:09:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.9 KiB, free 434.0 MiB)\n",
            "26/02/22 20:09:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.0 MiB)\n",
            "26/02/22 20:09:20 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[30] at map at RandomForest.scala:1056) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:20 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 20:09:20 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "26/02/22 20:09:20 INFO ShuffleBlockFetcherIterator: Getting 1 (82.3 KiB) non-empty blocks including 1 (82.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 20:09:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n",
            "26/02/22 20:09:21 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 120661 bytes result sent to driver\n",
            "26/02/22 20:09:21 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 406 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:21 INFO DAGScheduler: ResultStage 5 (collectAsMap at RandomForest.scala:1056) finished in 426 ms\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Canceling stage 5\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Job 4 finished: collectAsMap at RandomForest.scala:1056, took 1404.002408 ms\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 138.2 KiB, free 433.8 MiB)\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 30.4 KiB, free 433.8 MiB)\n",
            "26/02/22 20:09:21 INFO SparkContext: Created broadcast 9 from broadcast at RandomForest.scala:295\n",
            "26/02/22 20:09:21 INFO Instrumentation: [d0c87596] {\"numFeatures\":692}\n",
            "26/02/22 20:09:21 INFO Instrumentation: [d0c87596] {\"numClasses\":0}\n",
            "26/02/22 20:09:21 INFO Instrumentation: [d0c87596] {\"numExamples\":66}\n",
            "26/02/22 20:09:21 INFO Instrumentation: [d0c87596] {\"sumOfWeights\":66.0}\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.1 KiB, free 433.8 MiB)\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 433.8 MiB)\n",
            "26/02/22 20:09:21 INFO SparkContext: Created broadcast 10 from broadcast at RandomForest.scala:624\n",
            "26/02/22 20:09:21 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:665\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Registering RDD 33 (mapPartitions at RandomForest.scala:646) as input to shuffle 1\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Got job 5 (collectAsMap at RandomForest.scala:665) with 1 output partitions\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Final stage: ResultStage 7 (collectAsMap at RandomForest.scala:665)\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[33] at mapPartitions at RandomForest.scala:646), which has no missing parents\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 215.3 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 77.3 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:21 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[33] at mapPartitions at RandomForest.scala:646) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:21 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 20:09:21 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "26/02/22 20:09:21 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block rdd_32_0 stored as values in memory (estimated size 190.0 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:21 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2159 bytes result sent to driver\n",
            "26/02/22 20:09:21 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 317 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:21 INFO DAGScheduler: ShuffleMapStage 6 (mapPartitions at RandomForest.scala:646) finished in 377 ms\n",
            "26/02/22 20:09:21 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 20:09:21 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 20:09:21 INFO DAGScheduler: waiting: HashSet(ResultStage 7)\n",
            "26/02/22 20:09:21 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[35] at map at RandomForest.scala:665), which has no missing parents\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.4 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:21 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:21 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[35] at map at RandomForest.scala:665) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:21 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:21 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 20:09:21 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "26/02/22 20:09:21 INFO ShuffleBlockFetcherIterator: Getting 1 (343.8 KiB) non-empty blocks including 1 (343.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 20:09:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "26/02/22 20:09:22 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 6502 bytes result sent to driver\n",
            "26/02/22 20:09:22 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 503 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:22 INFO DAGScheduler: ResultStage 7 (collectAsMap at RandomForest.scala:665) finished in 517 ms\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Canceling stage 7\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Job 5 finished: collectAsMap at RandomForest.scala:665, took 913.228793 ms\n",
            "26/02/22 20:09:22 INFO TorrentBroadcast: Destroying Broadcast(10) (from destroy at RandomForest.scala:676)\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 1024.0 B, free 433.5 MiB)\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1188.0 B, free 433.5 MiB)\n",
            "26/02/22 20:09:22 INFO SparkContext: Created broadcast 13 from broadcast at RandomForest.scala:624\n",
            "26/02/22 20:09:22 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:665\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Registering RDD 36 (mapPartitions at RandomForest.scala:646) as input to shuffle 2\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Got job 6 (collectAsMap at RandomForest.scala:665) with 1 output partitions\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsMap at RandomForest.scala:665)\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[36] at mapPartitions at RandomForest.scala:646), which has no missing parents\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 197.3 KiB, free 433.3 MiB)\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 65.4 KiB, free 433.2 MiB)\n",
            "26/02/22 20:09:22 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[36] at mapPartitions at RandomForest.scala:646) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:22 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 20:09:22 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "26/02/22 20:09:22 INFO BlockManager: Found block rdd_32_0 locally\n",
            "26/02/22 20:09:22 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2116 bytes result sent to driver\n",
            "26/02/22 20:09:22 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 50 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:22 INFO DAGScheduler: ShuffleMapStage 8 (mapPartitions at RandomForest.scala:646) finished in 104 ms\n",
            "26/02/22 20:09:22 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 20:09:22 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 20:09:22 INFO DAGScheduler: waiting: HashSet(ResultStage 9)\n",
            "26/02/22 20:09:22 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[38] at map at RandomForest.scala:665), which has no missing parents\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 7.4 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:22 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[38] at map at RandomForest.scala:665) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:22 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 20:09:22 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "26/02/22 20:09:22 INFO ShuffleBlockFetcherIterator: Getting 1 (13.5 KiB) non-empty blocks including 1 (13.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 20:09:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "26/02/22 20:09:22 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2623 bytes result sent to driver\n",
            "26/02/22 20:09:22 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 50 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Removed TaskSet 9.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:22 INFO DAGScheduler: ResultStage 9 (collectAsMap at RandomForest.scala:665) finished in 64 ms\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Canceling stage 9\n",
            "26/02/22 20:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "26/02/22 20:09:22 INFO DAGScheduler: Job 6 finished: collectAsMap at RandomForest.scala:665, took 185.836494 ms\n",
            "26/02/22 20:09:22 INFO TorrentBroadcast: Destroying Broadcast(13) (from destroy at RandomForest.scala:676)\n",
            "26/02/22 20:09:22 INFO RandomForest: Internal timing for DecisionTree:\n",
            "26/02/22 20:09:22 INFO RandomForest:   init: 0.00417804\n",
            "  total: 1.246728155\n",
            "  findBestSplits: 1.220160511\n",
            "  chooseSplits: 1.210034005\n",
            "26/02/22 20:09:22 INFO MapPartitionsRDD: Removing RDD 32 from persistence list\n",
            "26/02/22 20:09:22 INFO BlockManager: Removing RDD 32\n",
            "26/02/22 20:09:22 INFO TorrentBroadcast: Destroying Broadcast(9) (from destroy at RandomForest.scala:307)\n",
            "26/02/22 20:09:22 INFO Instrumentation: [d0c87596] {\"numFeatures\":692}\n",
            "26/02/22 20:09:22 INFO Instrumentation: [d0c87596] training finished\n",
            "26/02/22 20:09:22 INFO Instrumentation: [a4e1522e] training finished\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 329.8 KiB, free 433.6 MiB)\n",
            "26/02/22 20:09:22 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:22 INFO SparkContext: Created broadcast 16 from broadcast at RandomForestRegressor.scala:242\n",
            "26/02/22 20:09:22 INFO Instrumentation: [384b56ca] training finished\n",
            "26/02/22 20:09:22 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 20:09:22 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 52.79687 ms\n",
            "26/02/22 20:09:23 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 213.7 KiB, free 433.3 MiB)\n",
            "26/02/22 20:09:23 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 433.6 MiB)\n",
            "26/02/22 20:09:23 INFO SparkContext: Created broadcast 17 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 20:09:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 20:09:23 INFO BlockManager: Removing RDD 32\n",
            "26/02/22 20:09:23 INFO SparkContext: Starting job: show at JavaRandomForestRegressorExample.java:74\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Got job 7 (show at JavaRandomForestRegressorExample.java:74) with 1 output partitions\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Final stage: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74)\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[42] at show at JavaRandomForestRegressorExample.java:74), which has no missing parents\n",
            "26/02/22 20:09:23 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 95.0 KiB, free 433.7 MiB)\n",
            "26/02/22 20:09:23 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.3 KiB, free 433.7 MiB)\n",
            "26/02/22 20:09:23 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[42] at show at JavaRandomForestRegressorExample.java:74) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:23 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:23 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10459 bytes) \n",
            "26/02/22 20:09:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "26/02/22 20:09:23 INFO Executor: Using REPL class URI: spark://090e06d941c5:39841/artifacts/dcb70d27-62a9-42c3-ba37-0ce09c5016e7/classes/\n",
            "26/02/22 20:09:23 INFO Executor: Created or updated repl class loader org.apache.spark.executor.ExecutorClassLoader@3904bea0 for dcb70d27-62a9-42c3-ba37-0ce09c5016e7.\n",
            "26/02/22 20:09:23 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 146.451287 ms\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 111.232344 ms\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 81.75923 ms\n",
            "26/02/22 20:09:23 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 41.26231 ms\n",
            "26/02/22 20:09:23 INFO CodeGenerator: Code generated in 72.365265 ms\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 29.158574 ms\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 43.541399 ms\n",
            "26/02/22 20:09:24 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 7182 bytes result sent to driver\n",
            "26/02/22 20:09:24 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 925 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:24 INFO DAGScheduler: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74) finished in 953 ms\n",
            "26/02/22 20:09:24 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:24 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:24 INFO TaskSchedulerImpl: Canceling stage 10\n",
            "26/02/22 20:09:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Job 7 finished: show at JavaRandomForestRegressorExample.java:74, took 977.517711 ms\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 36.970466 ms\n",
            "26/02/22 20:09:24 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 20:09:24 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 74.76776 ms\n",
            "26/02/22 20:09:24 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 213.7 KiB, free 433.5 MiB)\n",
            "26/02/22 20:09:24 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 433.4 MiB)\n",
            "26/02/22 20:09:24 INFO SparkContext: Created broadcast 19 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 20:09:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 20:09:24 INFO SparkContext: Starting job: treeAggregate at Statistics.scala:58\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Got job 8 (treeAggregate at Statistics.scala:58) with 1 output partitions\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Final stage: ResultStage 11 (treeAggregate at Statistics.scala:58)\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[51] at treeAggregate at Statistics.scala:58), which has no missing parents\n",
            "26/02/22 20:09:24 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 102.0 KiB, free 433.3 MiB)\n",
            "26/02/22 20:09:24 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.2 KiB, free 433.4 MiB)\n",
            "26/02/22 20:09:24 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 20:09:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 20:09:24 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "26/02/22 20:09:24 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 20:09:24 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 53.642678 ms\n",
            "26/02/22 20:09:24 INFO CodeGenerator: Code generated in 24.013034 ms\n",
            "26/02/22 20:09:24 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 20:09:25 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3509 bytes result sent to driver\n",
            "26/02/22 20:09:25 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 345 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 20:09:25 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool \n",
            "26/02/22 20:09:25 INFO DAGScheduler: ResultStage 11 (treeAggregate at Statistics.scala:58) finished in 408 ms\n",
            "26/02/22 20:09:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 20:09:25 INFO TaskSchedulerImpl: Canceling stage 11\n",
            "26/02/22 20:09:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "26/02/22 20:09:25 INFO DAGScheduler: Job 8 finished: treeAggregate at Statistics.scala:58, took 415.944263 ms\n",
            "26/02/22 20:09:25 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at JavaRandomForestRegressorExample.java:88.\n",
            "26/02/22 20:09:25 INFO SparkUI: Stopped Spark web UI at http://090e06d941c5:4040\n",
            "26/02/22 20:09:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "26/02/22 20:09:25 INFO MemoryStore: MemoryStore cleared\n",
            "26/02/22 20:09:25 INFO BlockManager: BlockManager stopped\n",
            "26/02/22 20:09:25 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "26/02/22 20:09:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "26/02/22 20:09:25 INFO SparkContext: Successfully stopped SparkContext\n",
            "26/02/22 20:09:25 INFO ShutdownHookManager: Shutdown hook called\n",
            "26/02/22 20:09:25 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-720549a1-ac6f-484a-b708-78b6980b2f8f\n",
            "26/02/22 20:09:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-c633c4c9-a2f9-41e3-8979-26fa3dbbedc2\n",
            "26/02/22 20:09:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f9f3c3b-cf54-4e04-9a73-9f5f4050a0e6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this guide, we demonstrated how to install the essential Spark servicesâ€”Spark Core, Spark Master, and Spark Workerâ€”using the Bigtop distribution. We also explored how to leverage Bigtop's utilities to easily launch a Spark engine. Additionally, we executed two example jobs included in the Spark package."
      ],
      "metadata": {
        "id": "llnOxcrrzw36"
      }
    }
  ]
}