{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DeF-OtwDrXVY",
        "SCoxn5xDimni",
        "JaaWPSEqsAyO",
        "m8hiZOGUr2FD",
        "7WND_44npS_d",
        "n8q4BIe316Ae",
        "ewBl-o1aH5hh",
        "LoPO2VrqEVCk",
        "AZuz6m48UyoO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Setting_up_Spark_Standalone_on_Google_Colab_BigtopEdition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\" alt=\"Logo Big Data for Beginners\"></div></a>\n",
        "# Install and run Spark in standalone mode‚ÄîApache Bigtop edition <div><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"45\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></div>\n",
        "\n",
        "<br>\n",
        "\n",
        "We will install Apache Spark on a single machine (the virtual machine hosting this notebook) in _standalone mode_, meaning it will run without any cluster manager like YARN, Mesos, or Kubernetes. For more information, see the [types of cluster managers supported by Spark](https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types)).\n",
        "\n",
        "We're following the official [Spark Standalone documentation](https://spark.apache.org/docs/latest/spark-standalone.html), using Apache Bigtop's Spark distribution, which conveniently packages Spark's start scripts as services.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Before running this notebook, you may want to update the Bigtop version (currently version `3.3.0` [with Hadoop 3.3.5](https://bigtop.apache.org/release-notes.html), see also the [full list of releases](https://bigtop.apache.org/download.html))."
      ],
      "metadata": {
        "id": "DeF-OtwDrXVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A side note:"
      ],
      "metadata": {
        "id": "Ayk0pz16sxMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://spark.apache.org/\"><img src=\"https://www.apache.org/logos/res/spark/spark.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Spark\" data-url=\"https://www.apache.org/logos/#spark\"></a>\n",
        "<a href=\"https://bigtop.apache.org/\"><img src=\"https://www.apache.org/logos/res/bigtop/bigtop.png\" width=\"120\" align=\"right\" style='vertical-align:middle; display:inline;' alt=\"Apache Bigtop\" data-url=\"https://www.apache.org/logos/#bigtop\"></a>\n",
        "\n",
        "\n",
        "I recently discovered [a website](https://www.apache.org/logos/) where you can find all Apache project logos, including Spark, with transparent backgrounds. It‚Äôs a great resource for anyone needing these assets for presentations or documentation. <p>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uribYroGLPlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Spark from Bigtop repository"
      ],
      "metadata": {
        "id": "8XOex6ixsSml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** since the current underlying Machine in Colab runs Ubuntu `22.04`, our choice of Bigtop versions is limited to at most `3.3.0`, since only Ubuntu `24.04` is supported since Bigtop `3.4.0`.\n",
        "\n",
        "**Note 2:** Bigtop `3.3.0` includes Spark `3.3.4` (here is the [list of all libraries included in the release](https://cwiki.apache.org/confluence/display/BIGTOP/Bigtop+3.3.0+Release). Spark `3.3.4` [runs on Java 8/11/17](https://archive.apache.org/dist/spark/docs/3.3.4), so we do not need to install anything in Colab because it comes with Java 17 pre-installed."
      ],
      "metadata": {
        "id": "jp6rE6PTpgYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lsb_release -rs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ-nBr7Ipgl3",
        "outputId": "74e947f3-e88e-48b7-f748-23539a0dc378"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxzRLcrntHfo",
        "outputId": "37b7dcf0-ae88-4efb-e764-a14e720a2a28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# 1. Use sudo to write the repo list\n",
        "echo \"Adding Bigtop repository...\"\n",
        "curl -s https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/$(lsb_release -is | tr '[:upper:]' '[:lower:]')-$(lsb_release -rs)/bigtop.list | sudo tee /etc/apt/sources.list.d/bigtop-3.3.0.list\n",
        "\n",
        "# 2. Add the GPG key (Updated to modern trusted.gpg.d method)\n",
        "echo \"Adding Bigtop GPG key...\"\n",
        "wget -qO - https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/GPG-KEY-bigtop | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/bigtop.gpg\n",
        "\n",
        "# 3. Use sudo for apt update\n",
        "echo \"Updating package cache...\"\n",
        "sudo apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDFoSvMF9rlW",
        "outputId": "8bc4feee-571e-4dd8-9f53-f147de870427"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Bigtop repository...\n",
            "deb http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/$(ARCH) bigtop contrib\n",
            "Adding Bigtop GPG key...\n",
            "Updating package cache...\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop InRelease [2,502 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 Packages [18.7 kB]\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,769 kB]\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.0 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,909 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,737 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,749 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,301 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,538 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,070 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,613 kB]\n",
            "Fetched 37.5 MB in 4s (9,149 kB/s)\n",
            "Reading package lists...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Bigtop and Spark packages"
      ],
      "metadata": {
        "id": "oShuNuhgtJwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo 'List all available packages that match \"bigtop\"'\n",
        "sudo apt-get -qq update && sudo apt search bigtop\n",
        "\n",
        "echo 'List all available packages that match \"spark\"'\n",
        "sudo apt search spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNfPDnHqBE4p",
        "outputId": "34cb83c4-5c83-41f2-b283-cf31c00e58a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all available packages that match \"bigtop\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "bigtop-groovy/stable 2.5.4-1 all\n",
            "  An agile and dynamic language for the Java Virtual Machine\n",
            "\n",
            "bigtop-jsvc/stable 1.2.4-1 amd64\n",
            "  Application to launch java daemon\n",
            "\n",
            "bigtop-utils/stable 3.3.0-1 all\n",
            "  Collection of useful tools for Bigtop\n",
            "\n",
            "List all available packages that match \"spark\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "alluxio/stable 2.9.3-1 all\n",
            "  Reliable file sharing at memory speed across cluster frameworks\n",
            "\n",
            "libjs-jquery.sparkline/jammy 2.1.2-3 all\n",
            "  library for jQuery to generate sparklines\n",
            "\n",
            "libsparkline-php/jammy 0.2-7 all\n",
            "  sparkline graphing library for php\n",
            "\n",
            "livy/stable 0.8.0-1 all\n",
            "  Livy is an open source REST interface for interacting with Apache Spark from anywhere.\n",
            "\n",
            "node-sparkles/jammy 1.0.1-2 all\n",
            "  Namespaced global event emitter\n",
            "\n",
            "nspark/jammy 1.7.8B2+git20210317.cb30779-2 amd64\n",
            "  Unarchiver for Spark and ArcFS files\n",
            "\n",
            "pcp-export-pcp2spark/jammy 5.3.6-1build1 amd64\n",
            "  Tool for exporting data from PCP to Apache Spark\n",
            "\n",
            "python3-sahara-plugin-spark/jammy 7.0.0-0ubuntu1 all\n",
            "  OpenStack data processing cluster as a service - Spark plugin\n",
            "\n",
            "python3-sparkpost/jammy 1.3.10-1 all\n",
            "  SparkPost Python API client (Python 3)\n",
            "\n",
            "r-cran-analysispipelines/jammy 1.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'analysisPipelines' (Compose Interoperable Analysis Pipelines & Put Them inProduction)\n",
            "\n",
            "r-cran-apache.sedona/jammy 1.8.1-1.ca2204.1 all\n",
            "  CRAN Package 'apache.sedona' (R Interface for Apache Sedona)\n",
            "\n",
            "r-cran-catalog/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'catalog' (Access the 'Spark Catalog' API via 'sparklyr')\n",
            "\n",
            "r-cran-databaseconnector/jammy 7.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'DatabaseConnector' (Connecting to Various Database Platforms)\n",
            "\n",
            "r-cran-fabricqueryr/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'fabricQueryR' (Query Data in 'Microsoft Fabric')\n",
            "\n",
            "r-cran-geospark/jammy 0.3.1-1.ca2204.1 all\n",
            "  CRAN Package 'geospark' (Bring Local Sf to Spark)\n",
            "\n",
            "r-cran-ggspark/jammy 0.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'ggspark' ('ggplot2' Functions to Create Tufte Style Sparklines)\n",
            "\n",
            "r-cran-graphframes/jammy 0.1.2-1.ca2204.1 all\n",
            "  CRAN Package 'graphframes' (Interface for 'GraphFrames')\n",
            "\n",
            "r-cran-hatchr/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'hatchR' (Predict Fish Hatch and Emergence Timing)\n",
            "\n",
            "r-cran-ibmdbr/jammy 1.51.0-1.ca2204.1 all\n",
            "  CRAN Package 'ibmdbR' (IBM in-Database Analytics for R)\n",
            "\n",
            "r-cran-ltxsparklines/jammy 1.1.3-1.ca2204.1 all\n",
            "  CRAN Package 'ltxsparklines' (Lightweight Sparklines for a LaTeX Document)\n",
            "\n",
            "r-cran-microplot/jammy 1.0-47-1.ca2204.1 all\n",
            "  CRAN Package 'microplot' (Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel')\n",
            "\n",
            "r-cran-notebookutils/jammy 1.6.2-1.ca2204.1 all\n",
            "  CRAN Package 'notebookutils' (Dummy R APIs Used in 'Azure Synapse Analytics' for LocalDevelopments)\n",
            "\n",
            "r-cran-oenokpm/jammy 2.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'OenoKPM' (Modeling the Kinetics of Carbon Dioxide Production in AlcoholicFermentation)\n",
            "\n",
            "r-cran-omoponspark/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'OmopOnSpark' (Using a Common Data Model on 'Spark')\n",
            "\n",
            "r-cran-parsnip/jammy 1.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'parsnip' (A Common API to Modeling and Analysis Functions)\n",
            "\n",
            "r-cran-paws.analytics/jammy 0.9.0-1.ca2204.1 all\n",
            "  CRAN Package 'paws.analytics' ('Amazon Web Services' Analytics Services)\n",
            "\n",
            "r-cran-pmmltransformations/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'pmmlTransformations' (Transforms Input Data from a PMML Perspective)\n",
            "\n",
            "r-cran-pointblank/jammy 0.12.3-1.ca2204.1 all\n",
            "  CRAN Package 'pointblank' (Data Validation and Organization of Metadata for Local andRemote Tables)\n",
            "\n",
            "r-cran-pysparklyr/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'pysparklyr' (Provides a 'PySpark' Back-End for the 'sparklyr' Package)\n",
            "\n",
            "r-cran-reactablefmtr/jammy 2.0.0-1.ca2204.1 all\n",
            "  CRAN Package 'reactablefmtr' (Streamlined Table Styling and Formatting for Reactable)\n",
            "\n",
            "r-cran-rquery/jammy 1.4.99-1.ca2204.1 all\n",
            "  CRAN Package 'rquery' (Relational Query Generator for Data Manipulation at Scale)\n",
            "\n",
            "r-cran-rsparkling/jammy 0.2.19-1.ca2204.1 all\n",
            "  CRAN Package 'rsparkling' (R Interface for H2O Sparkling Water)\n",
            "\n",
            "r-cran-s3.resourcer/jammy 1.1.2-1.ca2204.1 all\n",
            "  CRAN Package 's3.resourcer' (S3 Resource Resolver)\n",
            "\n",
            "r-cran-shinyml/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'shinyML' (Compare Supervised Machine Learning Models Using Shiny App)\n",
            "\n",
            "r-cran-skimr/jammy 2.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'skimr' (Compact and Flexible Summaries of Data)\n",
            "\n",
            "r-cran-spark.sas7bdat/jammy 1.4-1.ca2204.1 all\n",
            "  CRAN Package 'spark.sas7bdat' (Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkavro/jammy 0.3.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkavro' (Load Avro file into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkbq/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkbq' (Google 'BigQuery' Support for 'sparklyr')\n",
            "\n",
            "r-cran-sparkhail/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkhail' (A 'Sparklyr' Extension for 'Hail')\n",
            "\n",
            "r-cran-sparkline/jammy 2.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkline' ('jQuery' Sparkline 'htmlwidget')\n",
            "\n",
            "r-cran-sparklyr/jammy 1.9.3-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr' (R Interface to Apache Spark)\n",
            "\n",
            "r-cran-sparklyr.flint/jammy 0.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.flint' (Sparklyr Extension for 'Flint')\n",
            "\n",
            "r-cran-sparklyr.nested/jammy 0.0.4-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.nested' (A 'sparklyr' Extension for Nested Data)\n",
            "\n",
            "r-cran-sparktex/jammy 0.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparktex' (Generate LaTeX sparklines in R)\n",
            "\n",
            "r-cran-sparktf/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparktf' (Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark')\n",
            "\n",
            "r-cran-sparkwarc/jammy 0.1.6-1.ca2204.1 amd64\n",
            "  CRAN Package 'sparkwarc' (Load WARC Files into Apache Spark)\n",
            "\n",
            "r-cran-sparkxgb/jammy 0.2.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkxgb' (Interface for 'XGBoost' on 'Apache Spark')\n",
            "\n",
            "r-cran-sqlrender/jammy 1.19.4-1.ca2204.1 all\n",
            "  CRAN Package 'SqlRender' (Rendering Parameterized SQL and Translation to Dialects)\n",
            "\n",
            "r-cran-stddiff.spark/jammy 1.0-1.ca2204.1 all\n",
            "  CRAN Package 'stddiff.spark' (Calculate the Standardized Difference for Numeric, Binary andCategory Variables in Apache Spark)\n",
            "\n",
            "r-cran-svg/jammy 1.0.0-1.ca2204.1 amd64\n",
            "  CRAN Package 'SVG' (Spatially Variable Genes Detection Methods for SpatialTranscriptomics)\n",
            "\n",
            "r-cran-tidier/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'tidier' (Enhanced 'mutate')\n",
            "\n",
            "r-cran-timevizpro/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'TimeVizPro' (Dynamic Data Explorer: Visualize and Forecast with 'TimeVizPro')\n",
            "\n",
            "r-cran-variantspark/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'variantspark' (A 'Sparklyr' Extension for 'VariantSpark')\n",
            "\n",
            "spark-core/stable 3.3.4-1 all\n",
            "  Lightning-Fast Cluster Computing\n",
            "\n",
            "spark-datanucleus/stable 3.3.4-1 all\n",
            "  DataNucleus libraries for Apache Spark\n",
            "\n",
            "spark-external/stable 3.3.4-1 all\n",
            "  External libraries for Apache Spark\n",
            "\n",
            "spark-history-server/stable 3.3.4-1 all\n",
            "  History server for Apache Spark\n",
            "\n",
            "spark-master/stable 3.3.4-1 all\n",
            "  Server for Spark master\n",
            "\n",
            "spark-python/stable 3.3.4-1 all\n",
            "  Python client for Spark\n",
            "\n",
            "spark-sparkr/stable 3.3.4-1 all\n",
            "  R package for Apache Spark\n",
            "\n",
            "spark-thriftserver/stable 3.3.4-1 all\n",
            "  Thrift server for Spark SQL\n",
            "\n",
            "spark-worker/stable 3.3.4-1 all\n",
            "  Server for Spark worker\n",
            "\n",
            "spark-yarn-shuffle/stable 3.3.4-1 all\n",
            "  Spark YARN Shuffle Service\n",
            "\n",
            "sparkleshare/jammy 3.28+git20190525+cf446c0-3 all\n",
            "  distributed collaboration and sharing tool\n",
            "\n",
            "zeppelin/stable 0.11.0-1 all\n",
            "  Web-based notebook for data analysts\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the essential packages\n",
        "\n",
        "In order to run a Spark job, we need the core libraries as well as the Spark master and Spark worker. Master and worker in this case are going to run both on the same machine, the localhost.\n",
        "\n",
        "The package `bigtop-utils` will be used to start  the services."
      ],
      "metadata": {
        "id": "j8_0tUQ8tesJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-core spark-master spark-worker bigtop-utils; do\n",
        "  echo \"üõ†Ô∏è Installing $p\"\n",
        "  sudo apt install -qq -y $p\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCNjbZUe-69F",
        "outputId": "74500f77-e032-4e70-e26f-89fb2ff709fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Installing spark-core\n",
            "The following additional packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-client hadoop-hdfs\n",
            "  hadoop-mapreduce hadoop-yarn netcat-openbsd zookeeper\n",
            "The following NEW packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-client hadoop-hdfs\n",
            "  hadoop-mapreduce hadoop-yarn netcat-openbsd spark-core zookeeper\n",
            "0 upgraded, 11 newly installed, 0 to remove and 82 not upgraded.\n",
            "Need to get 732 MB of archives.\n",
            "After this operation, 905 MB of additional disk space will be used.\n",
            "Selecting previously unselected package netcat-openbsd.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 117540 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-netcat-openbsd_1.218-4ubuntu1_amd64.deb ...\r\n",
            "Unpacking netcat-openbsd (1.218-4ubuntu1) ...\r\n",
            "Selecting previously unselected package bigtop-utils.\r\n",
            "Preparing to unpack .../01-bigtop-utils_3.3.0-1_all.deb ...\r\n",
            "Unpacking bigtop-utils (3.3.0-1) ...\r\n",
            "Selecting previously unselected package bigtop-groovy.\r\n",
            "Preparing to unpack .../02-bigtop-groovy_2.5.4-1_all.deb ...\r\n",
            "Unpacking bigtop-groovy (2.5.4-1) ...\r\n",
            "Selecting previously unselected package bigtop-jsvc.\r\n",
            "Preparing to unpack .../03-bigtop-jsvc_1.2.4-1_amd64.deb ...\r\n",
            "Unpacking bigtop-jsvc (1.2.4-1) ...\r\n",
            "Selecting previously unselected package zookeeper.\r\n",
            "Preparing to unpack .../04-zookeeper_3.7.2-1_all.deb ...\r\n",
            "Unpacking zookeeper (3.7.2-1) ...\r\n",
            "Selecting previously unselected package hadoop.\r\n",
            "Preparing to unpack .../05-hadoop_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop (3.3.6-1) ...\r\n",
            "Selecting previously unselected package hadoop-hdfs.\r\n",
            "Preparing to unpack .../06-hadoop-hdfs_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop-hdfs (3.3.6-1) ...\r\n",
            "Selecting previously unselected package hadoop-yarn.\r\n",
            "Preparing to unpack .../07-hadoop-yarn_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop-yarn (3.3.6-1) ...\r\n",
            "Selecting previously unselected package hadoop-mapreduce.\r\n",
            "Preparing to unpack .../08-hadoop-mapreduce_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop-mapreduce (3.3.6-1) ...\r\n",
            "Selecting previously unselected package hadoop-client.\r\n",
            "Preparing to unpack .../09-hadoop-client_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop-client (3.3.6-1) ...\r\n",
            "Selecting previously unselected package spark-core.\r\n",
            "Preparing to unpack .../10-spark-core_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-core (3.3.4-1) ...\r\n",
            "Setting up netcat-openbsd (1.218-4ubuntu1) ...\r\n",
            "update-alternatives: using /bin/nc.openbsd to provide /bin/nc (nc) in auto mode\r\n",
            "Setting up bigtop-utils (3.3.0-1) ...\r\n",
            "Setting up zookeeper (3.7.2-1) ...\r\n",
            "update-alternatives: using /etc/zookeeper/conf.dist to provide /etc/zookeeper/conf (zookeeper-conf) in auto mode\r\n",
            "Setting up bigtop-groovy (2.5.4-1) ...\r\n",
            "Setting up hadoop (3.3.6-1) ...\r\n",
            "update-alternatives: using /etc/hadoop/conf.empty to provide /etc/hadoop/conf (hadoop-conf) in auto mode\r\n",
            "Setting up bigtop-jsvc (1.2.4-1) ...\r\n",
            "Setting up hadoop-yarn (3.3.6-1) ...\r\n",
            "Setting up hadoop-hdfs (3.3.6-1) ...\r\n",
            "Setting up hadoop-mapreduce (3.3.6-1) ...\r\n",
            "Setting up hadoop-client (3.3.6-1) ...\r\n",
            "Setting up spark-core (3.3.4-1) ...\r\n",
            "update-alternatives: using /etc/spark/conf.dist to provide /etc/spark/conf (spark-conf) in auto mode\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\r\n",
            "\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\r\n",
            "\r\n",
            "üõ†Ô∏è Installing spark-master\n",
            "The following NEW packages will be installed:\n",
            "  spark-master\n",
            "0 upgraded, 1 newly installed, 0 to remove and 82 not upgraded.\n",
            "Need to get 4,688 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Selecting previously unselected package spark-master.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121198 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-master_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-master (3.3.4-1) ...\r\n",
            "Setting up spark-master (3.3.4-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "üõ†Ô∏è Installing spark-worker\n",
            "The following NEW packages will be installed:\n",
            "  spark-worker\n",
            "0 upgraded, 1 newly installed, 0 to remove and 82 not upgraded.\n",
            "Need to get 4,662 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Selecting previously unselected package spark-worker.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 121202 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-worker_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-worker (3.3.4-1) ...\r\n",
            "Setting up spark-worker (3.3.4-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "üõ†Ô∏è Installing bigtop-utils\n",
            "bigtop-utils is already the newest version (3.3.0-1).\n",
            "bigtop-utils set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note;** in a future version of this notebook we are going to use an alternative to `apt` for installing packages in order to avoid the warning\n",
        "\n",
        "```\n",
        "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "IFxPmikEu0An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Spark services\n",
        "\n",
        "Thanks to the Bigtop's utilities, we can now start the Spark master and Spark worker as services. Normally one would use `systemctl` but since this is not allowed on Colab, we are going to resort to `service`."
      ],
      "metadata": {
        "id": "3jx-t17PvHGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-master spark-worker; do\n",
        "  echo \"Starting $p\"\n",
        "  # systemctl start $p\n",
        "  sudo service $p start\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6lMkulx-X2I",
        "outputId": "f607bff1-1834-428d-ea4c-c08a79e21065"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting spark-master\n",
            " * Starting Spark master (spark-master): \n",
            "Starting spark-worker\n",
            " * Starting Spark worker (spark-worker): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the `pi` example"
      ],
      "metadata": {
        "id": "6erQkYR6v7nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step may take some time.\n",
        "\n",
        "We'll run the `SparkPi` demo from the examples included in the Spark distribution, which are packaged in the `spark-examples*.jar` file.\n",
        "\n",
        "We'll submit the job using [`spark-submit`](https://spark.apache.org/docs/latest/submitting-applications.html), and the output will be an approximation of œÄ (for more details, see the [official Spark examples](https://spark.apache.org/examples.html).\n",
        "\n",
        "\n",
        "The following code defines the variable `$EXAMPLE_JAR`, which points to the archive containing all the examples from the Spark distribution.\n",
        "\n",
        "The following command submits the SparkPi application (located in the `org.apache.spark.examples.SparkPi` class) to the Spark master at `spark://${HOSTNAME}:7077` using `spark-submit`:\n",
        "\n",
        "```\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100\n",
        "```\n",
        "\n",
        "In this example, the number $100$ represents the number of iterations used to compute an approximation of œÄ by calculating the ratio of points inside versus outside the unit circle."
      ],
      "metadata": {
        "id": "DyfvrPtVVCfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRKc5yAHoEsH",
        "outputId": "f451718d-6258-4e69-ce8d-ff5c8598ca80"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/spark/examples/jars/spark-examples.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export EXAMPLES_JAR=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "  --class org.apache.spark.examples.SparkPi \\\n",
        "  --master spark://${HOSTNAME}:7077 \\\n",
        "  $EXAMPLES_JAR \\\n",
        "  100"
      ],
      "metadata": {
        "id": "a4COXhlrR1mG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051898f9-9586-4a3a-b2ed-8f6746a97344"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26/02/22 19:25:28 INFO SparkContext: Running Spark version 3.3.4\n",
            "26/02/22 19:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/02/22 19:25:28 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 19:25:28 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "26/02/22 19:25:28 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 19:25:28 INFO SparkContext: Submitted application: Spark Pi\n",
            "26/02/22 19:25:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "26/02/22 19:25:28 INFO ResourceProfile: Limiting resource is cpu\n",
            "26/02/22 19:25:28 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "26/02/22 19:25:28 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 19:25:28 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 19:25:28 INFO SecurityManager: Changing view acls groups to: \n",
            "26/02/22 19:25:28 INFO SecurityManager: Changing modify acls groups to: \n",
            "26/02/22 19:25:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "26/02/22 19:25:29 INFO Utils: Successfully started service 'sparkDriver' on port 37217.\n",
            "26/02/22 19:25:29 INFO SparkEnv: Registering MapOutputTracker\n",
            "26/02/22 19:25:29 INFO SparkEnv: Registering BlockManagerMaster\n",
            "26/02/22 19:25:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "26/02/22 19:25:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "26/02/22 19:25:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "26/02/22 19:25:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-00d71dd2-973b-4dc7-9de1-9609afb1e486\n",
            "26/02/22 19:25:29 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "26/02/22 19:25:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "26/02/22 19:25:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "26/02/22 19:25:29 INFO SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.3.4.jar at spark://090e06d941c5:37217/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771788328287\n",
            "26/02/22 19:25:29 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://090e06d941c5:7077...\n",
            "26/02/22 19:25:30 INFO TransportClientFactory: Successfully created connection to 090e06d941c5/172.28.0.12:7077 after 58 ms (0 ms spent in bootstraps)\n",
            "26/02/22 19:25:30 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20260222192530-0000\n",
            "26/02/22 19:25:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39949.\n",
            "26/02/22 19:25:30 INFO NettyBlockTransferService: Server created on 090e06d941c5:39949\n",
            "26/02/22 19:25:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "26/02/22 19:25:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 090e06d941c5, 39949, None)\n",
            "26/02/22 19:25:30 INFO BlockManagerMasterEndpoint: Registering block manager 090e06d941c5:39949 with 434.4 MiB RAM, BlockManagerId(driver, 090e06d941c5, 39949, None)\n",
            "26/02/22 19:25:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 090e06d941c5, 39949, None)\n",
            "26/02/22 19:25:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 090e06d941c5, 39949, None)\n",
            "26/02/22 19:25:30 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20260222192530-0000/0 on worker-20260222192519-172.28.0.12-7078 (172.28.0.12:7078) with 2 core(s)\n",
            "26/02/22 19:25:30 INFO StandaloneSchedulerBackend: Granted executor ID app-20260222192530-0000/0 on hostPort 172.28.0.12:7078 with 2 core(s), 1024.0 MiB RAM\n",
            "26/02/22 19:25:31 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20260222192530-0000/0 is now RUNNING\n",
            "26/02/22 19:25:31 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
            "26/02/22 19:25:33 INFO SparkContext: Starting job: reduce at SparkPi.scala:38\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:38) with 100 output partitions\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkPi.scala:38)\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34), which has no missing parents\n",
            "26/02/22 19:25:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.0 KiB, free 434.4 MiB)\n",
            "26/02/22 19:25:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KiB, free 434.4 MiB)\n",
            "26/02/22 19:25:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 090e06d941c5:39949 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "26/02/22 19:25:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1509\n",
            "26/02/22 19:25:33 INFO DAGScheduler: Submitting 100 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:34) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
            "26/02/22 19:25:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 100 tasks resource profile 0\n",
            "26/02/22 19:25:38 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:38988) with ID 0,  ResourceProfileId 0\n",
            "26/02/22 19:25:38 INFO BlockManagerMasterEndpoint: Registering block manager 172.28.0.12:42459 with 434.4 MiB RAM, BlockManagerId(0, 172.28.0.12, 42459, None)\n",
            "26/02/22 19:25:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.28.0.12, executor 0, partition 0, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (172.28.0.12, executor 0, partition 1, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.28.0.12:42459 (size: 2.3 KiB, free: 434.4 MiB)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (172.28.0.12, executor 0, partition 2, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (172.28.0.12, executor 0, partition 3, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1216 ms on 172.28.0.12 (executor 0) (1/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1315 ms on 172.28.0.12 (executor 0) (2/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (172.28.0.12, executor 0, partition 4, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 142 ms on 172.28.0.12 (executor 0) (3/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 133 ms on 172.28.0.12 (executor 0) (4/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (172.28.0.12, executor 0, partition 5, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (172.28.0.12, executor 0, partition 6, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 90 ms on 172.28.0.12 (executor 0) (5/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (172.28.0.12, executor 0, partition 7, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 100 ms on 172.28.0.12 (executor 0) (6/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (172.28.0.12, executor 0, partition 8, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 71 ms on 172.28.0.12 (executor 0) (7/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (172.28.0.12, executor 0, partition 9, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 110 ms on 172.28.0.12 (executor 0) (8/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (172.28.0.12, executor 0, partition 10, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 67 ms on 172.28.0.12 (executor 0) (9/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (172.28.0.12, executor 0, partition 11, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 127 ms on 172.28.0.12 (executor 0) (10/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (172.28.0.12, executor 0, partition 12, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 101 ms on 172.28.0.12 (executor 0) (11/100)\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (172.28.0.12, executor 0, partition 13, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:40 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 101 ms on 172.28.0.12 (executor 0) (12/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (172.28.0.12, executor 0, partition 14, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 91 ms on 172.28.0.12 (executor 0) (13/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (172.28.0.12, executor 0, partition 15, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 128 ms on 172.28.0.12 (executor 0) (14/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (172.28.0.12, executor 0, partition 16, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 110 ms on 172.28.0.12 (executor 0) (15/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (172.28.0.12, executor 0, partition 17, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 77 ms on 172.28.0.12 (executor 0) (16/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (172.28.0.12, executor 0, partition 18, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 80 ms on 172.28.0.12 (executor 0) (17/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (172.28.0.12, executor 0, partition 19, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 72 ms on 172.28.0.12 (executor 0) (18/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (172.28.0.12, executor 0, partition 20, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 69 ms on 172.28.0.12 (executor 0) (19/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (172.28.0.12, executor 0, partition 21, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 69 ms on 172.28.0.12 (executor 0) (20/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (172.28.0.12, executor 0, partition 22, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 70 ms on 172.28.0.12 (executor 0) (21/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (172.28.0.12, executor 0, partition 23, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 82 ms on 172.28.0.12 (executor 0) (22/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 24.0 in stage 0.0 (TID 24) (172.28.0.12, executor 0, partition 24, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 76 ms on 172.28.0.12 (executor 0) (23/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 25.0 in stage 0.0 (TID 25) (172.28.0.12, executor 0, partition 25, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 80 ms on 172.28.0.12 (executor 0) (24/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 24.0 in stage 0.0 (TID 24) in 85 ms on 172.28.0.12 (executor 0) (25/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 26.0 in stage 0.0 (TID 26) (172.28.0.12, executor 0, partition 26, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 27.0 in stage 0.0 (TID 27) (172.28.0.12, executor 0, partition 27, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 25.0 in stage 0.0 (TID 25) in 63 ms on 172.28.0.12 (executor 0) (26/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 28.0 in stage 0.0 (TID 28) (172.28.0.12, executor 0, partition 28, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 26.0 in stage 0.0 (TID 26) in 39 ms on 172.28.0.12 (executor 0) (27/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 29.0 in stage 0.0 (TID 29) (172.28.0.12, executor 0, partition 29, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 27.0 in stage 0.0 (TID 27) in 52 ms on 172.28.0.12 (executor 0) (28/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 30.0 in stage 0.0 (TID 30) (172.28.0.12, executor 0, partition 30, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 28.0 in stage 0.0 (TID 28) in 55 ms on 172.28.0.12 (executor 0) (29/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 31.0 in stage 0.0 (TID 31) (172.28.0.12, executor 0, partition 31, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 29.0 in stage 0.0 (TID 29) in 50 ms on 172.28.0.12 (executor 0) (30/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 32.0 in stage 0.0 (TID 32) (172.28.0.12, executor 0, partition 32, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 30.0 in stage 0.0 (TID 30) in 75 ms on 172.28.0.12 (executor 0) (31/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 33.0 in stage 0.0 (TID 33) (172.28.0.12, executor 0, partition 33, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 31.0 in stage 0.0 (TID 31) in 73 ms on 172.28.0.12 (executor 0) (32/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 34.0 in stage 0.0 (TID 34) (172.28.0.12, executor 0, partition 34, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 32.0 in stage 0.0 (TID 32) in 67 ms on 172.28.0.12 (executor 0) (33/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 35.0 in stage 0.0 (TID 35) (172.28.0.12, executor 0, partition 35, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 33.0 in stage 0.0 (TID 33) in 49 ms on 172.28.0.12 (executor 0) (34/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 36.0 in stage 0.0 (TID 36) (172.28.0.12, executor 0, partition 36, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 34.0 in stage 0.0 (TID 34) in 61 ms on 172.28.0.12 (executor 0) (35/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 37.0 in stage 0.0 (TID 37) (172.28.0.12, executor 0, partition 37, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 35.0 in stage 0.0 (TID 35) in 58 ms on 172.28.0.12 (executor 0) (36/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 38.0 in stage 0.0 (TID 38) (172.28.0.12, executor 0, partition 38, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 36.0 in stage 0.0 (TID 36) in 49 ms on 172.28.0.12 (executor 0) (37/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 39.0 in stage 0.0 (TID 39) (172.28.0.12, executor 0, partition 39, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 37.0 in stage 0.0 (TID 37) in 55 ms on 172.28.0.12 (executor 0) (38/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 40.0 in stage 0.0 (TID 40) (172.28.0.12, executor 0, partition 40, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 38.0 in stage 0.0 (TID 38) in 50 ms on 172.28.0.12 (executor 0) (39/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 41.0 in stage 0.0 (TID 41) (172.28.0.12, executor 0, partition 41, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 40.0 in stage 0.0 (TID 40) in 36 ms on 172.28.0.12 (executor 0) (40/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 42.0 in stage 0.0 (TID 42) (172.28.0.12, executor 0, partition 42, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 39.0 in stage 0.0 (TID 39) in 82 ms on 172.28.0.12 (executor 0) (41/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 43.0 in stage 0.0 (TID 43) (172.28.0.12, executor 0, partition 43, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 42.0 in stage 0.0 (TID 42) in 42 ms on 172.28.0.12 (executor 0) (42/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 44.0 in stage 0.0 (TID 44) (172.28.0.12, executor 0, partition 44, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 41.0 in stage 0.0 (TID 41) in 78 ms on 172.28.0.12 (executor 0) (43/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 45.0 in stage 0.0 (TID 45) (172.28.0.12, executor 0, partition 45, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Starting task 46.0 in stage 0.0 (TID 46) (172.28.0.12, executor 0, partition 46, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 44.0 in stage 0.0 (TID 44) in 63 ms on 172.28.0.12 (executor 0) (44/100)\n",
            "26/02/22 19:25:41 INFO TaskSetManager: Finished task 43.0 in stage 0.0 (TID 43) in 74 ms on 172.28.0.12 (executor 0) (45/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 47.0 in stage 0.0 (TID 47) (172.28.0.12, executor 0, partition 47, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 46.0 in stage 0.0 (TID 46) in 51 ms on 172.28.0.12 (executor 0) (46/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 48.0 in stage 0.0 (TID 48) (172.28.0.12, executor 0, partition 48, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 45.0 in stage 0.0 (TID 45) in 68 ms on 172.28.0.12 (executor 0) (47/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 49.0 in stage 0.0 (TID 49) (172.28.0.12, executor 0, partition 49, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 48.0 in stage 0.0 (TID 48) in 45 ms on 172.28.0.12 (executor 0) (48/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 50.0 in stage 0.0 (TID 50) (172.28.0.12, executor 0, partition 50, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 47.0 in stage 0.0 (TID 47) in 64 ms on 172.28.0.12 (executor 0) (49/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 51.0 in stage 0.0 (TID 51) (172.28.0.12, executor 0, partition 51, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 50.0 in stage 0.0 (TID 50) in 40 ms on 172.28.0.12 (executor 0) (50/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 52.0 in stage 0.0 (TID 52) (172.28.0.12, executor 0, partition 52, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 49.0 in stage 0.0 (TID 49) in 65 ms on 172.28.0.12 (executor 0) (51/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 53.0 in stage 0.0 (TID 53) (172.28.0.12, executor 0, partition 53, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 51.0 in stage 0.0 (TID 51) in 61 ms on 172.28.0.12 (executor 0) (52/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 54.0 in stage 0.0 (TID 54) (172.28.0.12, executor 0, partition 54, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 52.0 in stage 0.0 (TID 52) in 67 ms on 172.28.0.12 (executor 0) (53/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 55.0 in stage 0.0 (TID 55) (172.28.0.12, executor 0, partition 55, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 53.0 in stage 0.0 (TID 53) in 63 ms on 172.28.0.12 (executor 0) (54/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 56.0 in stage 0.0 (TID 56) (172.28.0.12, executor 0, partition 56, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 54.0 in stage 0.0 (TID 54) in 58 ms on 172.28.0.12 (executor 0) (55/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 56.0 in stage 0.0 (TID 56) in 23 ms on 172.28.0.12 (executor 0) (56/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 57.0 in stage 0.0 (TID 57) (172.28.0.12, executor 0, partition 57, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 58.0 in stage 0.0 (TID 58) (172.28.0.12, executor 0, partition 58, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 55.0 in stage 0.0 (TID 55) in 79 ms on 172.28.0.12 (executor 0) (57/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 59.0 in stage 0.0 (TID 59) (172.28.0.12, executor 0, partition 59, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 57.0 in stage 0.0 (TID 57) in 63 ms on 172.28.0.12 (executor 0) (58/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 60.0 in stage 0.0 (TID 60) (172.28.0.12, executor 0, partition 60, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 58.0 in stage 0.0 (TID 58) in 67 ms on 172.28.0.12 (executor 0) (59/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 61.0 in stage 0.0 (TID 61) (172.28.0.12, executor 0, partition 61, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 59.0 in stage 0.0 (TID 59) in 61 ms on 172.28.0.12 (executor 0) (60/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 62.0 in stage 0.0 (TID 62) (172.28.0.12, executor 0, partition 62, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 61.0 in stage 0.0 (TID 61) in 55 ms on 172.28.0.12 (executor 0) (61/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 63.0 in stage 0.0 (TID 63) (172.28.0.12, executor 0, partition 63, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 60.0 in stage 0.0 (TID 60) in 128 ms on 172.28.0.12 (executor 0) (62/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 64.0 in stage 0.0 (TID 64) (172.28.0.12, executor 0, partition 64, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 62.0 in stage 0.0 (TID 62) in 81 ms on 172.28.0.12 (executor 0) (63/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 65.0 in stage 0.0 (TID 65) (172.28.0.12, executor 0, partition 65, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 63.0 in stage 0.0 (TID 63) in 71 ms on 172.28.0.12 (executor 0) (64/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 64.0 in stage 0.0 (TID 64) in 51 ms on 172.28.0.12 (executor 0) (65/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 66.0 in stage 0.0 (TID 66) (172.28.0.12, executor 0, partition 66, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 67.0 in stage 0.0 (TID 67) (172.28.0.12, executor 0, partition 67, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 65.0 in stage 0.0 (TID 65) in 48 ms on 172.28.0.12 (executor 0) (66/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 68.0 in stage 0.0 (TID 68) (172.28.0.12, executor 0, partition 68, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 66.0 in stage 0.0 (TID 66) in 49 ms on 172.28.0.12 (executor 0) (67/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 69.0 in stage 0.0 (TID 69) (172.28.0.12, executor 0, partition 69, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 67.0 in stage 0.0 (TID 67) in 65 ms on 172.28.0.12 (executor 0) (68/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 70.0 in stage 0.0 (TID 70) (172.28.0.12, executor 0, partition 70, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 68.0 in stage 0.0 (TID 68) in 61 ms on 172.28.0.12 (executor 0) (69/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 71.0 in stage 0.0 (TID 71) (172.28.0.12, executor 0, partition 71, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 69.0 in stage 0.0 (TID 69) in 39 ms on 172.28.0.12 (executor 0) (70/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 72.0 in stage 0.0 (TID 72) (172.28.0.12, executor 0, partition 72, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 70.0 in stage 0.0 (TID 70) in 46 ms on 172.28.0.12 (executor 0) (71/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 73.0 in stage 0.0 (TID 73) (172.28.0.12, executor 0, partition 73, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 72.0 in stage 0.0 (TID 72) in 35 ms on 172.28.0.12 (executor 0) (72/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 74.0 in stage 0.0 (TID 74) (172.28.0.12, executor 0, partition 74, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 71.0 in stage 0.0 (TID 71) in 72 ms on 172.28.0.12 (executor 0) (73/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 75.0 in stage 0.0 (TID 75) (172.28.0.12, executor 0, partition 75, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 73.0 in stage 0.0 (TID 73) in 48 ms on 172.28.0.12 (executor 0) (74/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 76.0 in stage 0.0 (TID 76) (172.28.0.12, executor 0, partition 76, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 74.0 in stage 0.0 (TID 74) in 50 ms on 172.28.0.12 (executor 0) (75/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 77.0 in stage 0.0 (TID 77) (172.28.0.12, executor 0, partition 77, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 75.0 in stage 0.0 (TID 75) in 55 ms on 172.28.0.12 (executor 0) (76/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 78.0 in stage 0.0 (TID 78) (172.28.0.12, executor 0, partition 78, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 76.0 in stage 0.0 (TID 76) in 59 ms on 172.28.0.12 (executor 0) (77/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 79.0 in stage 0.0 (TID 79) (172.28.0.12, executor 0, partition 79, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 77.0 in stage 0.0 (TID 77) in 53 ms on 172.28.0.12 (executor 0) (78/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 80.0 in stage 0.0 (TID 80) (172.28.0.12, executor 0, partition 80, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 78.0 in stage 0.0 (TID 78) in 55 ms on 172.28.0.12 (executor 0) (79/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 81.0 in stage 0.0 (TID 81) (172.28.0.12, executor 0, partition 81, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 79.0 in stage 0.0 (TID 79) in 67 ms on 172.28.0.12 (executor 0) (80/100)\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Starting task 82.0 in stage 0.0 (TID 82) (172.28.0.12, executor 0, partition 82, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:42 INFO TaskSetManager: Finished task 80.0 in stage 0.0 (TID 80) in 69 ms on 172.28.0.12 (executor 0) (81/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 83.0 in stage 0.0 (TID 83) (172.28.0.12, executor 0, partition 83, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 84.0 in stage 0.0 (TID 84) (172.28.0.12, executor 0, partition 84, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 81.0 in stage 0.0 (TID 81) in 73 ms on 172.28.0.12 (executor 0) (82/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 82.0 in stage 0.0 (TID 82) in 66 ms on 172.28.0.12 (executor 0) (83/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 85.0 in stage 0.0 (TID 85) (172.28.0.12, executor 0, partition 85, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 83.0 in stage 0.0 (TID 83) in 48 ms on 172.28.0.12 (executor 0) (84/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 86.0 in stage 0.0 (TID 86) (172.28.0.12, executor 0, partition 86, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 84.0 in stage 0.0 (TID 84) in 67 ms on 172.28.0.12 (executor 0) (85/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 87.0 in stage 0.0 (TID 87) (172.28.0.12, executor 0, partition 87, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 85.0 in stage 0.0 (TID 85) in 53 ms on 172.28.0.12 (executor 0) (86/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 88.0 in stage 0.0 (TID 88) (172.28.0.12, executor 0, partition 88, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 87.0 in stage 0.0 (TID 87) in 31 ms on 172.28.0.12 (executor 0) (87/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 89.0 in stage 0.0 (TID 89) (172.28.0.12, executor 0, partition 89, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 86.0 in stage 0.0 (TID 86) in 71 ms on 172.28.0.12 (executor 0) (88/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 90.0 in stage 0.0 (TID 90) (172.28.0.12, executor 0, partition 90, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 88.0 in stage 0.0 (TID 88) in 51 ms on 172.28.0.12 (executor 0) (89/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 91.0 in stage 0.0 (TID 91) (172.28.0.12, executor 0, partition 91, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 89.0 in stage 0.0 (TID 89) in 77 ms on 172.28.0.12 (executor 0) (90/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 92.0 in stage 0.0 (TID 92) (172.28.0.12, executor 0, partition 92, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 90.0 in stage 0.0 (TID 90) in 57 ms on 172.28.0.12 (executor 0) (91/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 93.0 in stage 0.0 (TID 93) (172.28.0.12, executor 0, partition 93, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 91.0 in stage 0.0 (TID 91) in 40 ms on 172.28.0.12 (executor 0) (92/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 94.0 in stage 0.0 (TID 94) (172.28.0.12, executor 0, partition 94, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 92.0 in stage 0.0 (TID 92) in 44 ms on 172.28.0.12 (executor 0) (93/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 95.0 in stage 0.0 (TID 95) (172.28.0.12, executor 0, partition 95, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 93.0 in stage 0.0 (TID 93) in 43 ms on 172.28.0.12 (executor 0) (94/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 96.0 in stage 0.0 (TID 96) (172.28.0.12, executor 0, partition 96, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 97.0 in stage 0.0 (TID 97) (172.28.0.12, executor 0, partition 97, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 94.0 in stage 0.0 (TID 94) in 55 ms on 172.28.0.12 (executor 0) (95/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 95.0 in stage 0.0 (TID 95) in 33 ms on 172.28.0.12 (executor 0) (96/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 98.0 in stage 0.0 (TID 98) (172.28.0.12, executor 0, partition 98, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 97.0 in stage 0.0 (TID 97) in 42 ms on 172.28.0.12 (executor 0) (97/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Starting task 99.0 in stage 0.0 (TID 99) (172.28.0.12, executor 0, partition 99, PROCESS_LOCAL, 4582 bytes) taskResourceAssignments Map()\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 96.0 in stage 0.0 (TID 96) in 63 ms on 172.28.0.12 (executor 0) (98/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 98.0 in stage 0.0 (TID 98) in 66 ms on 172.28.0.12 (executor 0) (99/100)\n",
            "26/02/22 19:25:43 INFO TaskSetManager: Finished task 99.0 in stage 0.0 (TID 99) in 51 ms on 172.28.0.12 (executor 0) (100/100)\n",
            "26/02/22 19:25:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "26/02/22 19:25:43 INFO DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:38) finished in 10.207 s\n",
            "26/02/22 19:25:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:25:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "26/02/22 19:25:43 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 10.420025 s\n",
            "Pi is roughly 3.1422199142219913\n",
            "26/02/22 19:25:43 INFO SparkUI: Stopped Spark web UI at http://090e06d941c5:4040\n",
            "26/02/22 19:25:43 INFO StandaloneSchedulerBackend: Shutting down all executors\n",
            "26/02/22 19:25:43 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down\n",
            "26/02/22 19:25:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "26/02/22 19:25:43 INFO MemoryStore: MemoryStore cleared\n",
            "26/02/22 19:25:43 INFO BlockManager: BlockManager stopped\n",
            "26/02/22 19:25:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "26/02/22 19:25:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "26/02/22 19:25:43 INFO SparkContext: Successfully stopped SparkContext\n",
            "26/02/22 19:25:43 INFO ShutdownHookManager: Shutdown hook called\n",
            "26/02/22 19:25:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-76f2d6db-1ba0-4f8e-bd99-478f34815448\n",
            "26/02/22 19:25:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc4f54eb-9e15-4f43-b67b-1cf0c3295757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the Java Random Forest Regressor example\n",
        "\n",
        "Next, we will run the Java Random Forest Regressor example. Source: [JavaRandomForestRegressorExample.java](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java)."
      ],
      "metadata": {
        "id": "mLsH_Xpdyl97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        " j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        " echo \"Jar file containing examples: $j\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8JwPHSSJCJ8",
        "outputId": "ea0a6682-eda6-4260-83cf-cd1e802a15af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jar file containing examples: /usr/lib/spark/examples/jars/spark-examples.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you run\n",
        "\n",
        "```\n",
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j\n",
        "```\n",
        "\n",
        "you'll get an error message telling you that the file `/content/data/mllib/sample_libsvm_data.txt` is missing. We are just going to create this file, but first we need to find it!"
      ],
      "metadata": {
        "id": "d1idj6w9y493"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name 'sample_libsvm_data.txt' 2> /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS1eO8PtLHwl",
        "outputId": "381e0108-d9f3-497e-a316-0eeaa9b1ad71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/data/mllib/sample_libsvm_data.txt\n",
            "/usr/lib/spark/data/mllib/sample_libsvm_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the datafile to the desired location"
      ],
      "metadata": {
        "id": "ADBuYvjezWZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p data/mllib/\n",
        "cp /usr/lib/spark/data/mllib/sample_libsvm_data.txt data/mllib/sample_libsvm_data.txt"
      ],
      "metadata": {
        "id": "E5vwn1iNMHKQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the JavaRandomForestRegressorExample example."
      ],
      "metadata": {
        "id": "snh_A1DFzj7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "j=$(find $(which hadoop|awk -F 'bin/hadoop' '{print $1}') -name 'spark-examples.jar' -print -quit)\n",
        "spark-submit --class  org.apache.spark.examples.ml.JavaRandomForestRegressorExample $j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuvY9Pt_MUcw",
        "outputId": "cead3b4e-86f1-4f80-d531-8db62465162c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[122,123,124...|\n",
            "|       0.0|  0.0|(692,[125,126,127...|\n",
            "|      0.05|  0.0|(692,[126,127,128...|\n",
            "|      0.05|  0.0|(692,[126,127,128...|\n",
            "|      0.05|  0.0|(692,[128,129,130...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "Root Mean Squared Error (RMSE) on test data = 0.10850895607454299\n",
            "Learned regression forest model:\n",
            "RandomForestRegressionModel: uid=rfr_8579b0d5257b, numTrees=20, numFeatures=692\n",
            "  Tree 0 (weight 1.0):\n",
            "    If (feature 489 <= 37.5)\n",
            "     If (feature 492 <= 205.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 492 > 205.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 489 > 37.5)\n",
            "     Predict: 1.0\n",
            "  Tree 1 (weight 1.0):\n",
            "    If (feature 489 <= 5.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 489 > 5.5)\n",
            "     Predict: 1.0\n",
            "  Tree 2 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 3 (weight 1.0):\n",
            "    If (feature 378 <= 90.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 378 > 90.5)\n",
            "     Predict: 1.0\n",
            "  Tree 4 (weight 1.0):\n",
            "    If (feature 433 <= 66.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 66.5)\n",
            "     Predict: 1.0\n",
            "  Tree 5 (weight 1.0):\n",
            "    If (feature 407 <= 90.0)\n",
            "     If (feature 350 <= 253.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 350 > 253.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 407 > 90.0)\n",
            "     Predict: 1.0\n",
            "  Tree 6 (weight 1.0):\n",
            "    If (feature 517 <= 59.0)\n",
            "     If (feature 407 <= 9.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 407 > 9.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 517 > 59.0)\n",
            "     Predict: 1.0\n",
            "  Tree 7 (weight 1.0):\n",
            "    If (feature 272 <= 4.5)\n",
            "     If (feature 234 <= 4.5)\n",
            "      Predict: 1.0\n",
            "     Else (feature 234 > 4.5)\n",
            "      Predict: 0.0\n",
            "    Else (feature 272 > 4.5)\n",
            "     Predict: 0.0\n",
            "  Tree 8 (weight 1.0):\n",
            "    If (feature 461 <= 46.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 461 > 46.5)\n",
            "     Predict: 1.0\n",
            "  Tree 9 (weight 1.0):\n",
            "    If (feature 490 <= 44.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 490 > 44.5)\n",
            "     Predict: 1.0\n",
            "  Tree 10 (weight 1.0):\n",
            "    If (feature 434 <= 70.5)\n",
            "     If (feature 597 <= 253.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 597 > 253.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 434 > 70.5)\n",
            "     Predict: 1.0\n",
            "  Tree 11 (weight 1.0):\n",
            "    If (feature 518 <= 18.0)\n",
            "     If (feature 407 <= 90.0)\n",
            "      Predict: 0.0\n",
            "     Else (feature 407 > 90.0)\n",
            "      Predict: 1.0\n",
            "    Else (feature 518 > 18.0)\n",
            "     If (feature 525 <= 2.0)\n",
            "      Predict: 1.0\n",
            "     Else (feature 525 > 2.0)\n",
            "      Predict: 0.0\n",
            "  Tree 12 (weight 1.0):\n",
            "    If (feature 490 <= 44.5)\n",
            "     If (feature 344 <= 253.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 344 > 253.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 490 > 44.5)\n",
            "     Predict: 1.0\n",
            "  Tree 13 (weight 1.0):\n",
            "    If (feature 490 <= 44.5)\n",
            "     If (feature 351 <= 251.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 351 > 251.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 490 > 44.5)\n",
            "     Predict: 1.0\n",
            "  Tree 14 (weight 1.0):\n",
            "    If (feature 511 <= 1.5)\n",
            "     If (feature 465 <= 87.5)\n",
            "      Predict: 1.0\n",
            "     Else (feature 465 > 87.5)\n",
            "      Predict: 0.0\n",
            "    Else (feature 511 > 1.5)\n",
            "     Predict: 0.0\n",
            "  Tree 15 (weight 1.0):\n",
            "    If (feature 433 <= 66.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 433 > 66.5)\n",
            "     Predict: 1.0\n",
            "  Tree 16 (weight 1.0):\n",
            "    If (feature 517 <= 59.0)\n",
            "     If (feature 379 <= 251.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 379 > 251.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 517 > 59.0)\n",
            "     Predict: 1.0\n",
            "  Tree 17 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     If (feature 320 <= 251.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 320 > 251.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 18 (weight 1.0):\n",
            "    If (feature 406 <= 126.5)\n",
            "     If (feature 295 <= 253.5)\n",
            "      Predict: 0.0\n",
            "     Else (feature 295 > 253.5)\n",
            "      Predict: 1.0\n",
            "    Else (feature 406 > 126.5)\n",
            "     Predict: 1.0\n",
            "  Tree 19 (weight 1.0):\n",
            "    If (feature 517 <= 20.5)\n",
            "     Predict: 0.0\n",
            "    Else (feature 517 > 20.5)\n",
            "     If (feature 457 <= 5.0)\n",
            "      Predict: 1.0\n",
            "     Else (feature 457 > 5.0)\n",
            "      Predict: 0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Using incubator modules: jdk.incubator.vector\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "26/02/22 19:25:57 INFO SparkContext: Running Spark version 4.0.2\n",
            "26/02/22 19:25:57 INFO SparkContext: OS info Linux, 6.6.113+, amd64\n",
            "26/02/22 19:25:57 INFO SparkContext: Java version 17.0.17\n",
            "26/02/22 19:25:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "26/02/22 19:25:58 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 19:25:58 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "26/02/22 19:25:58 INFO ResourceUtils: ==============================================================\n",
            "26/02/22 19:25:58 INFO SparkContext: Submitted application: JavaRandomForestRegressorExample\n",
            "26/02/22 19:25:58 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "26/02/22 19:25:58 INFO ResourceProfile: Limiting resource is cpu\n",
            "26/02/22 19:25:58 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "26/02/22 19:25:58 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 19:25:58 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 19:25:58 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 19:25:58 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 19:25:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 19:25:58 INFO Utils: Successfully started service 'sparkDriver' on port 39161.\n",
            "26/02/22 19:25:58 INFO SparkEnv: Registering MapOutputTracker\n",
            "26/02/22 19:25:58 INFO SparkEnv: Registering BlockManagerMaster\n",
            "26/02/22 19:25:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "26/02/22 19:25:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "26/02/22 19:25:59 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "26/02/22 19:25:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd75c64d-7ab1-44fe-b262-45cb7e04f538\n",
            "26/02/22 19:25:59 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "26/02/22 19:25:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "26/02/22 19:25:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "26/02/22 19:25:59 INFO SparkContext: Added JAR file:/usr/lib/spark/examples/jars/spark-examples_2.12-3.3.4.jar at spark://090e06d941c5:39161/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771788357757\n",
            "26/02/22 19:25:59 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 19:25:59 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 19:25:59 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 19:25:59 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 19:25:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 19:25:59 INFO Executor: Starting executor ID driver on host 090e06d941c5\n",
            "26/02/22 19:25:59 INFO Executor: OS info Linux, 6.6.113+, amd64\n",
            "26/02/22 19:25:59 INFO Executor: Java version 17.0.17\n",
            "26/02/22 19:25:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "26/02/22 19:25:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@286866cb for default.\n",
            "26/02/22 19:25:59 INFO Executor: Fetching spark://090e06d941c5:39161/jars/spark-examples_2.12-3.3.4.jar with timestamp 1771788357757\n",
            "26/02/22 19:26:00 INFO TransportClientFactory: Successfully created connection to 090e06d941c5/172.28.0.12:39161 after 44 ms (0 ms spent in bootstraps)\n",
            "26/02/22 19:26:00 INFO Utils: Fetching spark://090e06d941c5:39161/jars/spark-examples_2.12-3.3.4.jar to /tmp/spark-e01f8bcf-499b-4915-aa92-242fef91516f/userFiles-d7ef87a6-a4a8-40b6-a702-f5901f90b357/fetchFileTemp16309045195739640556.tmp\n",
            "26/02/22 19:26:00 INFO Executor: Adding file:/tmp/spark-e01f8bcf-499b-4915-aa92-242fef91516f/userFiles-d7ef87a6-a4a8-40b6-a702-f5901f90b357/spark-examples_2.12-3.3.4.jar to class loader default\n",
            "26/02/22 19:26:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39883.\n",
            "26/02/22 19:26:00 INFO NettyBlockTransferService: Server created on 090e06d941c5:39883\n",
            "26/02/22 19:26:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "26/02/22 19:26:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 090e06d941c5, 39883, None)\n",
            "26/02/22 19:26:00 INFO BlockManagerMasterEndpoint: Registering block manager 090e06d941c5:39883 with 434.4 MiB RAM, BlockManagerId(driver, 090e06d941c5, 39883, None)\n",
            "26/02/22 19:26:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 090e06d941c5, 39883, None)\n",
            "26/02/22 19:26:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 090e06d941c5, 39883, None)\n",
            "26/02/22 19:26:03 WARN SparkSession: Failed to load session extension\n",
            "java.lang.NoClassDefFoundError: scala/Serializable\n",
            "\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n",
            "\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n",
            "\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)\n",
            "\tat java.base/java.net.URLClassLoader.defineClass(URLClassLoader.java:524)\n",
            "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:427)\n",
            "\tat java.base/java.net.URLClassLoader$1.run(URLClassLoader.java:421)\n",
            "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:420)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
            "\tat org.apache.spark.examples.extensions.SessionExtensionsWithLoader.apply(SessionExtensionsWithLoader.scala:28)\n",
            "\tat org.apache.spark.examples.extensions.SessionExtensionsWithLoader.apply(SessionExtensionsWithLoader.scala:24)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$.org$apache$spark$sql$classic$SparkSession$$loadExtensions(SparkSession.scala:1083)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.build(SparkSession.scala:843)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:859)\n",
            "\tat org.apache.spark.sql.classic.SparkSession$Builder.getOrCreate(SparkSession.scala:732)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:923)\n",
            "\tat org.apache.spark.examples.ml.JavaRandomForestRegressorExample.main(JavaRandomForestRegressorExample.java:39)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1027)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\n",
            "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\n",
            "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
            "Caused by: java.lang.ClassNotFoundException: scala.Serializable\n",
            "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
            "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
            "\t... 30 more\n",
            "26/02/22 19:26:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "26/02/22 19:26:04 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "26/02/22 19:26:05 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.\n",
            "26/02/22 19:26:05 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
            "26/02/22 19:26:05 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "26/02/22 19:26:06 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 19:26:06 INFO FileSourceStrategy: Post-Scan Filters: Set(NOT (length(trim(value#0, None)) = 0), NOT StartsWith(trim(value#0, None), #))\n",
            "26/02/22 19:26:07 INFO CodeGenerator: Code generated in 443.673313 ms\n",
            "26/02/22 19:26:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "26/02/22 19:26:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 213.9 KiB, free 434.2 MiB)\n",
            "26/02/22 19:26:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 434.2 MiB)\n",
            "26/02/22 19:26:08 INFO SparkContext: Created broadcast 0 from rdd at MLUtils.scala:126\n",
            "26/02/22 19:26:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 19:26:08 INFO SparkContext: Starting job: reduce at MLUtils.scala:96\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Got job 0 (reduce at MLUtils.scala:96) with 1 output partitions\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at MLUtils.scala:96)\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:94), which has no missing parents\n",
            "26/02/22 19:26:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 27.0 KiB, free 434.1 MiB)\n",
            "26/02/22 19:26:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 434.1 MiB)\n",
            "26/02/22 19:26:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at map at MLUtils.scala:94) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 19:26:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "26/02/22 19:26:09 INFO CodeGenerator: Code generated in 30.551997 ms\n",
            "26/02/22 19:26:09 INFO CodeGenerator: Code generated in 16.020517 ms\n",
            "26/02/22 19:26:09 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:09 INFO CodeGenerator: Code generated in 13.33206 ms\n",
            "26/02/22 19:26:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1812 bytes result sent to driver\n",
            "26/02/22 19:26:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 653 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:09 INFO DAGScheduler: ResultStage 0 (reduce at MLUtils.scala:96) finished in 954 ms\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:09 INFO TaskSchedulerImpl: Canceling stage 0\n",
            "26/02/22 19:26:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Job 0 finished: reduce at MLUtils.scala:96, took 1053.940295 ms\n",
            "26/02/22 19:26:09 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 19:26:09 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 19:26:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 213.7 KiB, free 433.9 MiB)\n",
            "26/02/22 19:26:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 433.9 MiB)\n",
            "26/02/22 19:26:09 INFO SparkContext: Created broadcast 2 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 19:26:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 19:26:09 INFO SparkContext: Starting job: treeReduce at VectorIndexer.scala:151\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Got job 1 (treeReduce at VectorIndexer.scala:151) with 1 output partitions\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Final stage: ResultStage 1 (treeReduce at VectorIndexer.scala:151)\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:151), which has no missing parents\n",
            "26/02/22 19:26:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.6 KiB, free 433.8 MiB)\n",
            "26/02/22 19:26:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.3 KiB, free 433.8 MiB)\n",
            "26/02/22 19:26:09 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[16] at treeReduce at VectorIndexer.scala:151) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:09 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:09 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 19:26:09 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "26/02/22 19:26:10 INFO CodeGenerator: Code generated in 39.332538 ms\n",
            "26/02/22 19:26:10 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:10 INFO CodeGenerator: Code generated in 50.009235 ms\n",
            "26/02/22 19:26:10 INFO CodeGenerator: Code generated in 45.147598 ms\n",
            "26/02/22 19:26:10 INFO CodeGenerator: Code generated in 98.765737 ms\n",
            "26/02/22 19:26:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 415401 bytes result sent to driver\n",
            "26/02/22 19:26:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 643 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:10 INFO DAGScheduler: ResultStage 1 (treeReduce at VectorIndexer.scala:151) finished in 721 ms\n",
            "26/02/22 19:26:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:10 INFO TaskSchedulerImpl: Canceling stage 1\n",
            "26/02/22 19:26:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "26/02/22 19:26:10 INFO DAGScheduler: Job 1 finished: treeReduce at VectorIndexer.scala:151, took 734.896492 ms\n",
            "26/02/22 19:26:11 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 19:26:11 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 19:26:11 INFO CodeGenerator: Code generated in 97.091768 ms\n",
            "26/02/22 19:26:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 213.7 KiB, free 433.9 MiB)\n",
            "26/02/22 19:26:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 433.9 MiB)\n",
            "26/02/22 19:26:11 INFO SparkContext: Created broadcast 4 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 19:26:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 19:26:11 INFO Instrumentation: [fc9ca94a] Stage class: RandomForestRegressor\n",
            "26/02/22 19:26:11 INFO Instrumentation: [fc9ca94a] Stage uid: rfr_8579b0d5257b\n",
            "26/02/22 19:26:11 INFO Instrumentation: [fc9ca94a] training: numPartitions=1 storageLevel=StorageLevel(1 replicas)\n",
            "26/02/22 19:26:11 INFO Instrumentation: [fc9ca94a] {\"labelCol\":\"label\",\"featuresCol\":\"indexedFeatures\"}\n",
            "26/02/22 19:26:12 INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Got job 2 (take at DecisionTreeMetadata.scala:119) with 1 output partitions\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Final stage: ResultStage 2 (take at DecisionTreeMetadata.scala:119)\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[26] at map at DecisionTreeMetadata.scala:119), which has no missing parents\n",
            "26/02/22 19:26:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 128.4 KiB, free 433.8 MiB)\n",
            "26/02/22 19:26:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 433.7 MiB)\n",
            "26/02/22 19:26:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[26] at map at DecisionTreeMetadata.scala:119) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 19:26:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 86.438438 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 93.582404 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 25.931161 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 54.840342 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 26.932661 ms\n",
            "26/02/22 19:26:12 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 26.796172 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 14.306026 ms\n",
            "26/02/22 19:26:12 INFO CodeGenerator: Code generated in 30.617684 ms\n",
            "26/02/22 19:26:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1865 bytes result sent to driver\n",
            "26/02/22 19:26:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 771 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:12 INFO DAGScheduler: ResultStage 2 (take at DecisionTreeMetadata.scala:119) finished in 814 ms\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:12 INFO TaskSchedulerImpl: Canceling stage 2\n",
            "26/02/22 19:26:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Job 2 finished: take at DecisionTreeMetadata.scala:119, took 825.504147 ms\n",
            "26/02/22 19:26:12 INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Got job 3 (aggregate at DecisionTreeMetadata.scala:125) with 1 output partitions\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Final stage: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125)\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[25] at retag at RandomForest.scala:276), which has no missing parents\n",
            "26/02/22 19:26:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 128.2 KiB, free 433.6 MiB)\n",
            "26/02/22 19:26:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 433.6 MiB)\n",
            "26/02/22 19:26:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at retag at RandomForest.scala:276) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 19:26:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "26/02/22 19:26:12 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1980 bytes result sent to driver\n",
            "26/02/22 19:26:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 233 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:13 INFO DAGScheduler: ResultStage 3 (aggregate at DecisionTreeMetadata.scala:125) finished in 258 ms\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:13 INFO TaskSchedulerImpl: Canceling stage 3\n",
            "26/02/22 19:26:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Job 3 finished: aggregate at DecisionTreeMetadata.scala:125, took 265.314073 ms\n",
            "26/02/22 19:26:13 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1056\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Registering RDD 28 (flatMap at RandomForest.scala:1041) as input to shuffle 0\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Got job 4 (collectAsMap at RandomForest.scala:1056) with 1 output partitions\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Final stage: ResultStage 5 (collectAsMap at RandomForest.scala:1056)\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 4)\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[28] at flatMap at RandomForest.scala:1041), which has no missing parents\n",
            "26/02/22 19:26:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 136.0 KiB, free 433.4 MiB)\n",
            "26/02/22 19:26:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 42.3 KiB, free 433.4 MiB)\n",
            "26/02/22 19:26:13 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[28] at flatMap at RandomForest.scala:1041) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 19:26:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "26/02/22 19:26:13 INFO SecurityManager: Changing view acls to: root\n",
            "26/02/22 19:26:13 INFO SecurityManager: Changing modify acls to: root\n",
            "26/02/22 19:26:13 INFO SecurityManager: Changing view acls groups to: root\n",
            "26/02/22 19:26:13 INFO SecurityManager: Changing modify acls groups to: root\n",
            "26/02/22 19:26:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled\n",
            "26/02/22 19:26:13 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:14 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2202 bytes result sent to driver\n",
            "26/02/22 19:26:14 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1168 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:14 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:14 INFO DAGScheduler: ShuffleMapStage 4 (flatMap at RandomForest.scala:1041) finished in 1215 ms\n",
            "26/02/22 19:26:14 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 19:26:14 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 19:26:14 INFO DAGScheduler: waiting: HashSet(ResultStage 5)\n",
            "26/02/22 19:26:14 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 19:26:14 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[30] at map at RandomForest.scala:1056), which has no missing parents\n",
            "26/02/22 19:26:14 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 18.9 KiB, free 433.4 MiB)\n",
            "26/02/22 19:26:14 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 433.4 MiB)\n",
            "26/02/22 19:26:14 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[30] at map at RandomForest.scala:1056) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:14 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:14 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 19:26:14 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "26/02/22 19:26:14 INFO ShuffleBlockFetcherIterator: Getting 1 (90.5 KiB) non-empty blocks including 1 (90.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 19:26:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 49 ms\n",
            "26/02/22 19:26:15 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 129107 bytes result sent to driver\n",
            "26/02/22 19:26:15 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 568 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:15 INFO DAGScheduler: ResultStage 5 (collectAsMap at RandomForest.scala:1056) finished in 598 ms\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:15 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:15 INFO TaskSchedulerImpl: Canceling stage 5\n",
            "26/02/22 19:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Job 4 finished: collectAsMap at RandomForest.scala:1056, took 1910.174051 ms\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 149.1 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 32.1 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:15 INFO SparkContext: Created broadcast 9 from broadcast at RandomForest.scala:295\n",
            "26/02/22 19:26:15 INFO Instrumentation: [fc9ca94a] {\"numFeatures\":692}\n",
            "26/02/22 19:26:15 INFO Instrumentation: [fc9ca94a] {\"numClasses\":0}\n",
            "26/02/22 19:26:15 INFO Instrumentation: [fc9ca94a] {\"numExamples\":69}\n",
            "26/02/22 19:26:15 INFO Instrumentation: [fc9ca94a] {\"sumOfWeights\":69.0}\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.1 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 13.2 KiB, free 433.1 MiB)\n",
            "26/02/22 19:26:15 INFO SparkContext: Created broadcast 10 from broadcast at RandomForest.scala:624\n",
            "26/02/22 19:26:15 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:665\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Registering RDD 33 (mapPartitions at RandomForest.scala:646) as input to shuffle 1\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Got job 5 (collectAsMap at RandomForest.scala:665) with 1 output partitions\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Final stage: ResultStage 7 (collectAsMap at RandomForest.scala:665)\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[33] at mapPartitions at RandomForest.scala:646), which has no missing parents\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 219.0 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 78.5 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:15 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[33] at mapPartitions at RandomForest.scala:646) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:15 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:15 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 19:26:15 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
            "26/02/22 19:26:15 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:15 INFO MemoryStore: Block rdd_32_0 stored as values in memory (estimated size 198.7 KiB, free 432.7 MiB)\n",
            "26/02/22 19:26:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2245 bytes result sent to driver\n",
            "26/02/22 19:26:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 668 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:16 INFO DAGScheduler: ShuffleMapStage 6 (mapPartitions at RandomForest.scala:646) finished in 718 ms\n",
            "26/02/22 19:26:16 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 19:26:16 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 19:26:16 INFO DAGScheduler: waiting: HashSet(ResultStage 7)\n",
            "26/02/22 19:26:16 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 19:26:16 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[35] at map at RandomForest.scala:665), which has no missing parents\n",
            "26/02/22 19:26:16 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 7.4 KiB, free 432.8 MiB)\n",
            "26/02/22 19:26:16 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 432.8 MiB)\n",
            "26/02/22 19:26:16 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[35] at map at RandomForest.scala:665) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 19:26:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
            "26/02/22 19:26:16 INFO ShuffleBlockFetcherIterator: Getting 1 (378.2 KiB) non-empty blocks including 1 (378.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 19:26:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
            "26/02/22 19:26:17 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 6541 bytes result sent to driver\n",
            "26/02/22 19:26:17 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 779 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:17 INFO DAGScheduler: ResultStage 7 (collectAsMap at RandomForest.scala:665) finished in 799 ms\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Canceling stage 7\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Job 5 finished: collectAsMap at RandomForest.scala:665, took 1547.202881 ms\n",
            "26/02/22 19:26:17 INFO TorrentBroadcast: Destroying Broadcast(10) (from destroy at RandomForest.scala:676)\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 13.4 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:17 INFO SparkContext: Created broadcast 13 from broadcast at RandomForest.scala:624\n",
            "26/02/22 19:26:17 INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:665\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Registering RDD 36 (mapPartitions at RandomForest.scala:646) as input to shuffle 2\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Got job 6 (collectAsMap at RandomForest.scala:665) with 1 output partitions\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Final stage: ResultStage 9 (collectAsMap at RandomForest.scala:665)\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[36] at mapPartitions at RandomForest.scala:646), which has no missing parents\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 217.2 KiB, free 433.0 MiB)\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 76.1 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:17 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[36] at mapPartitions at RandomForest.scala:646) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10420 bytes) \n",
            "26/02/22 19:26:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
            "26/02/22 19:26:17 INFO BlockManager: Found block rdd_32_0 locally\n",
            "26/02/22 19:26:17 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2116 bytes result sent to driver\n",
            "26/02/22 19:26:17 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 70 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:17 INFO DAGScheduler: ShuffleMapStage 8 (mapPartitions at RandomForest.scala:646) finished in 91 ms\n",
            "26/02/22 19:26:17 INFO DAGScheduler: looking for newly runnable stages\n",
            "26/02/22 19:26:17 INFO DAGScheduler: running: HashSet()\n",
            "26/02/22 19:26:17 INFO DAGScheduler: waiting: HashSet(ResultStage 9)\n",
            "26/02/22 19:26:17 INFO DAGScheduler: failed: HashSet()\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[38] at map at RandomForest.scala:665), which has no missing parents\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.6 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:17 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[38] at map at RandomForest.scala:665) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:17 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (090e06d941c5,executor driver, partition 0, NODE_LOCAL, 9637 bytes) \n",
            "26/02/22 19:26:17 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
            "26/02/22 19:26:17 INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "26/02/22 19:26:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "26/02/22 19:26:17 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 5329 bytes result sent to driver\n",
            "26/02/22 19:26:17 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 347 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:17 INFO DAGScheduler: ResultStage 9 (collectAsMap at RandomForest.scala:665) finished in 361 ms\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Canceling stage 9\n",
            "26/02/22 19:26:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "26/02/22 19:26:17 INFO DAGScheduler: Job 6 finished: collectAsMap at RandomForest.scala:665, took 468.254873 ms\n",
            "26/02/22 19:26:17 INFO TorrentBroadcast: Destroying Broadcast(13) (from destroy at RandomForest.scala:676)\n",
            "26/02/22 19:26:17 INFO RandomForest: Internal timing for DecisionTree:\n",
            "26/02/22 19:26:17 INFO RandomForest:   init: 0.008984805\n",
            "  total: 2.269371019\n",
            "  findBestSplits: 2.212721334\n",
            "  chooseSplits: 2.201019501\n",
            "26/02/22 19:26:17 INFO MapPartitionsRDD: Removing RDD 32 from persistence list\n",
            "26/02/22 19:26:17 INFO TorrentBroadcast: Destroying Broadcast(9) (from destroy at RandomForest.scala:307)\n",
            "26/02/22 19:26:17 INFO BlockManager: Removing RDD 32\n",
            "26/02/22 19:26:17 INFO Instrumentation: [fc9ca94a] {\"numFeatures\":692}\n",
            "26/02/22 19:26:17 INFO Instrumentation: [fc9ca94a] training finished\n",
            "26/02/22 19:26:17 INFO Instrumentation: [91109dac] training finished\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 332.9 KiB, free 433.0 MiB)\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 16.6 KiB, free 432.9 MiB)\n",
            "26/02/22 19:26:17 INFO SparkContext: Created broadcast 16 from broadcast at RandomForestRegressor.scala:242\n",
            "26/02/22 19:26:17 INFO Instrumentation: [33118c30] training finished\n",
            "26/02/22 19:26:17 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 19:26:17 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 19:26:17 INFO CodeGenerator: Code generated in 55.066352 ms\n",
            "26/02/22 19:26:17 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 213.7 KiB, free 432.7 MiB)\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 432.7 MiB)\n",
            "26/02/22 19:26:18 INFO SparkContext: Created broadcast 17 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 19:26:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 19:26:18 INFO SparkContext: Starting job: show at JavaRandomForestRegressorExample.java:74\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Got job 7 (show at JavaRandomForestRegressorExample.java:74) with 1 output partitions\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Final stage: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74)\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[42] at show at JavaRandomForestRegressorExample.java:74), which has no missing parents\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 95.0 KiB, free 432.6 MiB)\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 432.6 MiB)\n",
            "26/02/22 19:26:18 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[42] at show at JavaRandomForestRegressorExample.java:74) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:18 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:18 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10459 bytes) \n",
            "26/02/22 19:26:18 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "26/02/22 19:26:18 INFO Executor: Using REPL class URI: spark://090e06d941c5:39161/artifacts/c711a472-f8f2-468c-9cf5-c242c4e03417/classes/\n",
            "26/02/22 19:26:18 INFO Executor: Created or updated repl class loader org.apache.spark.executor.ExecutorClassLoader@15a44fd6 for c711a472-f8f2-468c-9cf5-c242c4e03417.\n",
            "26/02/22 19:26:18 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 90.984359 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 63.093953 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 66.238845 ms\n",
            "26/02/22 19:26:18 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 19.432362 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 41.721019 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 16.629777 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 31.927444 ms\n",
            "26/02/22 19:26:18 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 7296 bytes result sent to driver\n",
            "26/02/22 19:26:18 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 528 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:18 INFO DAGScheduler: ResultStage 10 (show at JavaRandomForestRegressorExample.java:74) finished in 547 ms\n",
            "26/02/22 19:26:18 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:18 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:18 INFO TaskSchedulerImpl: Canceling stage 10\n",
            "26/02/22 19:26:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Job 7 finished: show at JavaRandomForestRegressorExample.java:74, took 556.497635 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 16.834301 ms\n",
            "26/02/22 19:26:18 INFO FileSourceStrategy: Pushed Filters: \n",
            "26/02/22 19:26:18 INFO FileSourceStrategy: Post-Scan Filters: Set()\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 23.335298 ms\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 213.7 KiB, free 432.4 MiB)\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 38.5 KiB, free 432.6 MiB)\n",
            "26/02/22 19:26:18 INFO SparkContext: Created broadcast 19 from broadcast at LibSVMRelation.scala:156\n",
            "26/02/22 19:26:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "26/02/22 19:26:18 INFO BlockManager: Removing RDD 32\n",
            "26/02/22 19:26:18 INFO SparkContext: Starting job: treeAggregate at Statistics.scala:58\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Got job 8 (treeAggregate at Statistics.scala:58) with 1 output partitions\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Final stage: ResultStage 11 (treeAggregate at Statistics.scala:58)\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Parents of final stage: List()\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Missing parents: List()\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[51] at treeAggregate at Statistics.scala:58), which has no missing parents\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 102.0 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:18 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.3 KiB, free 433.2 MiB)\n",
            "26/02/22 19:26:18 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676\n",
            "26/02/22 19:26:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[51] at treeAggregate at Statistics.scala:58) (first 15 tasks are for partitions Vector(0))\n",
            "26/02/22 19:26:18 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
            "26/02/22 19:26:18 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (090e06d941c5,executor driver, partition 0, PROCESS_LOCAL, 10431 bytes) \n",
            "26/02/22 19:26:18 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 30.694594 ms\n",
            "26/02/22 19:26:18 INFO CodeGenerator: Code generated in 11.665263 ms\n",
            "26/02/22 19:26:19 INFO FileScanRDD: Reading File path: file:///content/data/mllib/sample_libsvm_data.txt, range: 0-104736, partition values: [empty row]\n",
            "26/02/22 19:26:19 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3509 bytes result sent to driver\n",
            "26/02/22 19:26:19 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 168 ms on 090e06d941c5 (executor driver) (1/1)\n",
            "26/02/22 19:26:19 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool \n",
            "26/02/22 19:26:19 INFO DAGScheduler: ResultStage 11 (treeAggregate at Statistics.scala:58) finished in 195 ms\n",
            "26/02/22 19:26:19 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "26/02/22 19:26:19 INFO TaskSchedulerImpl: Canceling stage 11\n",
            "26/02/22 19:26:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
            "26/02/22 19:26:19 INFO DAGScheduler: Job 8 finished: treeAggregate at Statistics.scala:58, took 200.259064 ms\n",
            "26/02/22 19:26:19 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at JavaRandomForestRegressorExample.java:88.\n",
            "26/02/22 19:26:19 INFO SparkUI: Stopped Spark web UI at http://090e06d941c5:4040\n",
            "26/02/22 19:26:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "26/02/22 19:26:19 INFO MemoryStore: MemoryStore cleared\n",
            "26/02/22 19:26:19 INFO BlockManager: BlockManager stopped\n",
            "26/02/22 19:26:19 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "26/02/22 19:26:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "26/02/22 19:26:19 INFO SparkContext: Successfully stopped SparkContext\n",
            "26/02/22 19:26:19 INFO ShutdownHookManager: Shutdown hook called\n",
            "26/02/22 19:26:19 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-df1b9284-528b-43e7-97de-d178014f61e2\n",
            "26/02/22 19:26:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-e01f8bcf-499b-4915-aa92-242fef91516f\n",
            "26/02/22 19:26:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-4cbdabec-7be3-4e95-8c76-3bedcc7cf7ff\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this guide, we demonstrated how to install the essential Spark services‚ÄîSpark Core, Spark Master, and Spark Worker‚Äîusing the Bigtop distribution. We also explored how to leverage Bigtop's utilities to easily launch a Spark engine. Additionally, we executed two example jobs included in the Spark package."
      ],
      "metadata": {
        "id": "llnOxcrrzw36"
      }
    }
  ]
}