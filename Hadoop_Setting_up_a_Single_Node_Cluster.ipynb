{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oEF3qldGPj3T",
        "oUuQjW2oNMcJ",
        "qFfOrktMPq8M",
        "ozBfXXK8HoSq",
        "KE7kSYSXQYLf",
        "RlgP1ytnRtUK",
        "KLmxLQeJSb4A",
        "kXbSKFyeMqr2",
        "k2-Fdp73cF0V",
        "-Uxmv3RdUwiF",
        "V68C4cDySyek",
        "HTDPwnVlSbHS",
        "xMrEiLB_VAeR",
        "CKRRbwDFv3ZQ",
        "G3KBe4R65bl1",
        "IF6-Z5RotAcO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
        "\n",
        "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
        "\n",
        "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
        "\n",
        "Finally, we'll proceed to shut down the cluster.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**\n",
        "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
        "\n",
        " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
        "\n",
        " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
        "\n",
        "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
        "\n",
        " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
        "\n",
        " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
        "\n",
        " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
        "\n",
        " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
        "\n",
        "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
        "\n",
        "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
        "\n",
        "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
        "\n",
        "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
        "\n",
        "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
        "\n",
        "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
        "\n",
        "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
        "\n",
        "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
        "\n",
        "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
        "\n",
        "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
        "\n",
        "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
        "\n",
        "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
        "\n",
        "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "hGm3LhVEWXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # â›” Do not run on Google Colab\n",
        "> Run this notebook on any Ubuntu Jammy machine instead."
      ],
      "metadata": {
        "id": "eQSizEcMTLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIIQHLVFTb7M",
        "outputId": "1729a957-cd13-4063-aae5-80133ab0e474"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the available Java version\n",
        " Apache Hadoop 3.4.2 supports Java > 8 (JDK>8). See: https://hadoop.apache.org/docs/r3.4.2/\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "26e80a35-bf46-43b9-cc1f-d9549e1a0cb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 11 || $JAVA_MAJOR_VERSION -eq 17 ]]\n",
        " then\n",
        " echo \"Java version is one of 11, 17 âœ“\"\n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "6eb5e46c-f3f6-443b-8e1f-94c8712a9dca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 11, 17 âœ“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`"
      ],
      "metadata": {
        "id": "ozBfXXK8HoSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "fe65a48b-0585-475a-cc78-516c8bf5c5dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "962e1d09-9e9b-4d6f-ed27-5b3f1f23b992"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract `JAVA_HOME` from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "java_home = subprocess.check_output(\n",
        "    \"readlink -f $(which java) | sed 's:/bin/java$::'\",\n",
        "    shell=True,\n",
        "    text=True\n",
        ").strip()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = java_home\n",
        "\n",
        "print(\"JAVA_HOME =\", os.environ[\"JAVA_HOME\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Z5lsrX-Gbb",
        "outputId": "d8262028-488b-4959-dd7c-bb1b4812416e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAVA_HOME = /usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download core Hadoop\n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
        "\n",
        "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LqS5Rkgyli",
        "outputId": "43a3a055-0357-4726-d9a8-a29f2c58761d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:43:59--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1065831750 (1016M) [application/x-gzip]\n",
            "Saving to: â€˜hadoop-3.4.2.tar.gzâ€™\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]   1016M  88.7MB/s    in 5.9s    \n",
            "\n",
            "2026-02-04 20:44:05 (172 MB/s) - â€˜hadoop-3.4.2.tar.gzâ€™ saved [1065831750/1065831750]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncompress archive"
      ],
      "metadata": {
        "id": "um2CARkgg22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -d \"hadoop-3.4.2\" ]; then\n",
        "  tar xzf hadoop-3.4.2.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "id": "C17WYI0mQRE8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file\n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download sha512 file"
      ],
      "metadata": {
        "id": "ATofMJRXhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "10c6728d-0ea8-4c2a-a112-6a946e169369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:44:26--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160 [text/plain]\n",
            "Saving to: â€˜hadoop-3.4.2.tar.gz.sha512â€™\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]     160  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-04 20:44:26 (2.93 MB/s) - â€˜hadoop-3.4.2.tar.gz.sha512â€™ saved [160/160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare"
      ],
      "metadata": {
        "id": "eL8FxjalhFAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "A=$(sha512sum hadoop-3.4.2.tar.gz | cut - -d' ' -f1)\n",
        "B=$(cut hadoop-3.4.2.tar.gz.sha512 -d' ' -f4)\n",
        "printf \"%s\\n%s\\n\" $A $B\n",
        "[[ $A == $B ]] && echo \"True\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL302M1OhFMH",
        "outputId": "8b4d94b9-545e-4d08-eb71-5da03d417eaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "a97e5dd3-5b28-481a-b297-535de3d3ac66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.2')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "b0acf4e1-e660-44c3-b513-8794d8cc9a0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHELL: /bin/bash\n",
            "COLAB_JUPYTER_TRANSPORT: ipc\n",
            "CGROUP_MEMORY_EVENTS: /sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "VM_GCE_METADATA_HOST: 169.254.169.253\n",
            "MODEL_PROXY_HOST: https://mp.kaggle.net\n",
            "HOSTNAME: 7a779a0ab01e\n",
            "LANGUAGE: en_US\n",
            "TBE_RUNTIME_ADDR: 172.28.0.1:8011\n",
            "GCE_METADATA_TIMEOUT: 3\n",
            "COLAB_JUPYTER_IP: 172.28.0.12\n",
            "COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL: http://172.28.0.1:8013/\n",
            "KMP_LISTEN_PORT: 6000\n",
            "TF_FORCE_GPU_ALLOW_GROWTH: true\n",
            "ENV: /root/.bashrc\n",
            "PWD: /\n",
            "TBE_EPHEM_CREDS_ADDR: 172.28.0.1:8009\n",
            "COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT: 30s\n",
            "TBE_CREDS_ADDR: 172.28.0.1:8008\n",
            "COLAB_JUPYTER_TOKEN: \n",
            "LAST_FORCED_REBUILD: 20250623\n",
            "TCLLIBPATH: /usr/share/tcltk/tcllib1.20\n",
            "COLAB_KERNEL_MANAGER_PROXY_HOST: 172.28.0.12\n",
            "UV_BUILD_CONSTRAINT: \n",
            "COLAB_WARMUP_DEFAULTS: 1\n",
            "HOME: /root\n",
            "LANG: en_US.UTF-8\n",
            "CLOUDSDK_CONFIG: /content/.config\n",
            "UV_SYSTEM_PYTHON: true\n",
            "COLAB_RELEASE_TAG: release-colab-external_20260202-060039_RC01\n",
            "KMP_TARGET_PORT: 9000\n",
            "KMP_EXTRA_ARGS: --logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-3rb4iwacj72d8 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true \n",
            "UV_INSTALL_DIR: /usr/local/bin\n",
            "COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS: /datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "COLAB_KERNEL_MANAGER_PROXY_PORT: 6000\n",
            "CLOUDSDK_PYTHON: python3\n",
            "NO_GCE_CHECK: False\n",
            "PYTHONPATH: /env/python\n",
            "SHLVL: 0\n",
            "COLAB_LANGUAGE_SERVER_PROXY: /usr/colab/bin/language_service\n",
            "UV_CONSTRAINT: \n",
            "PYTHONUTF8: 1\n",
            "COLAB_GPU: \n",
            "GCS_READ_CACHE_BLOCK_SIZE_MB: 16\n",
            "LC_ALL: en_US.UTF-8\n",
            "COLAB_FILE_HANDLER_ADDR: localhost:3453\n",
            "PATH: /content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "COLAB_IMAGE_TYPE: cpu\n",
            "COLAB_DEBUG_ADAPTER_MUX_PATH: /usr/local/bin/dap_multiplexer\n",
            "PYTHONWARNINGS: ignore:::pip._internal.cli.base_command\n",
            "DEBIAN_FRONTEND: noninteractive\n",
            "COLAB_BACKEND_VERSION: next\n",
            "OLDPWD: /\n",
            "_PYVIZ_COMMS_INSTALLED: 1\n",
            "PYDEVD_USE_FRAME_EVAL: NO\n",
            "JPY_SESSION_NAME: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JPY_PARENT_PID: 90\n",
            "TERM: xterm-color\n",
            "CLICOLOR: 1\n",
            "PAGER: cat\n",
            "GIT_PAGER: cat\n",
            "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
            "ENABLE_DIRECTORYPREFETCHER: 1\n",
            "USE_AUTH_EPHEM: 1\n",
            "COLAB_NOTEBOOK_ID: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n",
            "HADOOP_HOME: /content/hadoop-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "053b48e4-3bf6-47b0-d7af-a0fd32c6cf04"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/core-site.xml > /dev/null\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/hdfs-site.xml > /dev/null\n"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat hadoop-3.4.2/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "8a0f6a0c-2f0f-4521-d362-1cbf4f3b418f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.2/sbin`).\n",
        "```\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -n hadoop-3.4.2/etc/hadoop/hadoop-env.sh hadoop-3.4.2/etc/hadoop/hadoop-env.sh.org\n",
        "cat <<ðŸ˜ƒ >hadoop-3.4.2/etc/hadoop/hadoop-env.sh\n",
        "export JAVA_HOME=$JAVA_HOME\n",
        "export HADOOP_HOME=$HADOOP_HOME\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless access to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Install `openssh` and start server\n",
        "\n",
        "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
        "\n",
        "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install openssh-server\n",
        "# tee -a appends to the file using elevated privileges\n",
        "echo 'StrictHostKeyChecking no' | sudo tee -a /etc/ssh/ssh_config\n",
        "sudo /etc/init.d/ssh restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "0155e5c3-17e7-4d7e-a96b-baa7abf8d534"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:5 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,696 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,891 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,009 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,597 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,608 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,297 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,677 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,388 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Fetched 36.9 MB in 5s (7,039 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  ncurses-term openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere ssh-askpass ufw\n",
            "The following NEW packages will be installed:\n",
            "  ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 4 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 751 kB of archives.\n",
            "After this operation, 6,050 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-sftp-server amd64 1:8.9p1-3ubuntu0.13 [38.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-server amd64 1:8.9p1-3ubuntu0.13 [435 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ncurses-term all 6.3-2ubuntu0.1 [267 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssh-import-id all 5.11-0ubuntu1 [10.1 kB]\n",
            "Fetched 751 kB in 0s (3,352 kB/s)\n",
            "Selecting previously unselected package openssh-sftp-server.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 117540 files and directories currently installed.)\r\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package openssh-server.\r\n",
            "Preparing to unpack .../openssh-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package ncurses-term.\r\n",
            "Preparing to unpack .../ncurses-term_6.3-2ubuntu0.1_all.deb ...\r\n",
            "Unpacking ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package ssh-import-id.\r\n",
            "Preparing to unpack .../ssh-import-id_5.11-0ubuntu1_all.deb ...\r\n",
            "Unpacking ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Setting up openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "debconf: unable to initialize frontend: Dialog\r\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n",
            "debconf: falling back to frontend: Readline\r\n",
            "\r\n",
            "Creating config file /etc/ssh/sshd_config with new version\r\n",
            "Creating SSH2 RSA key; this may take some time ...\r\n",
            "3072 SHA256:nrwhsfoLfj9EzulEnOv0s4l7n7GIEMAG94+yQYqxM1I root@7a779a0ab01e (RSA)\r\n",
            "Creating SSH2 ECDSA key; this may take some time ...\r\n",
            "256 SHA256:e7IEqSjXGkNfTW4YeMkth3rdRwUdl6KBSFu7bfTTO4w root@7a779a0ab01e (ECDSA)\r\n",
            "Creating SSH2 ED25519 key; this may take some time ...\r\n",
            "256 SHA256:OB1UNS1HXZH8IpZnmFcy9S3qJ7TlNWci6LlthkhGDNQ root@7a779a0ab01e (ED25519)\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Created symlink /etc/systemd/system/sshd.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Setting up ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "StrictHostKeyChecking no\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate key\n",
        "Generate an SSH key that does not require a password.\n",
        "\n",
        "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
        "\n",
        "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm $HOME/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "a22da349-3294-4965-873c-513b371d22cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:mNsFMbLbp0RUPAjTz7ngfDDnh8OXBpI2rkNYz3AuZ+g root@7a779a0ab01e\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|      +o++.      |\n",
            "|       =ooo      |\n",
            "|      . o+ o     |\n",
            "|      o*X.*      |\n",
            "|     o+%SXo+ .   |\n",
            "|    . ++X+B =    |\n",
            "|     o.=o. =     |\n",
            "|      E          |\n",
            "|       .         |\n",
            "+----[SHA256]-----+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n",
            "Created directory '/root/.ssh'.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi ðŸ‘‹\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "fb9c2769-d2aa-46f5-9654-3fe35faefb4d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            "hi ðŸ‘‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo env JAVA_HOME=$JAVA_HOME $HADOOP_HOME/bin/hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "d987d6bd-e1dc-4c1e-d485-c12d1b1d8b96",
        "collapsed": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /content/hadoop-3.4.2/logs does not exist. Creating.\n",
            "2026-02-04 20:44:47,734 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 7a779a0ab01e/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -nonInteractive]\n",
            "STARTUP_MSG:   version = 3.4.2\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.4.2/etc/hadoop:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-shaded-guava-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-auth-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-util-ajax-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/bcprov-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-sctp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-annotations-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/zookeeper-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/nimbus-jose-jwt-9.37.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-udt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-logging-1.3.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-json-1.22.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-memcache-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-compress-1.26.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-configuration2-2.10.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/dnsjava-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-mqtt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-xml-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-xml-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-cli-1.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-webapp-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-collections4-4.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-proxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-haproxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-io-2.16.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/avro-1.11.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-redis-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-smtp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-rxtx-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-io-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-stomp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-all-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-http-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-security-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-http-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-buffer-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-lang3-3.17.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-socks-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/zookeeper-jute-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-util-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-http2-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-nfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-kms-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-common-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-registry-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-auth-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-annotations-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/zookeeper-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-udt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-logging-1.3.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-json-1.22.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-compress-1.26.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-configuration2-2.10.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/dnsjava-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-xml-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-xml-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-cli-1.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-webapp-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-collections4-4.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-io-2.16.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/avro-1.11.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-redis-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-io-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-all-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-http-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-security-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-http-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-buffer-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-lang3-3.17.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-socks-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/zookeeper-jute-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-util-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-http2-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-client-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn:/content/hadoop-3.4.2/share/hadoop/yarn/lib/guice-4.2.3.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/asm-commons-9.7.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-client-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jaxb-runtime-2.3.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/cache-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/ehcache-3.8.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/asm-tree-9.7.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/stax-ex-1.8.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-annotations-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/FastInfoset-1.2.15.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/istack-commons-runtime-3.0.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/txw2-2.3.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.activation-api-1.2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/codemodel-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/bcutil-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-common-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-client-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-api-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-jndi-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/bcpkix-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-plus-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-api-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-services-api-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-services-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-router-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-registry-3.4.2.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c; compiled by 'ahmarsu' on 2025-08-20T10:30Z\n",
            "STARTUP_MSG:   java = 17.0.17\n",
            "************************************************************/\n",
            "2026-02-04 20:44:47,783 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2026-02-04 20:44:47,992 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\n",
            "2026-02-04 20:44:48,985 INFO namenode.NameNode: Formatting using clusterid: CID-71db1618-3815-44e3-abe2-cc445a6edfe6\n",
            "2026-02-04 20:44:49,036 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2026-02-04 20:44:49,082 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2026-02-04 20:44:49,084 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2026-02-04 20:44:49,084 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2026-02-04 20:44:49,138 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2026-02-04 20:44:49,139 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2026-02-04 20:44:49,139 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2026-02-04 20:44:49,139 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2026-02-04 20:44:49,139 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2026-02-04 20:44:49,204 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2026-02-04 20:44:49,358 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2026-02-04 20:44:49,369 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2026-02-04 20:44:49,370 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2026-02-04 20:44:49,373 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2026-02-04 20:44:49,373 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 04 20:44:49\n",
            "2026-02-04 20:44:49,375 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2026-02-04 20:44:49,375 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:44:49,377 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2026-02-04 20:44:49,377 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2026-02-04 20:44:49,407 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2026-02-04 20:44:49,407 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2026-02-04 20:44:49,413 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\n",
            "2026-02-04 20:44:49,413 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2026-02-04 20:44:49,413 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2026-02-04 20:44:49,413 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2026-02-04 20:44:49,414 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2026-02-04 20:44:49,414 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2026-02-04 20:44:49,414 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2026-02-04 20:44:49,415 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2026-02-04 20:44:49,415 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2026-02-04 20:44:49,415 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2026-02-04 20:44:49,415 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2026-02-04 20:44:49,458 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2026-02-04 20:44:49,458 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:44:49,458 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:44:49,458 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:44:49,469 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2026-02-04 20:44:49,469 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:44:49,470 INFO util.GSet: 1.0% max memory 3.2 GB = 32.4 MB\n",
            "2026-02-04 20:44:49,470 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2026-02-04 20:44:49,481 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2026-02-04 20:44:49,482 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2026-02-04 20:44:49,482 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2026-02-04 20:44:49,482 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2026-02-04 20:44:49,488 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\n",
            "2026-02-04 20:44:49,489 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\n",
            "2026-02-04 20:44:49,491 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2026-02-04 20:44:49,498 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2026-02-04 20:44:49,499 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:44:49,499 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2026-02-04 20:44:49,500 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2026-02-04 20:44:49,512 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2026-02-04 20:44:49,512 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2026-02-04 20:44:49,512 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2026-02-04 20:44:49,517 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2026-02-04 20:44:49,517 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2026-02-04 20:44:49,519 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2026-02-04 20:44:49,520 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:44:49,520 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 996.6 KB\n",
            "2026-02-04 20:44:49,520 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2026-02-04 20:44:49,551 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1140391754-172.28.0.12-1770237889542\n",
            "2026-02-04 20:44:49,603 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2026-02-04 20:44:49,651 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2026-02-04 20:44:49,797 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2026-02-04 20:44:49,821 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2026-02-04 20:44:49,829 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2026-02-04 20:44:49,867 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2026-02-04 20:44:49,867 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2026-02-04 20:44:49,873 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2026-02-04 20:44:49,874 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 7a779a0ab01e/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo env JAVA_HOME=$JAVA_HOME $HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "3b5c1145-9df0-4b20-8ca6-6f079d66c922"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [7a779a0ab01e]\n",
            "7a779a0ab01e: Warning: Permanently added '7a779a0ab01e' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Check if HDFS is in safe mode\n",
        "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
        "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
        "  hdfs dfsadmin -safemode leave\n",
        "else\n",
        "  echo \"Namenode is not in safe mode.\"\n",
        "fi"
      ],
      "metadata": {
        "id": "pOHsiWv9or7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ef0c65-55c0-4f0c-d527-becca85ddc4c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namenode is not in safe mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple HDFS commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home\n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# if sampls_data does not exist, create it\n",
        "mkdir -p sample_data\n",
        "touch sample_data/mnist_test.csv\n",
        "\n",
        "# Check if the file is empty and fill it if needed\n",
        "if [ ! -s sample_data/mnist_test.csv ]; then\n",
        "  echo -e \"0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\" > sample_data/mnist_test.csv\n",
        "fi\n",
        "\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "48b5c668-4f65-4289-af03-e2d44a0954a9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2026-02-04 20:45 my_dir/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple MapReduce jobs\n",
        "\n",
        "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
        "\n",
        "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest MapReduce job\n",
        "\n",
        "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job"
      ],
      "metadata": {
        "id": "o6ICcKO2jcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output_simplest \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e60116-796e-4582-a5a3-27bbcc1e02d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_simplest': No such file or directory\n",
            "namenode is running as process 1722.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "2026-02-04 20:45:33,243 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:45:33,722 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:45:33,754 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:45:34,114 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local433737235_0001\n",
            "2026-02-04 20:45:34,116 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:45:34,398 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:45:34,400 INFO mapreduce.Job: Running job: job_local433737235_0001\n",
            "2026-02-04 20:45:34,402 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:45:34,404 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:45:34,414 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:45:34,414 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:45:34,481 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:45:34,483 INFO mapred.LocalJobRunner: Starting task: attempt_local433737235_0001_m_000000_0\n",
            "2026-02-04 20:45:34,526 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:45:34,526 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:45:34,554 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:45:34,564 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+18289443\n",
            "2026-02-04 20:45:34,611 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 20:45:34,692 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 20:45:34,692 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 20:45:34,692 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 20:45:34,692 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 20:45:34,692 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 20:45:34,696 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 20:45:34,699 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-04 20:45:34,707 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:45:34,710 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:45:34,712 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:45:34,712 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:45:34,713 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:45:34,713 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:45:34,715 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:45:34,715 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:45:34,715 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:45:34,716 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:45:34,717 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:45:34,717 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:45:34,920 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:34,922 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:34,946 INFO streaming.PipeMapRed: Records R/W=98/1\n",
            "2026-02-04 20:45:34,946 INFO streaming.PipeMapRed: R/W/S=100/1/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,080 INFO streaming.PipeMapRed: R/W/S=1000/789/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,398 INFO streaming.PipeMapRed: R/W/S=10000/9962/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,404 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:45:35,409 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:45:35,413 INFO mapreduce.Job: Job job_local433737235_0001 running in uber mode : false\n",
            "2026-02-04 20:45:35,414 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2026-02-04 20:45:35,418 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:45:35,420 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 20:45:35,420 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 20:45:35,420 INFO mapred.MapTask: bufstart = 0; bufend = 18319443; bufvoid = 104857600\n",
            "2026-02-04 20:45:35,420 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
            "2026-02-04 20:45:35,564 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 20:45:35,586 INFO mapred.Task: Task:attempt_local433737235_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:45:35,592 INFO mapred.LocalJobRunner: Records R/W=98/1\n",
            "2026-02-04 20:45:35,592 INFO mapred.Task: Task 'attempt_local433737235_0001_m_000000_0' done.\n",
            "2026-02-04 20:45:35,600 INFO mapred.Task: Final Counters for attempt_local433737235_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141963\n",
            "\t\tFILE: Number of bytes written=19217565\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=45\n",
            "\t\tTotal committed heap usage (bytes)=228589568\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "2026-02-04 20:45:35,600 INFO mapred.LocalJobRunner: Finishing task: attempt_local433737235_0001_m_000000_0\n",
            "2026-02-04 20:45:35,601 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:45:35,605 INFO mapred.LocalJobRunner: Starting task: attempt_local433737235_0001_r_000000_0\n",
            "2026-02-04 20:45:35,605 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 20:45:35,614 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:45:35,614 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:45:35,614 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:45:35,617 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2fdb18d5\n",
            "2026-02-04 20:45:35,621 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:45:35,639 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 20:45:35,643 INFO reduce.EventFetcher: attempt_local433737235_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 20:45:35,694 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local433737235_0001_m_000000_0 decomp: 18359445 len: 18359449 to MEMORY\n",
            "2026-02-04 20:45:35,722 INFO reduce.InMemoryMapOutput: Read 18359445 bytes from map-output for attempt_local433737235_0001_m_000000_0\n",
            "2026-02-04 20:45:35,725 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18359445, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18359445\n",
            "2026-02-04 20:45:35,726 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 20:45:35,727 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:45:35,728 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 20:45:35,735 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:45:35,735 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 20:45:35,822 INFO reduce.MergeManagerImpl: Merged 1 segments, 18359445 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 20:45:35,823 INFO reduce.MergeManagerImpl: Merging 1 files, 18359449 bytes from disk\n",
            "2026-02-04 20:45:35,824 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 20:45:35,824 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:45:35,825 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 20:45:35,826 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:45:35,828 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2026-02-04 20:45:35,847 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2026-02-04 20:45:35,856 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2026-02-04 20:45:35,912 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,913 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,917 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:35,966 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:36,210 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:36,212 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:45:36,213 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
            "2026-02-04 20:45:36,214 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:45:36,299 INFO mapred.Task: Task:attempt_local433737235_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:45:36,305 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:45:36,305 INFO mapred.Task: Task attempt_local433737235_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 20:45:36,335 INFO output.FileOutputCommitter: Saved output of task 'attempt_local433737235_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n",
            "2026-02-04 20:45:36,336 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
            "2026-02-04 20:45:36,337 INFO mapred.Task: Task 'attempt_local433737235_0001_r_000000_0' done.\n",
            "2026-02-04 20:45:36,338 INFO mapred.Task: Final Counters for attempt_local433737235_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36860893\n",
            "\t\tFILE: Number of bytes written=37577014\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=10000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=274726912\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 20:45:36,338 INFO mapred.LocalJobRunner: Finishing task: attempt_local433737235_0001_r_000000_0\n",
            "2026-02-04 20:45:36,338 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 20:45:36,424 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 20:45:36,425 INFO mapreduce.Job: Job job_local433737235_0001 completed successfully\n",
            "2026-02-04 20:45:36,446 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=37002856\n",
            "\t\tFILE: Number of bytes written=56794579\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=36578886\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=20000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=57\n",
            "\t\tTotal committed heap usage (bytes)=503316480\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 20:45:36,446 INFO streaming.StreamJob: Output directory: output_simplest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_simplest/part-00000"
      ],
      "metadata": {
        "id": "rB7FXYTbwNzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cd178b-fe9f-40bd-9d4d-a6553d78e787"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000   10000 18299443\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "id": "yJIm4SPZFPxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85caa884-f7e0-4928-8dd2-f9f7559bb260"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:45:39--  https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216485 (211K) [text/plain]\n",
            "Saving to: â€˜Linux_2k.logâ€™\n",
            "\n",
            "\rLinux_2k.log          0%[                    ]       0  --.-KB/s               \rLinux_2k.log        100%[===================>] 211.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-02-04 20:45:39 (8.23 MB/s) - â€˜Linux_2k.logâ€™ saved [216485/216485]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input || true\n",
        "hdfs dfs -put Linux_2k.log input/ || true"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "id": "4-rraIUdfdj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16a9301-f4a5-44b5-e592-3cc0f86e5358"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "id": "9qf1dFqIKgoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0796b98-f23a-419f-8169-85fc76bf3371"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py"
      ],
      "metadata": {
        "id": "TJ0kDRsigCC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9d9d32-c079-4580-9b95-7d76ce93412b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\n",
            "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\n",
            "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_filter\n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output_filter \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE\n"
      ],
      "metadata": {
        "id": "G7SEzMC2OqWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0713bae7-8cf5-4724-c80c-436f24a5c728"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py] [] /tmp/streamjob15454486606118402166.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_filter': No such file or directory\n",
            "2026-02-04 20:45:50,371 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 20:45:52,058 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:45:52,519 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:45:52,550 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:45:52,878 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local593607530_0001\n",
            "2026-02-04 20:45:52,879 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:45:53,209 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local593607530_0001_6ec44ffd-17be-4525-98fc-86d0ab0a1684/mapper.py\n",
            "2026-02-04 20:45:53,396 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:45:53,399 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:45:53,400 INFO mapreduce.Job: Running job: job_local593607530_0001\n",
            "2026-02-04 20:45:53,402 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:45:53,413 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:45:53,413 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:45:53,484 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:45:53,489 INFO mapred.LocalJobRunner: Starting task: attempt_local593607530_0001_m_000000_0\n",
            "2026-02-04 20:45:53,527 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:45:53,528 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:45:53,553 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:45:53,570 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 20:45:53,602 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-04 20:45:53,698 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2026-02-04 20:45:53,705 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:45:53,706 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:45:53,706 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:45:53,707 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:45:53,707 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:45:53,707 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:45:53,709 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:45:53,709 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:45:53,709 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:45:53,709 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:45:53,710 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:45:53,711 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:45:53,862 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:53,864 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:53,869 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:53,881 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:45:53,885 INFO streaming.PipeMapRed: Records R/W=1201/1\n",
            "2026-02-04 20:45:53,920 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:45:53,923 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:45:53,928 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:45:54,006 INFO mapred.Task: Task:attempt_local593607530_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:45:54,012 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:45:54,012 INFO mapred.Task: Task attempt_local593607530_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-04 20:45:54,043 INFO output.FileOutputCommitter: Saved output of task 'attempt_local593607530_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n",
            "2026-02-04 20:45:54,044 INFO mapred.LocalJobRunner: Records R/W=1201/1\n",
            "2026-02-04 20:45:54,045 INFO mapred.Task: Task 'attempt_local593607530_0001_m_000000_0' done.\n",
            "2026-02-04 20:45:54,051 INFO mapred.Task: Final Counters for attempt_local593607530_0001_m_000000_0: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=717078\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 20:45:54,052 INFO mapred.LocalJobRunner: Finishing task: attempt_local593607530_0001_m_000000_0\n",
            "2026-02-04 20:45:54,052 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:45:54,410 INFO mapreduce.Job: Job job_local593607530_0001 running in uber mode : false\n",
            "2026-02-04 20:45:54,411 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 20:45:54,414 INFO mapreduce.Job: Job job_local593607530_0001 completed successfully\n",
            "2026-02-04 20:45:54,421 INFO mapreduce.Job: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=717078\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 20:45:54,421 INFO streaming.StreamJob: Output directory: output_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output_filter"
      ],
      "metadata": {
        "id": "RhLA5HZEhfmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ea1480-e022-461b-9794-6b08524ca054"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 20:45 output_filter/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      85436 2026-02-04 20:45 output_filter/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_filter/part-00000 |head"
      ],
      "metadata": {
        "id": "Ffi4RvXnPH14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462a3f71-ff0b-4d0a-d5d4-e7a49d269cd0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "id": "fMKEqUF1T-v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8840ed7-f937-4092-e1b8-7ccaad29f2d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper"
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "k-R7VNoTVRjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c8ac9b-804c-4713-d447-6c7c48fb74ce"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:logrotate:\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "chmod +x myAggregatorForKeyCount.py\n",
        "\n",
        "hdfs dfs -rm -r output_aggregate\n",
        "\n",
        "mapred streaming \\\n",
        "  -input input \\\n",
        "  -output output_aggregate \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate \\\n",
        "  -file myAggregatorForKeyCount.py\n"
      ],
      "metadata": {
        "id": "XwxHJ7yyVR34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "521264b6-f1ee-4140-db92-2059d8cbbb0a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [myAggregatorForKeyCount.py] [] /tmp/streamjob13926960731478249436.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_aggregate': No such file or directory\n",
            "2026-02-04 20:46:05,123 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 20:46:06,863 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:46:07,377 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:46:07,404 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:46:07,730 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1472963981_0001\n",
            "2026-02-04 20:46:07,732 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:46:08,090 INFO mapred.LocalDistributedCacheManager: Localized file:/content/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local1472963981_0001_5b18ad8d-1d2b-4c2d-9aa0-ad10a5c63e4f/myAggregatorForKeyCount.py\n",
            "2026-02-04 20:46:08,232 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:46:08,234 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:46:08,235 INFO mapreduce.Job: Running job: job_local1472963981_0001\n",
            "2026-02-04 20:46:08,236 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:46:08,248 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:46:08,249 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:46:08,309 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:46:08,312 INFO mapred.LocalJobRunner: Starting task: attempt_local1472963981_0001_m_000000_0\n",
            "2026-02-04 20:46:08,341 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:46:08,341 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:46:08,362 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:46:08,370 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 20:46:08,400 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 20:46:08,497 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 20:46:08,498 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 20:46:08,498 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 20:46:08,498 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 20:46:08,498 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 20:46:08,503 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 20:46:08,511 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./myAggregatorForKeyCount.py]\n",
            "2026-02-04 20:46:08,518 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:46:08,519 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:46:08,519 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:46:08,520 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:46:08,520 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:46:08,521 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:46:08,522 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:46:08,522 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:46:08,523 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:46:08,523 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:46:08,524 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:46:08,525 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:46:08,730 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:46:08,730 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:46:08,732 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:46:08,743 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:46:08,750 INFO streaming.PipeMapRed: Records R/W=1435/1\n",
            "2026-02-04 20:46:08,784 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:46:08,786 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:46:08,790 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:46:08,790 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 20:46:08,790 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 20:46:08,790 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n",
            "2026-02-04 20:46:08,790 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n",
            "2026-02-04 20:46:08,917 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 20:46:08,944 INFO mapred.Task: Task:attempt_local1472963981_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:46:08,966 INFO mapred.LocalJobRunner: Records R/W=1435/1\n",
            "2026-02-04 20:46:08,968 INFO mapred.Task: Task 'attempt_local1472963981_0001_m_000000_0' done.\n",
            "2026-02-04 20:46:08,978 INFO mapred.Task: Final Counters for attempt_local1472963981_0001_m_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058\n",
            "\t\tFILE: Number of bytes written=721999\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=195035136\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "2026-02-04 20:46:08,978 INFO mapred.LocalJobRunner: Finishing task: attempt_local1472963981_0001_m_000000_0\n",
            "2026-02-04 20:46:08,979 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:46:08,983 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 20:46:08,985 INFO mapred.LocalJobRunner: Starting task: attempt_local1472963981_0001_r_000000_0\n",
            "2026-02-04 20:46:08,995 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:46:08,996 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:46:08,996 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:46:09,000 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@17538a03\n",
            "2026-02-04 20:46:09,002 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:46:09,024 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 20:46:09,026 INFO reduce.EventFetcher: attempt_local1472963981_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 20:46:09,089 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1472963981_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n",
            "2026-02-04 20:46:09,093 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local1472963981_0001_m_000000_0\n",
            "2026-02-04 20:46:09,096 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n",
            "2026-02-04 20:46:09,097 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 20:46:09,098 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:46:09,098 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 20:46:09,108 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:46:09,108 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 20:46:09,116 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 20:46:09,117 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n",
            "2026-02-04 20:46:09,117 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 20:46:09,119 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:46:09,121 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 20:46:09,122 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:46:09,246 INFO mapreduce.Job: Job job_local1472963981_0001 running in uber mode : false\n",
            "2026-02-04 20:46:09,248 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 20:46:09,267 INFO mapred.Task: Task:attempt_local1472963981_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:46:09,272 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:46:09,272 INFO mapred.Task: Task attempt_local1472963981_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 20:46:09,307 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1472963981_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n",
            "2026-02-04 20:46:09,309 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-04 20:46:09,309 INFO mapred.Task: Task 'attempt_local1472963981_0001_r_000000_0' done.\n",
            "2026-02-04 20:46:09,310 INFO mapred.Task: Final Counters for attempt_local1472963981_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2654\n",
            "\t\tFILE: Number of bytes written=722781\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=195035136\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 20:46:09,310 INFO mapred.LocalJobRunner: Finishing task: attempt_local1472963981_0001_r_000000_0\n",
            "2026-02-04 20:46:09,311 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 20:46:10,250 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 20:46:10,251 INFO mapreduce.Job: Job job_local1472963981_0001 completed successfully\n",
            "2026-02-04 20:46:10,261 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3712\n",
            "\t\tFILE: Number of bytes written=1444780\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=432970\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=60\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=17\n",
            "\t\tTotal committed heap usage (bytes)=390070272\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 20:46:10,261 INFO streaming.StreamJob: Output directory: output_aggregate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output_aggregate\n",
        "hdfs dfs -cat output_aggregate/part-00000"
      ],
      "metadata": {
        "id": "ET3KCfX1UC2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702af54c-4b20-4d5e-bc4c-cb318e465bd8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 20:46 output_aggregate/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        326 2026-02-04 20:46 output_aggregate/part-00000\n",
            "--\t1\n",
            "bluetooth:\t2\n",
            "cups:\t12\n",
            "ftpd\t916\n",
            "gdm(pam_unix)\t2\n",
            "gdm-binary\t1\n",
            "gpm\t2\n",
            "hcid\t1\n",
            "irqbalance:\t1\n",
            "kernel:\t76\n",
            "klogind\t46\n",
            "login(pam_unix)\t2\n",
            "logrotate:\t43\n",
            "named\t16\n",
            "network:\t2\n",
            "nfslock:\t1\n",
            "portmap:\t1\n",
            "random:\t1\n",
            "rc:\t1\n",
            "rpc.statd\t1\n",
            "rpcidmapd:\t1\n",
            "sdpd\t1\n",
            "snmpd\t1\n",
            "sshd(pam_unix)\t677\n",
            "su(pam_unix)\t172\n",
            "sysctl:\t1\n",
            "syslog:\t2\n",
            "syslogd\t7\n",
            "udev\t8\n",
            "xinetd\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
        "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
        "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
      ],
      "metadata": {
        "id": "Y8IYl4hAZhZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cbca3cb-b889-4b52-8e3e-6a1eca9be869"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftpd                 916\n",
            "sshd(pam_unix)       677\n",
            "su(pam_unix)         172\n",
            "kernel:              76\n",
            "klogind              46\n",
            "logrotate:           43\n",
            "named                16\n",
            "cups:                12\n",
            "udev                 8\n",
            "syslogd              7\n",
            "bluetooth:           2\n",
            "gdm(pam_unix)        2\n",
            "gpm                  2\n",
            "login(pam_unix)      2\n",
            "network:             2\n",
            "syslog:              2\n",
            "xinetd               2\n",
            "--                   1\n",
            "gdm-binary           1\n",
            "hcid                 1\n",
            "irqbalance:          1\n",
            "nfslock:             1\n",
            "portmap:             1\n",
            "random:              1\n",
            "rc:                  1\n",
            "rpcidmapd:           1\n",
            "rpc.statd            1\n",
            "sdpd                 1\n",
            "snmpd                1\n",
            "sysctl:              1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.4.2/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "id": "IoIYG5NlsIMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90b9fec-3831-4bc6-d60b-decc12d05d8e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [7a779a0ab01e]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop the `sshd` daemon"
      ],
      "metadata": {
        "id": "RGj96_e2ccZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/init.d/ssh stop"
      ],
      "metadata": {
        "id": "FUvKMpy6chQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455dd03e-37ea-4079-d593-206d862b26b8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding remarks\n",
        "\n",
        "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
        "\n",
        "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
        "\n",
        "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
        "\n",
        "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
        "\n",
        "If you found this notebook helpful, consider exploring:\n",
        " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
        " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
        " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5N7tb0HSbZB"
      }
    }
  ]
}