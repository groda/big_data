{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oEF3qldGPj3T",
        "oUuQjW2oNMcJ",
        "qFfOrktMPq8M",
        "ozBfXXK8HoSq",
        "KE7kSYSXQYLf",
        "RlgP1ytnRtUK",
        "KLmxLQeJSb4A",
        "kXbSKFyeMqr2",
        "k2-Fdp73cF0V",
        "-Uxmv3RdUwiF",
        "V68C4cDySyek",
        "HTDPwnVlSbHS",
        "xMrEiLB_VAeR",
        "CKRRbwDFv3ZQ",
        "G3KBe4R65bl1",
        "IF6-Z5RotAcO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
        "\n",
        "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
        "\n",
        "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
        "\n",
        "Finally, we'll proceed to shut down the cluster.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**\n",
        "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
        "\n",
        " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
        "\n",
        " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
        "\n",
        "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
        "\n",
        " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
        "\n",
        " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
        "\n",
        " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
        "\n",
        " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
        "\n",
        "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
        "\n",
        "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
        "\n",
        "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
        "\n",
        "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
        "\n",
        "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
        "\n",
        "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
        "\n",
        "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
        "\n",
        "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
        "\n",
        "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
        "\n",
        "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
        "\n",
        "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
        "\n",
        "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
        "\n",
        "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "hGm3LhVEWXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> # â›” Do not run on Google Colab\n",
        "> Run this notebook on any Ubuntu Jammy machine instead."
      ],
      "metadata": {
        "id": "eQSizEcMTLYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIIQHLVFTb7M",
        "outputId": "4f4f9a28-b4d6-4e22-a04c-4cc512cc661d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.5 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.5 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the available Java version\n",
        " Apache Hadoop 3.4.2 supports Java > 8 (JDK>8). See: https://hadoop.apache.org/docs/r3.4.2/\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "70968237-2561-4460-89fa-b828783e1c41"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 11 || $JAVA_MAJOR_VERSION -eq 17 ]]\n",
        " then\n",
        " echo \"Java version is one of 11, 17 âœ“\"\n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "933cad12-11c4-443f-8964-5ce4ae60c2b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 11, 17 âœ“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`"
      ],
      "metadata": {
        "id": "ozBfXXK8HoSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "af76a098-a8f1-4838-c3c4-98fd592789a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "15165c46-b733-4af3-ca1e-04ea01ae2f34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract `JAVA_HOME` from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "java_home = subprocess.check_output(\n",
        "    \"readlink -f $(which java) | sed 's:/bin/java$::'\",\n",
        "    shell=True,\n",
        "    text=True\n",
        ").strip()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = java_home\n",
        "\n",
        "print(\"JAVA_HOME =\", os.environ[\"JAVA_HOME\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Z5lsrX-Gbb",
        "outputId": "bb69bd0a-ce1f-4534-8408-5f96f7a0d3d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAVA_HOME = /usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download core Hadoop\n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
        "\n",
        "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LqS5Rkgyli",
        "outputId": "5f9da48e-4f20-4da9-9414-472d21520776"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:09:48--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1065831750 (1016M) [application/x-gzip]\n",
            "Saving to: â€˜hadoop-3.4.2.tar.gzâ€™\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]   1016M  10.4MB/s    in 95s     \n",
            "\n",
            "2026-02-04 20:11:23 (10.7 MB/s) - â€˜hadoop-3.4.2.tar.gzâ€™ saved [1065831750/1065831750]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncompress archive"
      ],
      "metadata": {
        "id": "um2CARkgg22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -d \"hadoop-3.4.2\" ]; then\n",
        "  tar xzf hadoop-3.4.2.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "id": "C17WYI0mQRE8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file\n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download sha512 file"
      ],
      "metadata": {
        "id": "ATofMJRXhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "5ec04773-ec95-4015-9a1f-3758e862f775"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:11:43--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160 [text/plain]\n",
            "Saving to: â€˜hadoop-3.4.2.tar.gz.sha512â€™\n",
            "\n",
            "hadoop-3.4.2.tar.gz 100%[===================>]     160  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-04 20:11:43 (3.08 MB/s) - â€˜hadoop-3.4.2.tar.gz.sha512â€™ saved [160/160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare"
      ],
      "metadata": {
        "id": "eL8FxjalhFAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "A=$(sha512sum hadoop-3.4.2.tar.gz | cut - -d' ' -f1)\n",
        "B=$(cut hadoop-3.4.2.tar.gz.sha512 -d' ' -f4)\n",
        "printf \"%s\\n%s\\n\" $A $B\n",
        "[[ $A == $B ]] && echo \"True\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL302M1OhFMH",
        "outputId": "5bacf744-5c47-4bb4-f16a-0f80ce7e083b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "27b70357-8bc8-4705-e80e-743b2f5cf4a7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.2')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "e26f7bcd-6888-4f27-ba15-4ffa22c2a131"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHELL: /bin/bash\n",
            "COLAB_JUPYTER_TRANSPORT: ipc\n",
            "CGROUP_MEMORY_EVENTS: /sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "VM_GCE_METADATA_HOST: 169.254.169.253\n",
            "MODEL_PROXY_HOST: https://mp.kaggle.net\n",
            "HOSTNAME: 41b6d299bf0e\n",
            "LANGUAGE: en_US\n",
            "TBE_RUNTIME_ADDR: 172.28.0.1:8011\n",
            "GCE_METADATA_TIMEOUT: 3\n",
            "COLAB_JUPYTER_IP: 172.28.0.12\n",
            "COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL: http://172.28.0.1:8013/\n",
            "KMP_LISTEN_PORT: 6000\n",
            "TF_FORCE_GPU_ALLOW_GROWTH: true\n",
            "ENV: /root/.bashrc\n",
            "PWD: /\n",
            "TBE_EPHEM_CREDS_ADDR: 172.28.0.1:8009\n",
            "COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT: 30s\n",
            "TBE_CREDS_ADDR: 172.28.0.1:8008\n",
            "COLAB_JUPYTER_TOKEN: \n",
            "LAST_FORCED_REBUILD: 20250623\n",
            "TCLLIBPATH: /usr/share/tcltk/tcllib1.20\n",
            "COLAB_KERNEL_MANAGER_PROXY_HOST: 172.28.0.12\n",
            "UV_BUILD_CONSTRAINT: \n",
            "COLAB_WARMUP_DEFAULTS: 1\n",
            "HOME: /root\n",
            "LANG: en_US.UTF-8\n",
            "CLOUDSDK_CONFIG: /content/.config\n",
            "UV_SYSTEM_PYTHON: true\n",
            "COLAB_RELEASE_TAG: release-colab-external_20260202-060039_RC01\n",
            "KMP_TARGET_PORT: 9000\n",
            "KMP_EXTRA_ARGS: --logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-hxcg2qku3hvw --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true \n",
            "UV_INSTALL_DIR: /usr/local/bin\n",
            "COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS: /datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "COLAB_KERNEL_MANAGER_PROXY_PORT: 6000\n",
            "CLOUDSDK_PYTHON: python3\n",
            "NO_GCE_CHECK: False\n",
            "PYTHONPATH: /env/python\n",
            "SHLVL: 0\n",
            "COLAB_LANGUAGE_SERVER_PROXY: /usr/colab/bin/language_service\n",
            "UV_CONSTRAINT: \n",
            "PYTHONUTF8: 1\n",
            "COLAB_GPU: \n",
            "GCS_READ_CACHE_BLOCK_SIZE_MB: 16\n",
            "LC_ALL: en_US.UTF-8\n",
            "COLAB_FILE_HANDLER_ADDR: localhost:3453\n",
            "PATH: /content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "COLAB_IMAGE_TYPE: cpu\n",
            "COLAB_DEBUG_ADAPTER_MUX_PATH: /usr/local/bin/dap_multiplexer\n",
            "PYTHONWARNINGS: ignore:::pip._internal.cli.base_command\n",
            "DEBIAN_FRONTEND: noninteractive\n",
            "COLAB_BACKEND_VERSION: next\n",
            "OLDPWD: /\n",
            "_PYVIZ_COMMS_INSTALLED: 1\n",
            "PYDEVD_USE_FRAME_EVAL: NO\n",
            "JPY_SESSION_NAME: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JPY_PARENT_PID: 87\n",
            "TERM: xterm-color\n",
            "CLICOLOR: 1\n",
            "PAGER: cat\n",
            "GIT_PAGER: cat\n",
            "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
            "ENABLE_DIRECTORYPREFETCHER: 1\n",
            "USE_AUTH_EPHEM: 1\n",
            "COLAB_NOTEBOOK_ID: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n",
            "HADOOP_HOME: /content/hadoop-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "58bf2a0e-d0fc-4bc7-cd06-57bf2ed9c244"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/core-site.xml > /dev/null\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/hdfs-site.xml > /dev/null\n"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat hadoop-3.4.2/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "8b792db6-12b5-4337-d1be-71e16507e0c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.2/sbin`).\n",
        "```\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -n hadoop-3.4.2/etc/hadoop/hadoop-env.sh hadoop-3.4.2/etc/hadoop/hadoop-env.sh.org\n",
        "cat <<ðŸ˜ƒ >hadoop-3.4.2/etc/hadoop/hadoop-env.sh\n",
        "export JAVA_HOME=\"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless access to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Install `openssh` and start server\n",
        "\n",
        "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
        "\n",
        "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install openssh-server\n",
        "# tee -a appends to the file using elevated privileges\n",
        "echo 'StrictHostKeyChecking no' | sudo tee -a /etc/ssh/ssh_config\n",
        "sudo /etc/init.d/ssh restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "d63351be-5e66-4aa6-d34f-b52e8534cab9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,009 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,696 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,597 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,608 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,891 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,388 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,297 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,677 kB]\n",
            "Fetched 36.9 MB in 3s (10.8 MB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  ncurses-term openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere ssh-askpass ufw\n",
            "The following NEW packages will be installed:\n",
            "  ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 4 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 751 kB of archives.\n",
            "After this operation, 6,050 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-sftp-server amd64 1:8.9p1-3ubuntu0.13 [38.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-server amd64 1:8.9p1-3ubuntu0.13 [435 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ncurses-term all 6.3-2ubuntu0.1 [267 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssh-import-id all 5.11-0ubuntu1 [10.1 kB]\n",
            "Fetched 751 kB in 1s (631 kB/s)\n",
            "Selecting previously unselected package openssh-sftp-server.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 117540 files and directories currently installed.)\r\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package openssh-server.\r\n",
            "Preparing to unpack .../openssh-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package ncurses-term.\r\n",
            "Preparing to unpack .../ncurses-term_6.3-2ubuntu0.1_all.deb ...\r\n",
            "Unpacking ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package ssh-import-id.\r\n",
            "Preparing to unpack .../ssh-import-id_5.11-0ubuntu1_all.deb ...\r\n",
            "Unpacking ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Setting up openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "debconf: unable to initialize frontend: Dialog\r\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\r\n",
            "debconf: falling back to frontend: Readline\r\n",
            "\r\n",
            "Creating config file /etc/ssh/sshd_config with new version\r\n",
            "Creating SSH2 RSA key; this may take some time ...\r\n",
            "3072 SHA256:RwsERXLWY6nL+ifkOYOAwyd5jofuwiO/hCv39d5FsVs root@41b6d299bf0e (RSA)\r\n",
            "Creating SSH2 ECDSA key; this may take some time ...\r\n",
            "256 SHA256:GbHqV9tCtZ6lBr7RYTLwRfFTyV+WstwBk6sF0LvAcAY root@41b6d299bf0e (ECDSA)\r\n",
            "Creating SSH2 ED25519 key; this may take some time ...\r\n",
            "256 SHA256:KfSYVCrEHsoKhnuCObnUdtI5Xx0/zwHRg1vkI83UoKQ root@41b6d299bf0e (ED25519)\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Created symlink /etc/systemd/system/sshd.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Setting up ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            "StrictHostKeyChecking no\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate key\n",
        "Generate an SSH key that does not require a password.\n",
        "\n",
        "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
        "\n",
        "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm $HOME/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "dd6c0a56-96e0-40ff-94a9-36f9e9eb5a79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:CseEnAypN7+V07VTF5t5vAPzttkiXkWLmnLU4tL46DE root@41b6d299bf0e\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|  ..             |\n",
            "|  .+ o        .  |\n",
            "| .  = .        *.|\n",
            "|. o  o    . oo*.+|\n",
            "| . o. ooS. = ++oo|\n",
            "|    .o+.. B +  * |\n",
            "|     o.. E B  o =|\n",
            "|    .     O ...o.|\n",
            "|        .o o.. . |\n",
            "+----[SHA256]-----+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n",
            "Created directory '/root/.ssh'.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi ðŸ‘‹\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "83845195-c627-4ec4-8922-9c0e8810677f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            "hi ðŸ‘‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo env JAVA_HOME=$JAVA_HOME $HADOOP_HOME/bin/hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "f76b24f2-2513-46fc-e6de-45fdd75a2401"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /content/hadoop-3.4.2/logs does not exist. Creating.\n",
            "2026-02-04 20:12:04,996 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 41b6d299bf0e/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -nonInteractive]\n",
            "STARTUP_MSG:   version = 3.4.2\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.4.2/etc/hadoop:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-shaded-guava-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-auth-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-util-ajax-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/bcprov-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-sctp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-annotations-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/zookeeper-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/nimbus-jose-jwt-9.37.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-udt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-logging-1.3.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jersey-json-1.22.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-memcache-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-compress-1.26.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-configuration2-2.10.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/dnsjava-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-mqtt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-xml-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-xml-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-cli-1.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-webapp-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-collections4-4.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-proxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-haproxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-io-2.16.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/avro-1.11.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-redis-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-smtp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-rxtx-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-io-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-stomp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-all-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-http-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-security-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-http-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-buffer-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-lang3-3.17.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-socks-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/zookeeper-jute-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-handler-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/jetty-util-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/netty-codec-http2-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-nfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-kms-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-common-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-registry-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/common/hadoop-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-auth-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-riscv64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-annotations-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/zookeeper-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.37.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-udt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-logging-1.3.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jersey-json-1.22.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-compress-1.26.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-configuration2-2.10.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/dnsjava-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.118.Final-osx-x86_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-xml-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-xml-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-cli-1.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-webapp-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-collections4-4.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-io-2.16.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/avro-1.11.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-redis-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.118.Final-linux-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-io-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-dns-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-all-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-http-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-security-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-http-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-buffer-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.118.Final-osx-aarch_64.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-lang3-3.17.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-socks-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/zookeeper-jute-3.8.4.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_25-1.4.0.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-handler-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/jetty-util-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/netty-codec-http2-4.1.118.Final.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-client-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/hdfs/hadoop-hdfs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.2-tests.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn:/content/hadoop-3.4.2/share/hadoop/yarn/lib/guice-4.2.3.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-server-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/asm-commons-9.7.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-servlet-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jline-3.9.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-client-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jaxb-runtime-2.3.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/cache-api-1.1.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/ehcache-3.8.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/asm-tree-9.7.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/stax-ex-1.8.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-annotations-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/FastInfoset-1.2.15.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/istack-commons-runtime-3.0.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/txw2-2.3.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax.activation-api-1.2.0.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/codemodel-2.6.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/bcutil-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-common-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-client-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/websocket-api-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-jndi-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/bcpkix-jdk18on-1.78.1.jar:/content/hadoop-3.4.2/share/hadoop/yarn/lib/jetty-plus-9.4.57.v20241219.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-api-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-services-api-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-common-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-services-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-client-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-router-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.2.jar:/content/hadoop-3.4.2/share/hadoop/yarn/hadoop-yarn-registry-3.4.2.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c; compiled by 'ahmarsu' on 2025-08-20T10:30Z\n",
            "STARTUP_MSG:   java = 17.0.17\n",
            "************************************************************/\n",
            "2026-02-04 20:12:05,057 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2026-02-04 20:12:05,275 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\n",
            "2026-02-04 20:12:06,334 INFO namenode.NameNode: Formatting using clusterid: CID-ef1a8815-85b0-4c8e-b8c8-585db608a7c6\n",
            "2026-02-04 20:12:06,407 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2026-02-04 20:12:06,465 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2026-02-04 20:12:06,468 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2026-02-04 20:12:06,468 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2026-02-04 20:12:06,519 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2026-02-04 20:12:06,519 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2026-02-04 20:12:06,519 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2026-02-04 20:12:06,519 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2026-02-04 20:12:06,519 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2026-02-04 20:12:06,595 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2026-02-04 20:12:06,770 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2026-02-04 20:12:06,784 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2026-02-04 20:12:06,784 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2026-02-04 20:12:06,788 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2026-02-04 20:12:06,789 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 04 20:12:06\n",
            "2026-02-04 20:12:06,791 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2026-02-04 20:12:06,791 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:12:06,793 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2026-02-04 20:12:06,793 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2026-02-04 20:12:06,822 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2026-02-04 20:12:06,822 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2026-02-04 20:12:06,830 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\n",
            "2026-02-04 20:12:06,830 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2026-02-04 20:12:06,830 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2026-02-04 20:12:06,830 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2026-02-04 20:12:06,831 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2026-02-04 20:12:06,877 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2026-02-04 20:12:06,877 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:12:06,877 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:12:06,877 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2026-02-04 20:12:06,894 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2026-02-04 20:12:06,894 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:12:06,895 INFO util.GSet: 1.0% max memory 3.2 GB = 32.4 MB\n",
            "2026-02-04 20:12:06,895 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2026-02-04 20:12:06,909 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2026-02-04 20:12:06,909 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2026-02-04 20:12:06,910 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2026-02-04 20:12:06,910 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2026-02-04 20:12:06,917 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\n",
            "2026-02-04 20:12:06,917 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\n",
            "2026-02-04 20:12:06,919 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2026-02-04 20:12:06,925 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2026-02-04 20:12:06,925 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:12:06,926 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2026-02-04 20:12:06,926 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2026-02-04 20:12:06,940 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2026-02-04 20:12:06,941 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2026-02-04 20:12:06,941 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2026-02-04 20:12:06,947 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2026-02-04 20:12:06,947 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2026-02-04 20:12:06,949 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2026-02-04 20:12:06,949 INFO util.GSet: VM type       = 64-bit\n",
            "2026-02-04 20:12:06,950 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 996.6 KB\n",
            "2026-02-04 20:12:06,950 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2026-02-04 20:12:06,984 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1930225675-172.28.0.12-1770235926975\n",
            "2026-02-04 20:12:07,037 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2026-02-04 20:12:07,088 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2026-02-04 20:12:07,219 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2026-02-04 20:12:07,243 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2026-02-04 20:12:07,248 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2026-02-04 20:12:07,289 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2026-02-04 20:12:07,290 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2026-02-04 20:12:07,295 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2026-02-04 20:12:07,296 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 41b6d299bf0e/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "6c80a765-4787-4926-eac0-ada8525bcecd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [41b6d299bf0e]\n",
            "41b6d299bf0e: Warning: Permanently added '41b6d299bf0e' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Check if HDFS is in safe mode\n",
        "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
        "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
        "  hdfs dfsadmin -safemode leave\n",
        "else\n",
        "  echo \"Namenode is not in safe mode.\"\n",
        "fi"
      ],
      "metadata": {
        "id": "pOHsiWv9or7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6c3d6f-0a63-4d3c-8a15-54e3ef55d732"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namenode is not in safe mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple HDFS commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home\n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# if sampls_data does not exist, create it\n",
        "mkdir -p sample_data\n",
        "touch sample_data/mnist_test.csv\n",
        "\n",
        "# Check if the file is empty and fill it if needed\n",
        "if [ ! -s sample_data/mnist_test.csv ]; then\n",
        "  echo -e \"0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\" > sample_data/mnist_test.csv\n",
        "fi\n",
        "\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "bea243ad-ddfc-46c0-c312-2242f604a781"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2026-02-04 20:12 my_dir/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple MapReduce jobs\n",
        "\n",
        "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
        "\n",
        "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest MapReduce job\n",
        "\n",
        "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job"
      ],
      "metadata": {
        "id": "o6ICcKO2jcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output_simplest \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61288f16-e6f5-4a92-af3d-facf3448092b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_simplest': No such file or directory\n",
            "namenode is running as process 2333.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "2026-02-04 20:12:53,822 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:12:54,406 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:12:54,428 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:12:54,837 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1630782929_0001\n",
            "2026-02-04 20:12:54,839 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:12:55,122 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:12:55,122 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:12:55,127 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:12:55,127 INFO mapreduce.Job: Running job: job_local1630782929_0001\n",
            "2026-02-04 20:12:55,143 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:12:55,144 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:12:55,246 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:12:55,250 INFO mapred.LocalJobRunner: Starting task: attempt_local1630782929_0001_m_000000_0\n",
            "2026-02-04 20:12:55,303 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:12:55,303 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:12:55,351 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:12:55,361 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+18289443\n",
            "2026-02-04 20:12:55,402 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 20:12:55,463 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 20:12:55,463 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 20:12:55,463 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 20:12:55,463 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 20:12:55,463 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 20:12:55,485 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 20:12:55,488 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-04 20:12:55,500 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:12:55,504 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:12:55,504 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:12:55,505 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:12:55,506 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:12:55,506 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:12:55,508 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:12:55,509 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:12:55,509 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:12:55,510 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:12:55,510 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:12:55,513 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:12:55,734 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:55,736 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:55,750 INFO streaming.PipeMapRed: Records R/W=73/1\n",
            "2026-02-04 20:12:55,751 INFO streaming.PipeMapRed: R/W/S=100/2/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:55,848 INFO streaming.PipeMapRed: R/W/S=1000/893/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,145 INFO mapreduce.Job: Job job_local1630782929_0001 running in uber mode : false\n",
            "2026-02-04 20:12:56,146 INFO streaming.PipeMapRed: R/W/S=10000/9962/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,146 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2026-02-04 20:12:56,147 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:12:56,149 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:12:56,157 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:12:56,158 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 20:12:56,158 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 20:12:56,158 INFO mapred.MapTask: bufstart = 0; bufend = 18319443; bufvoid = 104857600\n",
            "2026-02-04 20:12:56,158 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
            "2026-02-04 20:12:56,319 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 20:12:56,342 INFO mapred.Task: Task:attempt_local1630782929_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:12:56,348 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
            "2026-02-04 20:12:56,349 INFO mapred.Task: Task 'attempt_local1630782929_0001_m_000000_0' done.\n",
            "2026-02-04 20:12:56,357 INFO mapred.Task: Final Counters for attempt_local1630782929_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141963\n",
            "\t\tFILE: Number of bytes written=19221037\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=37\n",
            "\t\tTotal committed heap usage (bytes)=228589568\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "2026-02-04 20:12:56,357 INFO mapred.LocalJobRunner: Finishing task: attempt_local1630782929_0001_m_000000_0\n",
            "2026-02-04 20:12:56,358 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:12:56,361 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 20:12:56,361 INFO mapred.LocalJobRunner: Starting task: attempt_local1630782929_0001_r_000000_0\n",
            "2026-02-04 20:12:56,370 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:12:56,370 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:12:56,371 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:12:56,374 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3009dded\n",
            "2026-02-04 20:12:56,376 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:12:56,397 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 20:12:56,402 INFO reduce.EventFetcher: attempt_local1630782929_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 20:12:56,471 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1630782929_0001_m_000000_0 decomp: 18359445 len: 18359449 to MEMORY\n",
            "2026-02-04 20:12:56,503 INFO reduce.InMemoryMapOutput: Read 18359445 bytes from map-output for attempt_local1630782929_0001_m_000000_0\n",
            "2026-02-04 20:12:56,505 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18359445, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18359445\n",
            "2026-02-04 20:12:56,507 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 20:12:56,510 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:12:56,510 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 20:12:56,518 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:12:56,518 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 20:12:56,577 INFO reduce.MergeManagerImpl: Merged 1 segments, 18359445 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 20:12:56,578 INFO reduce.MergeManagerImpl: Merging 1 files, 18359449 bytes from disk\n",
            "2026-02-04 20:12:56,579 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 20:12:56,579 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:12:56,580 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 20:12:56,581 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:12:56,583 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2026-02-04 20:12:56,589 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2026-02-04 20:12:56,595 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2026-02-04 20:12:56,685 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,686 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,691 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,727 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,984 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:12:56,987 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
            "2026-02-04 20:12:56,987 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:12:56,987 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:12:57,073 INFO mapred.Task: Task:attempt_local1630782929_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:12:57,077 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:12:57,077 INFO mapred.Task: Task attempt_local1630782929_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 20:12:57,103 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1630782929_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n",
            "2026-02-04 20:12:57,104 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
            "2026-02-04 20:12:57,104 INFO mapred.Task: Task 'attempt_local1630782929_0001_r_000000_0' done.\n",
            "2026-02-04 20:12:57,105 INFO mapred.Task: Final Counters for attempt_local1630782929_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36860893\n",
            "\t\tFILE: Number of bytes written=37580486\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=10000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=6\n",
            "\t\tTotal committed heap usage (bytes)=304087040\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 20:12:57,105 INFO mapred.LocalJobRunner: Finishing task: attempt_local1630782929_0001_r_000000_0\n",
            "2026-02-04 20:12:57,105 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 20:12:57,151 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 20:12:57,152 INFO mapreduce.Job: Job job_local1630782929_0001 completed successfully\n",
            "2026-02-04 20:12:57,173 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=37002856\n",
            "\t\tFILE: Number of bytes written=56801523\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=36578886\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=20000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=43\n",
            "\t\tTotal committed heap usage (bytes)=532676608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 20:12:57,174 INFO streaming.StreamJob: Output directory: output_simplest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_simplest/part-00000"
      ],
      "metadata": {
        "id": "rB7FXYTbwNzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18ffb4f-27df-4723-a71e-d2ae0d85553d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000   10000 18299443\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "id": "yJIm4SPZFPxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f85daa-20a0-4116-9c54-02f761d5a95f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-04 20:13:00--  https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 216485 (211K) [text/plain]\n",
            "Saving to: â€˜Linux_2k.logâ€™\n",
            "\n",
            "Linux_2k.log        100%[===================>] 211.41K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2026-02-04 20:13:00 (7.63 MB/s) - â€˜Linux_2k.logâ€™ saved [216485/216485]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input || true\n",
        "hdfs dfs -put Linux_2k.log input/ || true"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "id": "4-rraIUdfdj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ce8698-f838-4a21-c5fa-4616e916dd8c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "id": "9qf1dFqIKgoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99fc6f16-be44-4233-f386-cbd73f8a6bf0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py"
      ],
      "metadata": {
        "id": "TJ0kDRsigCC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f413346-9183-4298-f3ce-73a3c630ce3d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\n",
            "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\n",
            "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_filter\n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output_filter \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE\n"
      ],
      "metadata": {
        "id": "G7SEzMC2OqWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0181a698-e93a-4e5f-fb5a-894b078594e9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [mapper.py] [] /tmp/streamjob5065757730699339553.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_filter': No such file or directory\n",
            "2026-02-04 20:13:11,967 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 20:13:13,875 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:13:14,397 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:13:14,426 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:13:14,755 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local914688199_0001\n",
            "2026-02-04 20:13:14,757 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:13:15,116 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local914688199_0001_e9072d45-012d-4863-8654-770499362b93/mapper.py\n",
            "2026-02-04 20:13:15,296 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:13:15,297 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:13:15,299 INFO mapreduce.Job: Running job: job_local914688199_0001\n",
            "2026-02-04 20:13:15,302 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:13:15,314 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:13:15,314 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:13:15,391 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:13:15,396 INFO mapred.LocalJobRunner: Starting task: attempt_local914688199_0001_m_000000_0\n",
            "2026-02-04 20:13:15,439 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:13:15,439 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:13:15,463 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:13:15,480 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 20:13:15,511 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-04 20:13:15,612 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2026-02-04 20:13:15,621 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:13:15,623 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:13:15,623 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:13:15,624 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:13:15,626 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:13:15,627 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:13:15,629 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:13:15,630 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:13:15,630 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:13:15,631 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:13:15,631 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:13:15,632 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:13:15,808 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:15,808 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:15,810 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:15,832 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:15,836 INFO streaming.PipeMapRed: Records R/W=1369/1\n",
            "2026-02-04 20:13:15,870 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:13:15,874 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:13:15,879 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:13:15,970 INFO mapred.Task: Task:attempt_local914688199_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:13:15,975 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:13:15,976 INFO mapred.Task: Task attempt_local914688199_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-04 20:13:16,009 INFO output.FileOutputCommitter: Saved output of task 'attempt_local914688199_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n",
            "2026-02-04 20:13:16,010 INFO mapred.LocalJobRunner: Records R/W=1369/1\n",
            "2026-02-04 20:13:16,010 INFO mapred.Task: Task 'attempt_local914688199_0001_m_000000_0' done.\n",
            "2026-02-04 20:13:16,018 INFO mapred.Task: Final Counters for attempt_local914688199_0001_m_000000_0: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=717076\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 20:13:16,018 INFO mapred.LocalJobRunner: Finishing task: attempt_local914688199_0001_m_000000_0\n",
            "2026-02-04 20:13:16,019 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:13:16,311 INFO mapreduce.Job: Job job_local914688199_0001 running in uber mode : false\n",
            "2026-02-04 20:13:16,312 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 20:13:16,315 INFO mapreduce.Job: Job job_local914688199_0001 completed successfully\n",
            "2026-02-04 20:13:16,325 INFO mapreduce.Job: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=717076\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 20:13:16,325 INFO streaming.StreamJob: Output directory: output_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output_filter"
      ],
      "metadata": {
        "id": "RhLA5HZEhfmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19d5007-6783-4151-bdaf-640e91170862"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 20:13 output_filter/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      85436 2026-02-04 20:13 output_filter/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_filter/part-00000 |head"
      ],
      "metadata": {
        "id": "Ffi4RvXnPH14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88004957-4680-4732-90a7-7c9ae5c6323f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "id": "fMKEqUF1T-v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b132ab-6e84-4f44-cf4c-eb60e4482e0f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper"
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "k-R7VNoTVRjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f1ee1a2-f55a-4d41-9171-4f40d50df85e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:logrotate:\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "chmod +x myAggregatorForKeyCount.py\n",
        "\n",
        "hdfs dfs -rm -r output_aggregate\n",
        "\n",
        "mapred streaming \\\n",
        "  -input input \\\n",
        "  -output output_aggregate \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate \\\n",
        "  -file myAggregatorForKeyCount.py\n"
      ],
      "metadata": {
        "id": "XwxHJ7yyVR34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138422d6-ca47-4d4e-cbe1-10d1586bf049"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [myAggregatorForKeyCount.py] [] /tmp/streamjob12033609516928390146.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `output_aggregate': No such file or directory\n",
            "2026-02-04 20:13:27,833 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 20:13:29,659 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:13:30,169 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 20:13:30,203 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 20:13:30,551 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local109692003_0001\n",
            "2026-02-04 20:13:30,553 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 20:13:30,926 INFO mapred.LocalDistributedCacheManager: Localized file:/content/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local109692003_0001_f7fe5b40-8524-4038-8c65-0cf98e48cbc0/myAggregatorForKeyCount.py\n",
            "2026-02-04 20:13:31,092 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 20:13:31,092 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 20:13:31,094 INFO mapreduce.Job: Running job: job_local109692003_0001\n",
            "2026-02-04 20:13:31,095 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 20:13:31,105 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:13:31,106 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:13:31,167 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 20:13:31,170 INFO mapred.LocalJobRunner: Starting task: attempt_local109692003_0001_m_000000_0\n",
            "2026-02-04 20:13:31,202 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:13:31,202 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:13:31,232 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:13:31,240 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 20:13:31,269 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 20:13:31,355 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 20:13:31,355 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 20:13:31,355 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 20:13:31,355 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 20:13:31,355 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 20:13:31,361 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 20:13:31,373 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./myAggregatorForKeyCount.py]\n",
            "2026-02-04 20:13:31,381 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 20:13:31,383 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 20:13:31,383 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 20:13:31,383 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 20:13:31,384 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 20:13:31,384 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 20:13:31,386 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 20:13:31,386 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 20:13:31,386 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 20:13:31,387 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 20:13:31,387 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 20:13:31,391 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 20:13:31,585 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:31,586 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:31,589 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:31,610 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 20:13:31,619 INFO streaming.PipeMapRed: Records R/W=1201/1\n",
            "2026-02-04 20:13:31,663 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 20:13:31,670 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 20:13:31,676 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 20:13:31,678 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 20:13:31,678 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 20:13:31,678 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n",
            "2026-02-04 20:13:31,678 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n",
            "2026-02-04 20:13:31,749 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 20:13:31,775 INFO mapred.Task: Task:attempt_local109692003_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:13:31,784 INFO mapred.LocalJobRunner: Records R/W=1201/1\n",
            "2026-02-04 20:13:31,784 INFO mapred.Task: Task 'attempt_local109692003_0001_m_000000_0' done.\n",
            "2026-02-04 20:13:31,793 INFO mapred.Task: Final Counters for attempt_local109692003_0001_m_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058\n",
            "\t\tFILE: Number of bytes written=718520\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=6\n",
            "\t\tTotal committed heap usage (bytes)=195035136\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "2026-02-04 20:13:31,794 INFO mapred.LocalJobRunner: Finishing task: attempt_local109692003_0001_m_000000_0\n",
            "2026-02-04 20:13:31,794 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 20:13:31,799 INFO mapred.LocalJobRunner: Starting task: attempt_local109692003_0001_r_000000_0\n",
            "2026-02-04 20:13:31,799 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 20:13:31,808 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 20:13:31,808 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 20:13:31,808 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 20:13:31,811 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3c58307d\n",
            "2026-02-04 20:13:31,813 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 20:13:31,833 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 20:13:31,840 INFO reduce.EventFetcher: attempt_local109692003_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 20:13:31,882 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local109692003_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n",
            "2026-02-04 20:13:31,887 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local109692003_0001_m_000000_0\n",
            "2026-02-04 20:13:31,890 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n",
            "2026-02-04 20:13:31,891 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 20:13:31,892 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:13:31,892 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 20:13:31,900 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:13:31,900 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 20:13:31,903 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 20:13:31,904 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n",
            "2026-02-04 20:13:31,904 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 20:13:31,904 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 20:13:31,906 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 20:13:31,906 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:13:32,104 INFO mapreduce.Job: Job job_local109692003_0001 running in uber mode : false\n",
            "2026-02-04 20:13:32,106 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 20:13:32,444 INFO mapred.Task: Task:attempt_local109692003_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 20:13:32,448 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 20:13:32,449 INFO mapred.Task: Task attempt_local109692003_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 20:13:32,474 INFO output.FileOutputCommitter: Saved output of task 'attempt_local109692003_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n",
            "2026-02-04 20:13:32,475 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-04 20:13:32,475 INFO mapred.Task: Task 'attempt_local109692003_0001_r_000000_0' done.\n",
            "2026-02-04 20:13:32,476 INFO mapred.Task: Final Counters for attempt_local109692003_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2654\n",
            "\t\tFILE: Number of bytes written=719302\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=195035136\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 20:13:32,476 INFO mapred.LocalJobRunner: Finishing task: attempt_local109692003_0001_r_000000_0\n",
            "2026-02-04 20:13:32,477 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 20:13:33,108 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 20:13:33,109 INFO mapreduce.Job: Job job_local109692003_0001 completed successfully\n",
            "2026-02-04 20:13:33,118 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3712\n",
            "\t\tFILE: Number of bytes written=1437822\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=432970\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=60\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=6\n",
            "\t\tTotal committed heap usage (bytes)=390070272\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 20:13:33,118 INFO streaming.StreamJob: Output directory: output_aggregate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output_aggregate\n",
        "hdfs dfs -cat output_aggregate/part-00000"
      ],
      "metadata": {
        "id": "ET3KCfX1UC2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1d5542-5e1a-402f-b5b8-d4290ccf9b94"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 20:13 output_aggregate/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        326 2026-02-04 20:13 output_aggregate/part-00000\n",
            "--\t1\n",
            "bluetooth:\t2\n",
            "cups:\t12\n",
            "ftpd\t916\n",
            "gdm(pam_unix)\t2\n",
            "gdm-binary\t1\n",
            "gpm\t2\n",
            "hcid\t1\n",
            "irqbalance:\t1\n",
            "kernel:\t76\n",
            "klogind\t46\n",
            "login(pam_unix)\t2\n",
            "logrotate:\t43\n",
            "named\t16\n",
            "network:\t2\n",
            "nfslock:\t1\n",
            "portmap:\t1\n",
            "random:\t1\n",
            "rc:\t1\n",
            "rpc.statd\t1\n",
            "rpcidmapd:\t1\n",
            "sdpd\t1\n",
            "snmpd\t1\n",
            "sshd(pam_unix)\t677\n",
            "su(pam_unix)\t172\n",
            "sysctl:\t1\n",
            "syslog:\t2\n",
            "syslogd\t7\n",
            "udev\t8\n",
            "xinetd\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
        "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
        "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
      ],
      "metadata": {
        "id": "Y8IYl4hAZhZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a75dab-2468-44cd-f5ed-31a60d447b4f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftpd                 916\n",
            "sshd(pam_unix)       677\n",
            "su(pam_unix)         172\n",
            "kernel:              76\n",
            "klogind              46\n",
            "logrotate:           43\n",
            "named                16\n",
            "cups:                12\n",
            "udev                 8\n",
            "syslogd              7\n",
            "bluetooth:           2\n",
            "gdm(pam_unix)        2\n",
            "gpm                  2\n",
            "login(pam_unix)      2\n",
            "network:             2\n",
            "syslog:              2\n",
            "xinetd               2\n",
            "--                   1\n",
            "gdm-binary           1\n",
            "hcid                 1\n",
            "irqbalance:          1\n",
            "nfslock:             1\n",
            "portmap:             1\n",
            "random:              1\n",
            "rc:                  1\n",
            "rpcidmapd:           1\n",
            "rpc.statd            1\n",
            "sdpd                 1\n",
            "snmpd                1\n",
            "sysctl:              1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.4.2/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "id": "IoIYG5NlsIMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be02bd71-d572-43bf-95a0-8493e15ccb57"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [41b6d299bf0e]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop the `sshd` daemon"
      ],
      "metadata": {
        "id": "RGj96_e2ccZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/init.d/ssh stop"
      ],
      "metadata": {
        "id": "FUvKMpy6chQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d7a9a5-f948-4607-a24b-c9b57a82161e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding remarks\n",
        "\n",
        "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
        "\n",
        "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
        "\n",
        "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
        "\n",
        "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
        "\n",
        "If you found this notebook helpful, consider exploring:\n",
        " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
        " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
        " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5N7tb0HSbZB"
      }
    }
  ]
}