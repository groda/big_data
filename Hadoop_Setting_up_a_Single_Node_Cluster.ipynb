{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qFfOrktMPq8M",
        "IF6-Z5RotAcO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/Hadoop_Setting_up_a_Single_Node_Cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
        "\n",
        "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
        "\n",
        "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
        "\n",
        "Finally, we'll proceed to shut down the cluster.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**\n",
        "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
        "\n",
        " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
        "\n",
        " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
        "\n",
        "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
        "\n",
        " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
        "\n",
        " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
        "\n",
        " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
        "\n",
        " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
        "\n",
        "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
        "\n",
        "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
        "\n",
        "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
        "\n",
        "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
        "\n",
        "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
        "\n",
        "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
        "\n",
        "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
        "\n",
        "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
        "\n",
        "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
        "\n",
        "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
        "\n",
        "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
        "\n",
        "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
        "\n",
        "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "hGm3LhVEWXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the available Java version\n",
        " Apache Hadoop 3.3 and upper supports Java 8 and Java 11 (runtime only). See: https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "2bf18811-0898-4d12-ac7c-20558aec8437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.27\" 2025-04-15\n",
            "OpenJDK Runtime Environment (build 11.0.27+6-post-Ubuntu-0ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 8 || $JAVA_MAJOR_VERSION -eq 11 ]]\n",
        " then\n",
        " echo \"Java version is one of 8, 11 âœ“\"\n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "05c1947a-8370-4813-89b7-76cb6984d4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 8, 11 âœ“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the variable for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "pWROofISgKKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "4efba03e-3fc7-4214-c37b-dd238a840f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract JAVA_HOME from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "40430612-b7e6-4c82-99ca-3a348ba9d99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download core Hadoop\n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
        "\n",
        "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LqS5Rkgyli",
        "outputId": "6ab71854-2018-4711-fda9-2e268cf6024c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 12:23:17--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 965537117 (921M) [application/x-gzip]\n",
            "Saving to: â€˜hadoop-3.4.0.tar.gzâ€™\n",
            "\n",
            "hadoop-3.4.0.tar.gz 100%[===================>] 920.81M  73.9MB/s    in 13s     \n",
            "\n",
            "2025-06-24 12:23:45 (72.1 MB/s) - â€˜hadoop-3.4.0.tar.gzâ€™ saved [965537117/965537117]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncompress archive"
      ],
      "metadata": {
        "id": "um2CARkgg22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -d \"hadoop-3.4.0\" ]; then\n",
        "  tar xzf hadoop-3.4.0.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "id": "C17WYI0mQRE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file\n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download sha512 file"
      ],
      "metadata": {
        "id": "ATofMJRXhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "7d7e0b60-0ed0-4239-c91b-a2b6daa25315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-24 12:24:08--  https://dlcdn.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz.sha512\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160 [text/plain]\n",
            "Saving to: â€˜hadoop-3.4.0.tar.gz.sha512â€™\n",
            "\n",
            "hadoop-3.4.0.tar.gz 100%[===================>]     160  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-24 12:24:08 (6.50 MB/s) - â€˜hadoop-3.4.0.tar.gz.sha512â€™ saved [160/160]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare"
      ],
      "metadata": {
        "id": "eL8FxjalhFAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "A=$(sha512sum hadoop-3.4.0.tar.gz | cut - -d' ' -f1)\n",
        "B=$(cut hadoop-3.4.0.tar.gz.sha512 -d' ' -f4)\n",
        "printf \"%s\\n%s\\n\" $A $B\n",
        "[[ $A == $B ]] && echo \"True\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL302M1OhFMH",
        "outputId": "a070f00d-3860-4a51-e33c-a86c20688ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n",
            "6f653c0109f97430047bd3677c50da7c8a2809d153b231794cf980b3208a6b4beff8ff1a03a01094299d459a3a37a3fe16731629987165d71f328657dbf2f24c\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "9403498c-8e5e-4401-df95-44de81e11c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.0')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "#os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "729bbd33-1098-431e-f31b-a0dd06b35b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'COLAB_JUPYTER_TRANSPORT': 'ipc', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'CGROUP_MEMORY_EVENTS': '/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.253', 'MODEL_PROXY_HOST': 'https://mp.kaggle.net', 'HOSTNAME': '553c798b7f43', 'LANGUAGE': 'en_US', 'TBE_RUNTIME_ADDR': '172.28.0.1:8011', 'COLAB_TPU_1VM': '', 'GCE_METADATA_TIMEOUT': '3', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'COLAB_JUPYTER_IP': '172.28.0.12', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL': 'http://172.28.0.1:8013/', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'ENV': '/root/.bashrc', 'PWD': '/', 'TBE_EPHEM_CREDS_ADDR': '172.28.0.1:8009', 'COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT': '30s', 'TBE_CREDS_ADDR': '172.28.0.1:8008', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'COLAB_JUPYTER_TOKEN': '', 'LAST_FORCED_REBUILD': '20250509', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'COLAB_KERNEL_MANAGER_PROXY_HOST': '172.28.0.12', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250620-060048_RC00', 'KMP_TARGET_PORT': '9000', 'KMP_EXTRA_ARGS': '--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-1ngvb7o5d7f8n --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true ', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS': '/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'NO_GCE_CHECK': 'False', 'PYTHONPATH': '/env/python', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NVARCH': 'x86_64', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'COLAB_GPU': '', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GCS_READ_CACHE_BLOCK_SIZE_MB': '16', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'PATH': '/content/hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'PYTHONWARNINGS': 'ignore:::pip._internal.cli.base_command', 'DEBIAN_FRONTEND': 'noninteractive', 'COLAB_BACKEND_VERSION': 'next', 'OLDPWD': '/', '_PYVIZ_COMMS_INSTALLED': '1', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_PARENT_PID': '91', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'ENABLE_DIRECTORYPREFETCHER': '1', 'USE_AUTH_EPHEM': '1', 'COLAB_NOTEBOOK_ID': '177aNXDHZHU7pYxKhQtMYrF6QKH2KT3cG', 'HADOOP_HOME': '/content/hadoop-3.4.0'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "44c3158a-e8e8-4283-cce3-466f9a31a79b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.4.0/etc/hadoop/core-site.xml\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" >hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat hadoop-3.4.0/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "a69ca821-a48b-4979-e663-3bdad46c3219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.0/sbin`).\n",
        "```\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -n hadoop-3.4.0/etc/hadoop/hadoop-env.sh hadoop-3.4.0/etc/hadoop/hadoop-env.sh.org\n",
        "cat <<ðŸ˜ƒ >hadoop-3.4.0/etc/hadoop/hadoop-env.sh\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless access to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Install `openssh` and start server\n",
        "\n",
        "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
        "\n",
        "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install openssh-server\n",
        "echo 'StrictHostKeyChecking no' | sudo tee -a /etc/ssh/ssh_config",
        "sudo systemctl restart ssh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "06b37eb9-a0ec-49ab-b1f0-22b3688cf080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,994 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,746 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,305 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,798 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,045 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.2 kB]\n",
            "Fetched 23.1 MB in 3s (8,159 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  ncurses-term openssh-sftp-server ssh-import-id\n",
            "Suggested packages:\n",
            "  molly-guard monkeysphere ssh-askpass ufw\n",
            "The following NEW packages will be installed:\n",
            "  ncurses-term openssh-server openssh-sftp-server ssh-import-id\n",
            "0 upgraded, 4 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 751 kB of archives.\n",
            "After this operation, 6,050 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-sftp-server amd64 1:8.9p1-3ubuntu0.13 [38.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openssh-server amd64 1:8.9p1-3ubuntu0.13 [435 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ncurses-term all 6.3-2ubuntu0.1 [267 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssh-import-id all 5.11-0ubuntu1 [10.1 kB]\n",
            "Preconfiguring packages ...\n",
            "Fetched 751 kB in 0s (3,330 kB/s)\n",
            "Selecting previously unselected package openssh-sftp-server.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126319 files and directories currently installed.)\r\n",
            "Preparing to unpack .../openssh-sftp-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package openssh-server.\r\n",
            "Preparing to unpack .../openssh-server_1%3a8.9p1-3ubuntu0.13_amd64.deb ...\r\n",
            "Unpacking openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Selecting previously unselected package ncurses-term.\r\n",
            "Preparing to unpack .../ncurses-term_6.3-2ubuntu0.1_all.deb ...\r\n",
            "Unpacking ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Selecting previously unselected package ssh-import-id.\r\n",
            "Preparing to unpack .../ssh-import-id_5.11-0ubuntu1_all.deb ...\r\n",
            "Unpacking ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up openssh-sftp-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "Setting up openssh-server (1:8.9p1-3ubuntu0.13) ...\r\n",
            "\r\n",
            "Creating config file /etc/ssh/sshd_config with new version\r\n",
            "Creating SSH2 RSA key; this may take some time ...\r\n",
            "3072 SHA256:LN9iP+e+FqdiJ41AgrqFaYjstpCmF3nuE6c9FxcMwyo root@dca594300707 (RSA)\r\n",
            "Creating SSH2 ECDSA key; this may take some time ...\r\n",
            "256 SHA256:XxAhx6dxh1TzvngaGrfHMhR/klLo9Wsa/06cIHOySCo root@dca594300707 (ECDSA)\r\n",
            "Creating SSH2 ED25519 key; this may take some time ...\r\n",
            "256 SHA256:77yUhcxU2fs7apYejVTyG+5IR1owWZkVhxIJcX4foTg root@dca594300707 (ED25519)\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "Created symlink /etc/systemd/system/sshd.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service â†’ /lib/systemd/system/ssh.service.\r\n",
            "Setting up ssh-import-id (5.11-0ubuntu1) ...\r\n",
            "Setting up ncurses-term (6.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for man-db (2.10.2-1) ...\r\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate key\n",
        "Generate an SSH key that does not require a password.\n",
        "\n",
        "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
        "\n",
        "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm $HOME/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "475e9552-4f57-4108-f4a0-c632b52a58dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:gWl5kl0Wh2foCEcMIY7rxDoDgYXAVR9plOU1ZQQXQME root@dca594300707\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|+.o.o.+B=.*@B*.  |\n",
            "|oo o .oX=o+E*    |\n",
            "|o . . O+++ o     |\n",
            "| o . . o...      |\n",
            "|. +     S        |\n",
            "|.+               |\n",
            "|+ .              |\n",
            "| o               |\n",
            "|                 |\n",
            "+----[SHA256]-----+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: cannot remove '/root/.ssh/id_rsa': No such file or directory\n",
            "Created directory '/root/.ssh'.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi ðŸ‘‹\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "a6a130c9-1717-4b74-a93a-f945c7fdd911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            "hi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "4778340e-d7ea-472b-97ce-631e91e6e850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-18 18:17:05,472 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = dca594300707/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format, -nonInteractive]\n",
            "STARTUP_MSG:   version = 3.4.0\n",
            "STARTUP_MSG:   classpath = /content/hadoop-3.4.0/etc/hadoop:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-codec-1.15.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jsp-api-2.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-server-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/avro-1.9.2.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-3.8.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-client-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-compress-1.24.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jline-3.9.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/dnsjava-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-io-2.14.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-logging-1.2.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-cli-1.5.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/re2j-1.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-identity-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jettison-1.5.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/gson-2.9.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jersey-json-1.20.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-admin-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/token-provider-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-common-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jsch-0.1.55.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/guava-27.0-jre.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/failureaccess-1.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/common/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/common/hadoop-kms-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/hadoop-registry-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/common/hadoop-common-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/common/hadoop-nfs-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/avro-1.9.2.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jline-3.9.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-logging-1.2.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/gson-2.9.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn:/content/hadoop-3.4.0/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/commons-lang-2.6.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jna-5.2.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/asm-commons-9.6.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/codemodel-2.6.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/javax.inject-1.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/objenesis-2.6.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/asm-tree-9.6.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/guice-4.2.3.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/fst-2.50.jar:/content/hadoop-3.4.0/share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/content/hadoop-3.4.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar\n",
            "STARTUP_MSG:   build = git@github.com:apache/hadoop.git -r bd8b77f398f626bb7791783192ee7a5dfaeec760; compiled by 'root' on 2024-03-04T06:35Z\n",
            "STARTUP_MSG:   java = 11.0.27\n",
            "************************************************************/\n",
            "2025-06-18 18:17:05,572 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2025-06-18 18:17:05,753 INFO namenode.NameNode: createNameNode [-format, -nonInteractive]\n",
            "2025-06-18 18:17:06,544 INFO namenode.NameNode: Formatting using clusterid: CID-18dc422d-eb56-4cac-b7e0-10d390c37600\n",
            "2025-06-18 18:17:06,619 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2025-06-18 18:17:06,689 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2025-06-18 18:17:06,692 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2025-06-18 18:17:06,692 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2025-06-18 18:17:06,725 INFO namenode.FSNamesystem: fsOwner                = root (auth:SIMPLE)\n",
            "2025-06-18 18:17:06,725 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2025-06-18 18:17:06,725 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2025-06-18 18:17:06,725 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2025-06-18 18:17:06,726 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2025-06-18 18:17:06,810 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2025-06-18 18:17:07,032 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2025-06-18 18:17:07,066 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2025-06-18 18:17:07,066 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2025-06-18 18:17:07,072 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2025-06-18 18:17:07,073 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jun 18 18:17:07\n",
            "2025-06-18 18:17:07,076 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2025-06-18 18:17:07,076 INFO util.GSet: VM type       = 64-bit\n",
            "2025-06-18 18:17:07,079 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2025-06-18 18:17:07,079 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2025-06-18 18:17:07,135 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2025-06-18 18:17:07,136 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2025-06-18 18:17:07,145 INFO blockmanagement.BlockManagerSafeMode: Using 1000 as SafeModeMonitor Interval\n",
            "2025-06-18 18:17:07,145 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2025-06-18 18:17:07,146 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2025-06-18 18:17:07,146 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2025-06-18 18:17:07,147 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2025-06-18 18:17:07,192 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2025-06-18 18:17:07,192 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2025-06-18 18:17:07,192 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2025-06-18 18:17:07,192 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2025-06-18 18:17:07,210 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2025-06-18 18:17:07,210 INFO util.GSet: VM type       = 64-bit\n",
            "2025-06-18 18:17:07,210 INFO util.GSet: 1.0% max memory 3.2 GB = 32.5 MB\n",
            "2025-06-18 18:17:07,210 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2025-06-18 18:17:07,242 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2025-06-18 18:17:07,242 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2025-06-18 18:17:07,242 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2025-06-18 18:17:07,242 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2025-06-18 18:17:07,251 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotFSLimit: 65536, maxSnapshotLimit: 65536\n",
            "2025-06-18 18:17:07,254 INFO snapshot.SnapshotManager: dfs.namenode.snapshot.deletion.ordered = false\n",
            "2025-06-18 18:17:07,258 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2025-06-18 18:17:07,270 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2025-06-18 18:17:07,270 INFO util.GSet: VM type       = 64-bit\n",
            "2025-06-18 18:17:07,270 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2025-06-18 18:17:07,270 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2025-06-18 18:17:07,298 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2025-06-18 18:17:07,298 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2025-06-18 18:17:07,298 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2025-06-18 18:17:07,304 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2025-06-18 18:17:07,304 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2025-06-18 18:17:07,307 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2025-06-18 18:17:07,307 INFO util.GSet: VM type       = 64-bit\n",
            "2025-06-18 18:17:07,307 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 997.2 KB\n",
            "2025-06-18 18:17:07,307 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "Running in non-interactive mode, and data appears to exist in Storage Directory root= /tmp/hadoop-root/dfs/name; location= null. Not formatting.\n",
            "2025-06-18 18:17:07,334 INFO blockmanagement.DatanodeManager: Slow peers collection thread shutdown\n",
            "2025-06-18 18:17:07,378 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2025-06-18 18:17:07,378 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2025-06-18 18:17:07,381 INFO util.ExitUtil: Exiting with status 1: ExitException\n",
            "2025-06-18 18:17:07,389 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at dca594300707/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "deb79696-2f24-410a-90e1-e09111c68b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [dca594300707]\n",
            "dca594300707: Warning: Permanently added 'dca594300707' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Check if HDFS is in safe mode\n",
        "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
        "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
        "  hdfs dfsadmin -safemode leave\n",
        "else\n",
        "  echo \"Namenode is not in safe mode.\"\n",
        "fi"
      ],
      "metadata": {
        "id": "pOHsiWv9or7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple HDFS commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home\n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# if sampls_data does not exist, create it (so that the notebook can run also outside of Colab)\n",
        "mkdir -p sample_data\n",
        "touch sample_data/mnist_test.csv\n",
        "\n",
        "# Check if the file is empty and fill it if needed\n",
        "if [ ! -s sample_data/mnist_test.csv ]; then\n",
        "  echo -e \"0 1 2 3 4\\n5 6 7 8 9\" > sample_data/mnist_test.csv\n",
        "fi\n",
        "\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "25357ddf-891c-4639-8cc2-a6ce862b96dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2025-06-18 18:17 my_dir/mnist_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple MapReduce jobs\n",
        "\n",
        "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
        "\n",
        "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest MapReduce job\n",
        "\n",
        "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job"
      ],
      "metadata": {
        "id": "o6ICcKO2jcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output_simplest \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_simplest/part-00000"
      ],
      "metadata": {
        "id": "rB7FXYTbwNzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "id": "yJIm4SPZFPxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input || true\n",
        "hdfs dfs -put Linux_2k.log input/ || true"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "id": "4-rraIUdfdj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "id": "9qf1dFqIKgoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py"
      ],
      "metadata": {
        "id": "TJ0kDRsigCC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_filter\n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output_filter \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE\n"
      ],
      "metadata": {
        "id": "G7SEzMC2OqWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output_filter"
      ],
      "metadata": {
        "id": "RhLA5HZEhfmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_filter/part-00000 |head"
      ],
      "metadata": {
        "id": "Ffi4RvXnPH14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "id": "fMKEqUF1T-v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f905e8-800c-4441-ef2e-1d044c1d2b76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper"
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "k-R7VNoTVRjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "chmod +x myAggregatorForKeyCount.py\n",
        "\n",
        "hdfs dfs -rm -r output_aggregate\n",
        "\n",
        "mapred streaming \\\n",
        "  -input input \\\n",
        "  -output output_aggregate \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate \\\n",
        "  -file myAggregatorForKeyCount.py\n"
      ],
      "metadata": {
        "id": "XwxHJ7yyVR34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output_aggregate\n",
        "hdfs dfs -cat output_aggregate/part-00000"
      ],
      "metadata": {
        "id": "ET3KCfX1UC2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
        "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
        "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
      ],
      "metadata": {
        "id": "Y8IYl4hAZhZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.4.0/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "id": "IoIYG5NlsIMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop the `sshd` daemon"
      ],
      "metadata": {
        "id": "RGj96_e2ccZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/init.d/ssh stop"
      ],
      "metadata": {
        "id": "FUvKMpy6chQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding remarks\n",
        "\n",
        "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
        "\n",
        "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
        "\n",
        "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
        "\n",
        "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
        "\n",
        "If you found this notebook helpful, consider exploring:\n",
        " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
        " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
        " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5N7tb0HSbZB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYyg1O7ysUs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
