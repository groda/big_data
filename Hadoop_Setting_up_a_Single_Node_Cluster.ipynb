{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oEF3qldGPj3T",
        "oUuQjW2oNMcJ",
        "qFfOrktMPq8M",
        "ozBfXXK8HoSq",
        "KE7kSYSXQYLf",
        "RlgP1ytnRtUK",
        "KLmxLQeJSb4A",
        "kXbSKFyeMqr2",
        "k2-Fdp73cF0V",
        "-Uxmv3RdUwiF",
        "V68C4cDySyek",
        "HTDPwnVlSbHS",
        "xMrEiLB_VAeR",
        "CKRRbwDFv3ZQ",
        "G3KBe4R65bl1",
        "IF6-Z5RotAcO"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# HDFS and MapReduce on a single-node Hadoop cluster\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "In this tutorial/notebook we'll showcase the setup of a single-node cluster, following the guidelines outlined on https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html. Subsequently, we'll demonstrate the seamless execution of elementary HDFS and MapReduce commands.\n",
        "\n",
        "Upon downloading the software, several preliminary steps must be taken, including setting environment variables, generating SSH keys, and more. To streamline these tasks, we've consolidated them under the \"Prologue\" section.\n",
        "\n",
        "Upon completion of the prologue, we can launch a single-node Hadoop cluster on the current virtual machine.\n",
        "\n",
        "Following that, we'll execute a series of test HDFS commands and MapReduce jobs on the Hadoop cluster. These will be performed using a dataset sourced from a publicly available collection.\n",
        "\n",
        "Finally, we'll proceed to shut down the cluster.\n"
      ],
      "metadata": {
        "id": "oEF3qldGPj3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TABLE OF CONTENTS**\n",
        "* **[Prologue](#scrollTo=oUuQjW2oNMcJ)**\n",
        "\n",
        " * [Check the available Java version](#scrollTo=qFfOrktMPq8M)\n",
        "\n",
        " * [Download core Hadoop](#scrollTo=KE7kSYSXQYLf)\n",
        "\n",
        "   * [Verify the downloaded file](#scrollTo=lGI4TNXPamMr)\n",
        "\n",
        " * [Configure `PATH`](#scrollTo=RlgP1ytnRtUK)\n",
        "\n",
        " * [Configure `core-site.xml` and `hdfs-site.xml`](#scrollTo=KLmxLQeJSb4A)\n",
        "\n",
        " * [Set environment variables](#scrollTo=kXbSKFyeMqr2)\n",
        "\n",
        " * [Setup localhost access via SSH key](#scrollTo=k2-Fdp73cF0V)\n",
        "\n",
        "   * [Install `openssh` and start server](#scrollTo=-Uxmv3RdUwiF)\n",
        "\n",
        "   * [Generate key](#scrollTo=PYKoSlaENuyG)\n",
        "\n",
        "   * [Check SSH connection to localhost](#scrollTo=FwA6rKpScnVi)\n",
        "\n",
        "* **[Launch a single-node Hadoop cluster](#scrollTo=V68C4cDySyek)**\n",
        "\n",
        "   * [Initialize the namenode](#scrollTo=HTDPwnVlSbHS)\n",
        "\n",
        "   * [Start cluster](#scrollTo=xMrEiLB_VAeR)\n",
        "\n",
        "* **[Run some simple HDFS commands](#scrollTo=CKRRbwDFv3ZQ)**\n",
        "\n",
        "* **[Run some simple MapReduce jobs](#scrollTo=G3KBe4R65bl1)**\n",
        "\n",
        "   * [Simplest MapReduce job](#scrollTo=yVJA-3jSATGV)\n",
        "\n",
        "   * [Another MapReduce example: filter a log file](#scrollTo=BbosNo0TD3oH)\n",
        "\n",
        "   * [Aggregate data with MapReduce](#scrollTo=Sam22f-YT1xR)\n",
        "\n",
        "* **[Stop cluster](#scrollTo=IF6-Z5RotAcO)**\n",
        "\n",
        "* **[Concluding remarks](#scrollTo=w5N7tb0HSbZB)**\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "hGm3LhVEWXr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prologue"
      ],
      "metadata": {
        "id": "oUuQjW2oNMcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the available Java version\n",
        " Apache Hadoop 3.4.2 supports Java > 8 (JDK>8). See: https://hadoop.apache.org/docs/r3.4.2/\n"
      ],
      "metadata": {
        "id": "qFfOrktMPq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if Java version is one of `8`, `11`"
      ],
      "metadata": {
        "id": "EuWqBiV89ryq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7X0EZaMPrsD",
        "outputId": "bf004c2b-178e-426a-8fdb-9d4a7fdf73a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
            "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_MAJOR_VERSION=$(java -version 2>&1 | grep -m1 -Po '(\\d+\\.)+\\d+' | cut -d '.' -f1)\n",
        "if [[ $JAVA_MAJOR_VERSION -eq 11 || $JAVA_MAJOR_VERSION -eq 17 ]]\n",
        " then\n",
        " echo \"Java version is one of 11, 17 âœ“\"\n",
        " fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lABuOV124G4x",
        "outputId": "c6ac00ed-4fde-4365-b4d0-7088eeb28d56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java version is one of 11, 17 âœ“\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`"
      ],
      "metadata": {
        "id": "ozBfXXK8HoSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the path for the environment variable `JAVA_HOME`"
      ],
      "metadata": {
        "id": "uH4AGbkLP3iK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f $(which java)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCmk5GOqv0Y-",
        "outputId": "d0bd71d5-7b77-42ec-b625-798d84b709d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64/bin/java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "JAVA_HOME=$(readlink -f $(which java) | sed 's/\\/bin\\/java$//')\n",
        "echo $JAVA_HOME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd7en2Cv68ce",
        "outputId": "78357352-bea8-4fa8-ba4c-46333b6012d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract `JAVA_HOME` from the Java path by removing the `bin/java` part in the end"
      ],
      "metadata": {
        "id": "rGHKH3Vu9Nwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "java_home = subprocess.check_output(\n",
        "    \"readlink -f $(which java) | sed 's:/bin/java$::'\",\n",
        "    shell=True,\n",
        "    text=True\n",
        ").strip()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = java_home\n",
        "\n",
        "print(\"JAVA_HOME =\", os.environ[\"JAVA_HOME\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Z5lsrX-Gbb",
        "outputId": "9e5d6dff-0304-4741-d848-9511d83acac7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAVA_HOME = /usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download core Hadoop\n",
        "Download the latest stable version of the core Hadoop distribution from one of the download mirrors locations https://www.apache.org/dyn/closer.cgi/hadoop/common/.\n",
        "\n",
        "**Note** with the option `--no-clobber`, `wget` will not download the file if it already exists."
      ],
      "metadata": {
        "id": "KE7kSYSXQYLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LqS5Rkgyli",
        "outputId": "c6be4a08-a6d6-469d-ba57-38f41b4d6e94"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜hadoop-3.4.2.tar.gzâ€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncompress archive"
      ],
      "metadata": {
        "id": "um2CARkgg22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -d \"hadoop-3.4.2\" ]; then\n",
        "  tar xzf hadoop-3.4.2.tar.gz\n",
        "fi"
      ],
      "metadata": {
        "id": "C17WYI0mQRE8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify the downloaded file\n",
        "\n",
        "(see https://www.apache.org/dyn/closer.cgi/hadoop/common/)"
      ],
      "metadata": {
        "id": "lGI4TNXPamMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download sha512 file"
      ],
      "metadata": {
        "id": "ATofMJRXhJ4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget --no-clobber https://dlcdn.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz.sha512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhTinHLqCrFQ",
        "outputId": "b75e6fa0-bf83-4728-c5b2-6c5dd45444e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜hadoop-3.4.2.tar.gz.sha512â€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare"
      ],
      "metadata": {
        "id": "eL8FxjalhFAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "A=$(sha512sum hadoop-3.4.2.tar.gz | cut - -d' ' -f1)\n",
        "B=$(cut hadoop-3.4.2.tar.gz.sha512 -d' ' -f4)\n",
        "printf \"%s\\n%s\\n\" $A $B\n",
        "[[ $A == $B ]] && echo \"True\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL302M1OhFMH",
        "outputId": "99118be2-0eac-45c8-eb95-3ac9b9721dce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "79a383e156022d6690da359120b25db8146452265d92a4e890d9ea78c2078a01b661daf78163ee9b4acef7106b01fd5c8d1a55f7ad284f88b31ab3f402ae3acf\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `PATH`\n",
        "\n",
        "Add the Hadoop folder to the `PATH` environment variable\n"
      ],
      "metadata": {
        "id": "RlgP1ytnRtUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49xx-zSxIdxa",
        "outputId": "8fe9886d-2a97-4b54-faa1-fbbef9732c64"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HADOOP_HOME'] = os.path.join(os.getcwd(), 'hadoop-3.4.2')\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'"
      ],
      "metadata": {
        "id": "6V03we10Igek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for key, value in os.environ.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aif21X1ONvwH",
        "outputId": "f4351ec4-b6c7-4b05-c8c7-4115558ee0ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHELL: /bin/bash\n",
            "COLAB_JUPYTER_TRANSPORT: ipc\n",
            "CGROUP_MEMORY_EVENTS: /sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events\n",
            "VM_GCE_METADATA_HOST: 169.254.169.253\n",
            "MODEL_PROXY_HOST: https://mp.kaggle.net\n",
            "HOSTNAME: 642d0bb387c8\n",
            "LANGUAGE: en_US\n",
            "TBE_RUNTIME_ADDR: 172.28.0.1:8011\n",
            "GCE_METADATA_TIMEOUT: 3\n",
            "COLAB_JUPYTER_IP: 172.28.0.12\n",
            "COLAB_LANGUAGE_SERVER_PROXY_ROOT_URL: http://172.28.0.1:8013/\n",
            "KMP_LISTEN_PORT: 6000\n",
            "TF_FORCE_GPU_ALLOW_GROWTH: true\n",
            "ENV: /root/.bashrc\n",
            "PWD: /\n",
            "TBE_EPHEM_CREDS_ADDR: 172.28.0.1:8009\n",
            "COLAB_LANGUAGE_SERVER_PROXY_REQUEST_TIMEOUT: 30s\n",
            "TBE_CREDS_ADDR: 172.28.0.1:8008\n",
            "COLAB_JUPYTER_TOKEN: \n",
            "LAST_FORCED_REBUILD: 20250623\n",
            "TCLLIBPATH: /usr/share/tcltk/tcllib1.20\n",
            "COLAB_KERNEL_MANAGER_PROXY_HOST: 172.28.0.12\n",
            "UV_BUILD_CONSTRAINT: \n",
            "COLAB_WARMUP_DEFAULTS: 1\n",
            "HOME: /root\n",
            "LANG: en_US.UTF-8\n",
            "CLOUDSDK_CONFIG: /content/.config\n",
            "UV_SYSTEM_PYTHON: true\n",
            "COLAB_RELEASE_TAG: release-colab-external_20260202-060039_RC01\n",
            "KMP_TARGET_PORT: 9000\n",
            "KMP_EXTRA_ARGS: --logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https://colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/m-s-23hqupjoprsyd --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true \n",
            "UV_INSTALL_DIR: /usr/local/bin\n",
            "COLAB_LANGUAGE_SERVER_PROXY_LSP_DIRS: /datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages\n",
            "COLAB_KERNEL_MANAGER_PROXY_PORT: 6000\n",
            "CLOUDSDK_PYTHON: python3\n",
            "NO_GCE_CHECK: False\n",
            "PYTHONPATH: /env/python\n",
            "SHLVL: 0\n",
            "COLAB_LANGUAGE_SERVER_PROXY: /usr/colab/bin/language_service\n",
            "UV_CONSTRAINT: \n",
            "PYTHONUTF8: 1\n",
            "COLAB_GPU: \n",
            "GCS_READ_CACHE_BLOCK_SIZE_MB: 16\n",
            "LC_ALL: en_US.UTF-8\n",
            "COLAB_FILE_HANDLER_ADDR: localhost:3453\n",
            "PATH: /content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n",
            "COLAB_IMAGE_TYPE: cpu\n",
            "COLAB_DEBUG_ADAPTER_MUX_PATH: /usr/local/bin/dap_multiplexer\n",
            "PYTHONWARNINGS: ignore:::pip._internal.cli.base_command\n",
            "DEBIAN_FRONTEND: noninteractive\n",
            "COLAB_BACKEND_VERSION: next\n",
            "OLDPWD: /\n",
            "_PYVIZ_COMMS_INSTALLED: 1\n",
            "PYDEVD_USE_FRAME_EVAL: NO\n",
            "JPY_SESSION_NAME: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JPY_PARENT_PID: 87\n",
            "TERM: xterm-color\n",
            "CLICOLOR: 1\n",
            "PAGER: cat\n",
            "GIT_PAGER: cat\n",
            "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
            "ENABLE_DIRECTORYPREFETCHER: 1\n",
            "USE_AUTH_EPHEM: 1\n",
            "COLAB_NOTEBOOK_ID: 1A_2HaeNRuh87zaAGoljrtXoQUKBFU7TP\n",
            "JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64\n",
            "HADOOP_HOME: /content/hadoop-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcM-idgZQqfV",
        "outputId": "7ffdf9ad-c297-4446-da3e-502318864cbd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure `core-site.xml` and `hdfs-site.xml`\n",
        "\n",
        "Edit the file `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml` to configure pseudo-distributed operation.\n",
        "\n",
        "**`etc/hadoop/core-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```\n",
        "\n",
        "**`etc/hadoop/hdfs-site.xml`**\n",
        "```\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "```"
      ],
      "metadata": {
        "id": "KLmxLQeJSb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>fs.defaultFS</name> \\n\\\n",
        "        <value>hdfs://localhost:9000</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/core-site.xml > /dev/null\n",
        "\n",
        "echo -e \"<configuration> \\n\\\n",
        "    <property> \\n\\\n",
        "        <name>dfs.replication</name> \\n\\\n",
        "        <value>1</value> \\n\\\n",
        "    </property> \\n\\\n",
        "</configuration>\" | sudo tee hadoop-3.4.2/etc/hadoop/hdfs-site.xml > /dev/null\n"
      ],
      "metadata": {
        "id": "_n2d2lqXSLU1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "5mdkNb-Cg9HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat hadoop-3.4.2/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ISxE4Gqg_LG",
        "outputId": "75fbc2b8-2fe6-4137-9ac8-c6267d1344c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<configuration> \n",
            "    <property> \n",
            "        <name>dfs.replication</name> \n",
            "        <value>1</value> \n",
            "    </property> \n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set environment variables\n",
        "\n",
        "Add the following lines to the Hadoop configuration script `hadoop-env.sh`(the script is in `hadoop-3.4.2/sbin`).\n",
        "```\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "```"
      ],
      "metadata": {
        "id": "kXbSKFyeMqr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp -n hadoop-3.4.2/etc/hadoop/hadoop-env.sh hadoop-3.4.2/etc/hadoop/hadoop-env.sh.org\n",
        "cat <<ðŸ˜ƒ >hadoop-3.4.2/etc/hadoop/hadoop-env.sh\n",
        "export JAVA_HOME=\"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "export HDFS_NAMENODE_USER=root\n",
        "export HDFS_DATANODE_USER=root\n",
        "export HDFS_SECONDARYNAMENODE_USER=root\n",
        "export YARN_RESOURCEMANAGER_USER=root\n",
        "export YARN_NODEMANAGER_USER=root\n",
        "ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "2_vn-TGyPe9V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup localhost access via SSH key\n",
        "\n",
        "We are going to allow passphraseless access to `localhost` with a secure key.\n",
        "\n",
        "SSH must be installed and sshd must be running in order to use the Hadoop scripts that manage remote Hadoop daemons.\n"
      ],
      "metadata": {
        "id": "k2-Fdp73cF0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Install `openssh` and start server\n",
        "\n",
        "I'm not sure why we need the option `StrictHostKeyChecking no`. This option tells the `ssh` server to allow key authentication only from known hosts, in particular it prevents a host from authenticating with key if the key has changed. I guess this option is needed since a new ssh key is generated every time one runs this notebook.\n",
        "\n",
        "Alternatively, one could just delete the file `~/.ssh/known_hosts` or else use `ssh-keygen -R hostname` to remove all keys belonging to hostname from the `known_hosts` file (see for instance [How to remove strict RSA key checking in SSH and what's the problem here?](https://serverfault.com/questions/6233/how-to-remove-strict-rsa-key-checking-in-ssh-and-whats-the-problem-here) or [Remove key from known_hosts](https://superuser.com/questions/30087/remove-key-from-known-hosts)). The option `ssh-keygen -R hostname` would be the most appropriate in a production setting where the file `~/.ssh/known_hosts` might contain other entries that you do not want to delete.\n"
      ],
      "metadata": {
        "id": "-Uxmv3RdUwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install openssh-server\n",
        "# tee -a appends to the file using elevated privileges\n",
        "echo 'StrictHostKeyChecking no' | sudo tee -a /etc/ssh/ssh_config\n",
        "sudo /etc/init.d/ssh restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOxz683FNuYH",
        "outputId": "0eb7b49e-0992-4538-8362-1504122322e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "openssh-server is already the newest version (1:8.9p1-3ubuntu0.13).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "StrictHostKeyChecking no\n",
            " * Restarting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate key\n",
        "Generate an SSH key that does not require a password.\n",
        "\n",
        "The private key is contained in the file `id_rsa` located in the folder `~/.ssh`.\n",
        "\n",
        "The public key is added to the file `~/.ssh/authorized_keys` in order to allow authentication with that key."
      ],
      "metadata": {
        "id": "PYKoSlaENuyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm $HOME/.ssh/id_rsa\n",
        "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\n",
        "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
        "chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHOjUaxHSsQD",
        "outputId": "f1f2972f-03f5-48bc-b35b-23984f563431"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:D62qnIgTNIXm2eqjJOooYKo3pz1DJYqBdPz/XkS2f7s root@642d0bb387c8\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|  o              |\n",
            "| + +             |\n",
            "|= = .      o     |\n",
            "|o= ....  .o .    |\n",
            "|.oo. o. S .o     |\n",
            "|+o. .  . +. .    |\n",
            "|*o .    o .. . . |\n",
            "|O+o++. . ..   . .|\n",
            "|@+++=+. ..     E.|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check SSH connection to localhost\n",
        "\n",
        "The following command should output \"hi!\" if the connection works."
      ],
      "metadata": {
        "id": "FwA6rKpScnVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh localhost \"echo hi ðŸ‘‹\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqIRVxcfce0F",
        "outputId": "ff4a20cb-96e0-474a-b3a7-f13fbd53ae1d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi ðŸ‘‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Launch a single-node Hadoop cluster"
      ],
      "metadata": {
        "id": "V68C4cDySyek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the namenode"
      ],
      "metadata": {
        "id": "HTDPwnVlSbHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo env JAVA_HOME=$JAVA_HOME $HADOOP_HOME/bin/hdfs namenode -format -nonInteractive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-aicnKKLVKQ",
        "outputId": "a3292158-4732-4106-9620-bb4c3ca12ce2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "namenode is running as process 5374.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start cluster"
      ],
      "metadata": {
        "id": "xMrEiLB_VAeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXHowFfFEwAF",
        "outputId": "5d47e2e4-3a1d-45f1-8f49-7312664bada3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "localhost: namenode is running as process 5374.  Stop it first and ensure /tmp/hadoop-root-namenode.pid file is empty before retry.\n",
            "Starting datanodes\n",
            "localhost: datanode is running as process 5495.  Stop it first and ensure /tmp/hadoop-root-datanode.pid file is empty before retry.\n",
            "Starting secondary namenodes [642d0bb387c8]\n",
            "642d0bb387c8: secondarynamenode is running as process 5695.  Stop it first and ensure /tmp/hadoop-root-secondarynamenode.pid file is empty before retry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Check if HDFS is in safe mode\n",
        "if hdfs dfsadmin -safemode get | grep 'ON'; then\n",
        "  echo \"Namenode is in safe mode. Leaving safe mode...\"\n",
        "  hdfs dfsadmin -safemode leave\n",
        "else\n",
        "  echo \"Namenode is not in safe mode.\"\n",
        "fi"
      ],
      "metadata": {
        "id": "pOHsiWv9or7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f119d2-008e-4162-a93f-ba6df2b62133"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namenode is not in safe mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple HDFS commands"
      ],
      "metadata": {
        "id": "CKRRbwDFv3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# create directory \"my_dir\" in HDFS home\n",
        "hdfs dfs -mkdir /user\n",
        "hdfs dfs -mkdir /user/root # this is the \"home\" of user root on HDFS\n",
        "hdfs dfs -mkdir my_dir\n",
        "\n",
        "# if sampls_data does not exist, create it\n",
        "mkdir -p sample_data\n",
        "touch sample_data/mnist_test.csv\n",
        "\n",
        "# Check if the file is empty and fill it if needed\n",
        "if [ ! -s sample_data/mnist_test.csv ]; then\n",
        "  echo -e \"0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\" > sample_data/mnist_test.csv\n",
        "fi\n",
        "\n",
        "\n",
        "# upload file mnist_test.csv to my_dir\n",
        "hdfs dfs -put sample_data/mnist_test.csv my_dir/\n",
        "\n",
        "# show contents of directory my_dir\n",
        "hdfs dfs -ls -h my_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wuvOJTxX4O",
        "outputId": "07ec4877-d2c7-4299-d94e-52ccb9139366"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup     17.4 M 2026-02-04 18:51 my_dir/mnist_test.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mkdir: `/user': File exists\n",
            "mkdir: `/user/root': File exists\n",
            "mkdir: `my_dir': File exists\n",
            "put: `my_dir/mnist_test.csv': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run some simple MapReduce jobs\n",
        "\n",
        "We'll employ the [streaming](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html) library, which broadens our options by enabling the use of any programming language for both the mapper and/or the reducer.\n",
        "\n",
        "With this utility any executable or file containing code that the operating system can interpret and execute directly, can serve as mapper and/or reducer."
      ],
      "metadata": {
        "id": "G3KBe4R65bl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simplest MapReduce job\n",
        "\n",
        "This is a \"no-code\" example since we are going to use the existing Unix commands `cat` and `wc` respectively as mapper and as reducer. The result will show a line with three values: the counts of lines, words, and characters in the input file(s).\n",
        "\n",
        "Input folder is `/user/my_user/my_dir/`, output folder `/user/my_user/output_simplest`.\n",
        "\n",
        "**Note**: the output folder should not exist because it is created by Hadoop (this is in accordance with Hadoop's principle of not overwriting data)."
      ],
      "metadata": {
        "id": "yVJA-3jSATGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job"
      ],
      "metadata": {
        "id": "o6ICcKO2jcHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_simplest || hdfs namenode -format -nonInteractive\n",
        "mapred streaming \\\n",
        "  -input my_dir \\\n",
        "  -output output_simplest \\\n",
        "  -mapper /bin/cat \\\n",
        "  -reducer /usr/bin/wc"
      ],
      "metadata": {
        "id": "VDuQYWGi5b7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213d1d66-a009-4ca3-a45f-bf37e74267ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted output_simplest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-04 18:54:28,959 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 18:54:29,351 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 18:54:29,368 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 18:54:29,677 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1756936430_0001\n",
            "2026-02-04 18:54:29,680 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 18:54:29,883 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 18:54:29,883 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 18:54:29,885 INFO mapreduce.Job: Running job: job_local1756936430_0001\n",
            "2026-02-04 18:54:29,886 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 18:54:29,898 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:29,898 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:29,955 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 18:54:29,960 INFO mapred.LocalJobRunner: Starting task: attempt_local1756936430_0001_m_000000_0\n",
            "2026-02-04 18:54:29,996 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:29,998 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:30,027 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 18:54:30,034 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/my_dir/mnist_test.csv:0+18289443\n",
            "2026-02-04 18:54:30,062 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 18:54:30,111 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 18:54:30,111 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 18:54:30,111 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 18:54:30,111 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 18:54:30,111 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 18:54:30,114 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 18:54:30,116 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-04 18:54:30,121 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 18:54:30,125 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 18:54:30,125 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 18:54:30,125 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 18:54:30,126 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 18:54:30,126 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 18:54:30,127 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 18:54:30,128 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 18:54:30,128 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 18:54:30,128 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 18:54:30,129 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 18:54:30,129 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 18:54:30,275 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:30,277 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:30,288 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:30,289 INFO streaming.PipeMapRed: Records R/W=100/1\n",
            "2026-02-04 18:54:30,370 INFO streaming.PipeMapRed: R/W/S=1000/785/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:30,546 INFO streaming.PipeMapRed: R/W/S=10000/9962/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:30,548 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 18:54:30,550 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 18:54:30,554 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 18:54:30,554 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 18:54:30,554 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 18:54:30,554 INFO mapred.MapTask: bufstart = 0; bufend = 18319443; bufvoid = 104857600\n",
            "2026-02-04 18:54:30,554 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
            "2026-02-04 18:54:30,686 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 18:54:30,699 INFO mapred.Task: Task:attempt_local1756936430_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 18:54:30,704 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
            "2026-02-04 18:54:30,704 INFO mapred.Task: Task 'attempt_local1756936430_0001_m_000000_0' done.\n",
            "2026-02-04 18:54:30,710 INFO mapred.Task: Final Counters for attempt_local1756936430_0001_m_000000_0: Counters: 23\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141963\n",
            "\t\tFILE: Number of bytes written=19221037\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10000\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=26\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "2026-02-04 18:54:30,711 INFO mapred.LocalJobRunner: Finishing task: attempt_local1756936430_0001_m_000000_0\n",
            "2026-02-04 18:54:30,711 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 18:54:30,715 INFO mapred.LocalJobRunner: Starting task: attempt_local1756936430_0001_r_000000_0\n",
            "2026-02-04 18:54:30,716 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 18:54:30,726 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:30,726 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:30,726 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 18:54:30,729 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@35ad4e61\n",
            "2026-02-04 18:54:30,730 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 18:54:30,753 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 18:54:30,759 INFO reduce.EventFetcher: attempt_local1756936430_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 18:54:30,799 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1756936430_0001_m_000000_0 decomp: 18359445 len: 18359449 to MEMORY\n",
            "2026-02-04 18:54:30,822 INFO reduce.InMemoryMapOutput: Read 18359445 bytes from map-output for attempt_local1756936430_0001_m_000000_0\n",
            "2026-02-04 18:54:30,828 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 18359445, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->18359445\n",
            "2026-02-04 18:54:30,832 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 18:54:30,837 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:30,840 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 18:54:30,860 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 18:54:30,860 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 18:54:30,894 INFO mapreduce.Job: Job job_local1756936430_0001 running in uber mode : false\n",
            "2026-02-04 18:54:30,899 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 18:54:30,944 INFO reduce.MergeManagerImpl: Merged 1 segments, 18359445 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 18:54:30,944 INFO reduce.MergeManagerImpl: Merging 1 files, 18359449 bytes from disk\n",
            "2026-02-04 18:54:30,945 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 18:54:30,945 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 18:54:30,947 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 18357610 bytes\n",
            "2026-02-04 18:54:30,948 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:30,949 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/bin/wc]\n",
            "2026-02-04 18:54:30,952 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2026-02-04 18:54:30,954 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2026-02-04 18:54:30,999 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:31,000 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:31,004 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:31,030 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:31,241 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:31,243 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
            "2026-02-04 18:54:31,243 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 18:54:31,244 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 18:54:31,298 INFO mapred.Task: Task:attempt_local1756936430_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 18:54:31,303 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:31,303 INFO mapred.Task: Task attempt_local1756936430_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 18:54:31,331 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1756936430_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_simplest\n",
            "2026-02-04 18:54:31,332 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
            "2026-02-04 18:54:31,332 INFO mapred.Task: Task 'attempt_local1756936430_0001_r_000000_0' done.\n",
            "2026-02-04 18:54:31,333 INFO mapred.Task: Final Counters for attempt_local1756936430_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=36860893\n",
            "\t\tFILE: Number of bytes written=37580486\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=18289443\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=10000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=5\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 18:54:31,333 INFO mapred.LocalJobRunner: Finishing task: attempt_local1756936430_0001_r_000000_0\n",
            "2026-02-04 18:54:31,334 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 18:54:31,911 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 18:54:31,912 INFO mapreduce.Job: Job job_local1756936430_0001 completed successfully\n",
            "2026-02-04 18:54:31,922 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=37002856\n",
            "\t\tFILE: Number of bytes written=56801523\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=36578886\n",
            "\t\tHDFS: Number of bytes written=26\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10000\n",
            "\t\tMap output records=10000\n",
            "\t\tMap output bytes=18319443\n",
            "\t\tMap output materialized bytes=18359449\n",
            "\t\tInput split bytes=105\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=10000\n",
            "\t\tReduce shuffle bytes=18359449\n",
            "\t\tReduce input records=10000\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=20000\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=31\n",
            "\t\tTotal committed heap usage (bytes)=591396864\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=18289443\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=26\n",
            "2026-02-04 18:54:31,922 INFO streaming.StreamJob: Output directory: output_simplest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `output` directory contains the empty file `_SUCCESS`, this means that the job was successful."
      ],
      "metadata": {
        "id": "UiZ6FH2gFfE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the output of the MapReduce job."
      ],
      "metadata": {
        "id": "kHPEoIIWFubx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_simplest/part-00000"
      ],
      "metadata": {
        "id": "rB7FXYTbwNzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77701166-d2ea-4f4e-efe7-9c7a1582839f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  10000   10000 18299443\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of words is in this case equal to the number of lines because there are no word separators (empty spaces) in the file, so each line is a word."
      ],
      "metadata": {
        "id": "BDObCPW2F39S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another MapReduce example: filter a log file\n",
        "\n",
        "We're going to use a Linux logfile and look for the string `sshd` in a given position. The file stems from [Loghub](https://github.com/logpai/loghub), a freely available collection of system logs for AI-driven log analytics research.\n",
        "\n",
        "The mapper `mapper.py` filters the file for the given string `sshd` at field 4.\n",
        "\n",
        "The job has no reducer (option `-reducer NONE`). Note that without a reducer the sorting and shuffling phase after the map phase is skipped.\n"
      ],
      "metadata": {
        "id": "BbosNo0TD3oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the logfile `Linux_2k.log`:"
      ],
      "metadata": {
        "id": "iVdUuulwGzq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-clobber https://raw.githubusercontent.com/logpai/loghub/master/Linux/Linux_2k.log"
      ],
      "metadata": {
        "id": "yJIm4SPZFPxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af56392e-b712-437d-f78e-20c0f5a9203a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜Linux_2k.logâ€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -mkdir input || true\n",
        "hdfs dfs -put Linux_2k.log input/ || true"
      ],
      "metadata": {
        "id": "M1WgyQE3MYWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed7744f-39cb-4972-94f4-8e8c34df09f8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mkdir: `input': File exists\n",
            "put: `input/Linux_2k.log': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the mapper"
      ],
      "metadata": {
        "id": "ILUOCdzEH3Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    # split the line into words\n",
        "    line = line.strip()\n",
        "    fields = line.split()\n",
        "    if (len(fields)>=5 and fields[4].startswith('sshd')):\n",
        "      print(line)\n"
      ],
      "metadata": {
        "id": "4-rraIUdfdj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc4125b-3126-49b5-d785-a5a622d01082"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the script (after setting the correct permissions)"
      ],
      "metadata": {
        "id": "W8AxdFFPIuDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 mapper.py"
      ],
      "metadata": {
        "id": "QwOk_y7egbGM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the first 10 lines"
      ],
      "metadata": {
        "id": "fhv95VzfIAnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -10 Linux_2k.log"
      ],
      "metadata": {
        "id": "9qf1dFqIKgoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20be93f7-a4fc-42ef-d23d-3bd2dbf5a853"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\r\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4 \r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper in the shell (not using MapReduce):"
      ],
      "metadata": {
        "id": "eQ09Y1AqR6Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -100 Linux_2k.log| ./mapper.py"
      ],
      "metadata": {
        "id": "TJ0kDRsigCC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f003eb5-d6cc-410e-9d81-45f635100dbc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20896]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20897]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20898]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23397]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23395]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23404]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23399]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23406]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23394]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23396]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23407]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23403]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: check pass; user unknown\n",
            "Jun 15 12:12:34 combo sshd(pam_unix)[23412]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: check pass; user unknown\n",
            "Jun 15 12:13:19 combo sshd(pam_unix)[23414]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: check pass; user unknown\n",
            "Jun 15 12:13:20 combo sshd(pam_unix)[23416]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23661]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23663]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: check pass; user unknown\n",
            "Jun 15 14:53:32 combo sshd(pam_unix)[23664]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: check pass; user unknown\n",
            "Jun 15 14:53:33 combo sshd(pam_unix)[23665]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: check pass; user unknown\n",
            "Jun 15 14:53:34 combo sshd(pam_unix)[23669]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23671]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23673]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: check pass; user unknown\n",
            "Jun 15 14:53:35 combo sshd(pam_unix)[23674]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23678]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: check pass; user unknown\n",
            "Jun 15 14:53:36 combo sshd(pam_unix)[23677]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=061092085098.ctinets.com\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24138]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24137]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24141]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24140]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: check pass; user unknown\n",
            "Jun 15 20:05:31 combo sshd(pam_unix)[24139]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=d211-116-254-214.rev.krline.net\n",
            "Jun 17 19:43:13 combo sshd(pam_unix)[30565]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=211.46.224.253  user=guest\n",
            "Jun 17 20:29:26 combo sshd(pam_unix)[30631]: session opened for user test by (uid=509)\n",
            "Jun 17 20:34:57 combo sshd(pam_unix)[30631]: session closed for user test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the MapReduce job on the pseudo-cluster"
      ],
      "metadata": {
        "id": "DXbgh5g7OraF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "hdfs dfs -rm -r output_filter\n",
        "\n",
        "mapred streaming \\\n",
        "  -file mapper.py \\\n",
        "  -input input \\\n",
        "  -output output_filter \\\n",
        "  -mapper mapper.py \\\n",
        "  -reducer NONE\n"
      ],
      "metadata": {
        "id": "G7SEzMC2OqWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b370f4b-8008-4576-9fe7-5629848dcc57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted output_filter\n",
            "packageJobJar: [mapper.py] [] /tmp/streamjob9992742135488871831.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-04 18:54:42,365 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 18:54:43,630 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 18:54:43,976 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 18:54:43,998 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 18:54:44,214 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1309914515_0001\n",
            "2026-02-04 18:54:44,215 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 18:54:44,449 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1309914515_0001_da72e068-339a-4e35-a998-d2900de7f97e/mapper.py\n",
            "2026-02-04 18:54:44,556 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 18:54:44,558 INFO mapreduce.Job: Running job: job_local1309914515_0001\n",
            "2026-02-04 18:54:44,559 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 18:54:44,560 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 18:54:44,568 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:44,568 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:44,650 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 18:54:44,653 INFO mapred.LocalJobRunner: Starting task: attempt_local1309914515_0001_m_000000_0\n",
            "2026-02-04 18:54:44,677 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:44,677 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:44,692 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 18:54:44,699 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 18:54:44,723 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-04 18:54:44,801 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2026-02-04 18:54:44,809 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 18:54:44,811 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 18:54:44,811 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 18:54:44,812 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 18:54:44,812 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 18:54:44,813 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 18:54:44,814 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 18:54:44,814 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 18:54:44,814 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 18:54:44,815 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 18:54:44,815 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 18:54:44,816 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 18:54:44,920 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:44,923 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:44,924 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:44,937 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:44,948 INFO streaming.PipeMapRed: Records R/W=1289/1\n",
            "2026-02-04 18:54:44,970 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 18:54:44,974 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 18:54:44,979 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 18:54:45,035 INFO mapred.Task: Task:attempt_local1309914515_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 18:54:45,038 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 18:54:45,038 INFO mapred.Task: Task attempt_local1309914515_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-04 18:54:45,059 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1309914515_0001_m_000000_0' to hdfs://localhost:9000/user/root/output_filter\n",
            "2026-02-04 18:54:45,062 INFO mapred.LocalJobRunner: Records R/W=1289/1\n",
            "2026-02-04 18:54:45,062 INFO mapred.Task: Task 'attempt_local1309914515_0001_m_000000_0' done.\n",
            "2026-02-04 18:54:45,068 INFO mapred.Task: Final Counters for attempt_local1309914515_0001_m_000000_0: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=720551\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 18:54:45,068 INFO mapred.LocalJobRunner: Finishing task: attempt_local1309914515_0001_m_000000_0\n",
            "2026-02-04 18:54:45,068 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 18:54:45,567 INFO mapreduce.Job: Job job_local1309914515_0001 running in uber mode : false\n",
            "2026-02-04 18:54:45,568 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-04 18:54:45,570 INFO mapreduce.Job: Job job_local1309914515_0001 completed successfully\n",
            "2026-02-04 18:54:45,575 INFO mapreduce.Job: Counters: 21\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=663\n",
            "\t\tFILE: Number of bytes written=720551\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=85436\n",
            "\t\tHDFS: Number of read operations=9\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=677\n",
            "\t\tInput split bytes=102\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=92274688\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=85436\n",
            "2026-02-04 18:54:45,575 INFO streaming.StreamJob: Output directory: output_filter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the result"
      ],
      "metadata": {
        "id": "iuZJ2ACzSTJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls output_filter"
      ],
      "metadata": {
        "id": "RhLA5HZEhfmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a035769-1c48-4508-ea29-606f8bf74fa4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 18:54 output_filter/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup      85436 2026-02-04 18:54 output_filter/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -cat output_filter/part-00000 |head"
      ],
      "metadata": {
        "id": "Ffi4RvXnPH14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064aed9e-f308-48c8-853e-f00b0f9e65ad"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jun 14 15:16:01 combo sshd(pam_unix)[19939]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: check pass; user unknown\t\n",
            "Jun 14 15:16:02 combo sshd(pam_unix)[19937]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=218.188.2.4\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20882]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20884]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20883]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20885]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20886]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "Jun 15 02:04:59 combo sshd(pam_unix)[20893]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\t\n",
            "cat: Unable to write to output stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate data with MapReduce\n",
        "\n",
        "Following the example in [Hadoop Streaming/Aggregate package](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Aggregate_Package)"
      ],
      "metadata": {
        "id": "Sam22f-YT1xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile myAggregatorForKeyCount.py\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "\n",
        "def generateLongCountToken(id):\n",
        "    return \"LongValueSum:\" + id + \"\\t\" + \"1\"\n",
        "\n",
        "def main(argv):\n",
        "    line = sys.stdin.readline()\n",
        "    try:\n",
        "        while line:\n",
        "            line = line[:-1]\n",
        "            fields = line.split()\n",
        "            s = fields[4].split('[')[0]\n",
        "            print(generateLongCountToken(s))\n",
        "            line = sys.stdin.readline()\n",
        "    except \"end of file\":\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "     main(sys.argv)"
      ],
      "metadata": {
        "id": "fMKEqUF1T-v9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254e1fb9-0853-45f4-85d1-d0854e4935d7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting myAggregatorForKeyCount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set permissions"
      ],
      "metadata": {
        "id": "4b2S9K8FWDMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 700 myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "35DP8K2_WDYO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the mapper"
      ],
      "metadata": {
        "id": "r9M8lgxMVRYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -20 Linux_2k.log| ./myAggregatorForKeyCount.py"
      ],
      "metadata": {
        "id": "k-R7VNoTVRjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b586d1-5aa4-40a7-e248-d08623829581"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:logrotate:\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:su(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n",
            "LongValueSum:sshd(pam_unix)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the MapReduce job"
      ],
      "metadata": {
        "id": "vOEpMFvsVRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "chmod +x myAggregatorForKeyCount.py\n",
        "\n",
        "hdfs dfs -rm -r output_aggregate\n",
        "\n",
        "mapred streaming \\\n",
        "  -input input \\\n",
        "  -output output_aggregate \\\n",
        "  -mapper myAggregatorForKeyCount.py \\\n",
        "  -reducer aggregate \\\n",
        "  -file myAggregatorForKeyCount.py\n"
      ],
      "metadata": {
        "id": "XwxHJ7yyVR34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5fba70b-c54c-4009-f3fe-533808271f89"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted output_aggregate\n",
            "packageJobJar: [myAggregatorForKeyCount.py] [] /tmp/streamjob3004401346663127159.jar tmpDir=null\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-04 18:54:53,598 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "2026-02-04 18:54:54,782 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 18:54:55,188 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-04 18:54:55,202 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-04 18:54:55,458 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1780223367_0001\n",
            "2026-02-04 18:54:55,459 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-04 18:54:55,739 INFO mapred.LocalDistributedCacheManager: Localized file:/content/myAggregatorForKeyCount.py as file:/tmp/hadoop-root/mapred/local/job_local1780223367_0001_4f71e57e-3be1-43ba-8f16-ce5cc0b0da90/myAggregatorForKeyCount.py\n",
            "2026-02-04 18:54:55,851 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-04 18:54:55,853 INFO mapreduce.Job: Running job: job_local1780223367_0001\n",
            "2026-02-04 18:54:55,854 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-04 18:54:55,856 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-04 18:54:55,863 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:55,863 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:55,910 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-04 18:54:55,913 INFO mapred.LocalJobRunner: Starting task: attempt_local1780223367_0001_m_000000_0\n",
            "2026-02-04 18:54:55,936 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:55,939 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:55,961 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 18:54:55,971 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/root/input/Linux_2k.log:0+216485\n",
            "2026-02-04 18:54:55,996 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-04 18:54:56,049 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-04 18:54:56,050 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-04 18:54:56,050 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-04 18:54:56,051 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-04 18:54:56,051 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-04 18:54:56,057 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-04 18:54:56,066 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./myAggregatorForKeyCount.py]\n",
            "2026-02-04 18:54:56,076 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-04 18:54:56,079 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-04 18:54:56,080 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-04 18:54:56,081 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-04 18:54:56,081 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-04 18:54:56,082 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-04 18:54:56,083 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-04 18:54:56,083 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-04 18:54:56,084 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-04 18:54:56,084 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-04 18:54:56,084 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-04 18:54:56,087 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-04 18:54:56,217 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:56,217 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:56,219 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:56,228 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-04 18:54:56,242 INFO streaming.PipeMapRed: Records R/W=1201/1\n",
            "2026-02-04 18:54:56,284 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-04 18:54:56,287 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-04 18:54:56,293 INFO mapred.LocalJobRunner: \n",
            "2026-02-04 18:54:56,293 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-04 18:54:56,293 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-04 18:54:56,293 INFO mapred.MapTask: bufstart = 0; bufend = 48923; bufvoid = 104857600\n",
            "2026-02-04 18:54:56,296 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26206400(104825600); length = 7997/6553600\n",
            "2026-02-04 18:54:56,349 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-04 18:54:56,365 INFO mapred.Task: Task:attempt_local1780223367_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 18:54:56,373 INFO mapred.LocalJobRunner: Records R/W=1201/1\n",
            "2026-02-04 18:54:56,373 INFO mapred.Task: Task 'attempt_local1780223367_0001_m_000000_0' done.\n",
            "2026-02-04 18:54:56,379 INFO mapred.Task: Final Counters for attempt_local1780223367_0001_m_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1058\n",
            "\t\tFILE: Number of bytes written=721997\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=0\n",
            "\t\tHDFS: Number of read operations=5\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=1\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=3\n",
            "\t\tTotal committed heap usage (bytes)=192937984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "2026-02-04 18:54:56,379 INFO mapred.LocalJobRunner: Finishing task: attempt_local1780223367_0001_m_000000_0\n",
            "2026-02-04 18:54:56,380 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-04 18:54:56,383 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-04 18:54:56,383 INFO mapred.LocalJobRunner: Starting task: attempt_local1780223367_0001_r_000000_0\n",
            "2026-02-04 18:54:56,390 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-04 18:54:56,390 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-04 18:54:56,390 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-04 18:54:56,392 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2b235a8e\n",
            "2026-02-04 18:54:56,393 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-04 18:54:56,406 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-04 18:54:56,410 INFO reduce.EventFetcher: attempt_local1780223367_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-04 18:54:56,439 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1780223367_0001_m_000000_0 decomp: 778 len: 782 to MEMORY\n",
            "2026-02-04 18:54:56,442 INFO reduce.InMemoryMapOutput: Read 778 bytes from map-output for attempt_local1780223367_0001_m_000000_0\n",
            "2026-02-04 18:54:56,443 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 778, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->778\n",
            "2026-02-04 18:54:56,444 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-04 18:54:56,445 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:56,445 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-04 18:54:56,451 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 18:54:56,451 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 18:54:56,452 INFO reduce.MergeManagerImpl: Merged 1 segments, 778 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-04 18:54:56,453 INFO reduce.MergeManagerImpl: Merging 1 files, 782 bytes from disk\n",
            "2026-02-04 18:54:56,453 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-04 18:54:56,453 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-04 18:54:56,454 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 760 bytes\n",
            "2026-02-04 18:54:56,455 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:56,547 INFO mapred.Task: Task:attempt_local1780223367_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-04 18:54:56,550 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-04 18:54:56,550 INFO mapred.Task: Task attempt_local1780223367_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-04 18:54:56,577 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1780223367_0001_r_000000_0' to hdfs://localhost:9000/user/root/output_aggregate\n",
            "2026-02-04 18:54:56,579 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-04 18:54:56,579 INFO mapred.Task: Task 'attempt_local1780223367_0001_r_000000_0' done.\n",
            "2026-02-04 18:54:56,580 INFO mapred.Task: Final Counters for attempt_local1780223367_0001_r_000000_0: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2654\n",
            "\t\tFILE: Number of bytes written=722779\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=216485\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=10\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=3\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=30\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=192937984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 18:54:56,580 INFO mapred.LocalJobRunner: Finishing task: attempt_local1780223367_0001_r_000000_0\n",
            "2026-02-04 18:54:56,580 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-04 18:54:56,861 INFO mapreduce.Job: Job job_local1780223367_0001 running in uber mode : false\n",
            "2026-02-04 18:54:56,863 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-04 18:54:56,864 INFO mapreduce.Job: Job job_local1780223367_0001 completed successfully\n",
            "2026-02-04 18:54:56,873 INFO mapreduce.Job: Counters: 36\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3712\n",
            "\t\tFILE: Number of bytes written=1444776\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=432970\n",
            "\t\tHDFS: Number of bytes written=326\n",
            "\t\tHDFS: Number of read operations=15\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2000\n",
            "\t\tMap output records=2000\n",
            "\t\tMap output bytes=48923\n",
            "\t\tMap output materialized bytes=782\n",
            "\t\tInput split bytes=102\n",
            "\t\tCombine input records=2000\n",
            "\t\tCombine output records=30\n",
            "\t\tReduce input groups=30\n",
            "\t\tReduce shuffle bytes=782\n",
            "\t\tReduce input records=30\n",
            "\t\tReduce output records=30\n",
            "\t\tSpilled Records=60\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=3\n",
            "\t\tTotal committed heap usage (bytes)=385875968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=216485\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=326\n",
            "2026-02-04 18:54:56,873 INFO streaming.StreamJob: Output directory: output_aggregate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check result"
      ],
      "metadata": {
        "id": "NkuYUkh5W0Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -ls output_aggregate\n",
        "hdfs dfs -cat output_aggregate/part-00000"
      ],
      "metadata": {
        "id": "ET3KCfX1UC2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ca57b7e-4b45-4df3-f014-eea9f6080191"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2026-02-04 18:54 output_aggregate/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        326 2026-02-04 18:54 output_aggregate/part-00000\n",
            "--\t1\n",
            "bluetooth:\t2\n",
            "cups:\t12\n",
            "ftpd\t916\n",
            "gdm(pam_unix)\t2\n",
            "gdm-binary\t1\n",
            "gpm\t2\n",
            "hcid\t1\n",
            "irqbalance:\t1\n",
            "kernel:\t76\n",
            "klogind\t46\n",
            "login(pam_unix)\t2\n",
            "logrotate:\t43\n",
            "named\t16\n",
            "network:\t2\n",
            "nfslock:\t1\n",
            "portmap:\t1\n",
            "random:\t1\n",
            "rc:\t1\n",
            "rpc.statd\t1\n",
            "rpcidmapd:\t1\n",
            "sdpd\t1\n",
            "snmpd\t1\n",
            "sshd(pam_unix)\t677\n",
            "su(pam_unix)\t172\n",
            "sysctl:\t1\n",
            "syslog:\t2\n",
            "syslogd\t7\n",
            "udev\t8\n",
            "xinetd\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty-print table of aggregated data"
      ],
      "metadata": {
        "id": "Vj9qz8wSa1w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -get output_aggregate/part-00000 result # download results file\n",
        "# Use awk to format the output into columns and then sort by the second field numerically in descending order\n",
        "awk '{printf \"%-20s %s\\n\", $1, $2}' result | sort -k2nr"
      ],
      "metadata": {
        "id": "Y8IYl4hAZhZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c6fc03-0e6d-4cbc-b0b4-8ebab8445b39"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftpd                 916\n",
            "sshd(pam_unix)       677\n",
            "su(pam_unix)         172\n",
            "kernel:              76\n",
            "klogind              46\n",
            "logrotate:           43\n",
            "named                16\n",
            "cups:                12\n",
            "udev                 8\n",
            "syslogd              7\n",
            "bluetooth:           2\n",
            "gdm(pam_unix)        2\n",
            "gpm                  2\n",
            "login(pam_unix)      2\n",
            "network:             2\n",
            "syslog:              2\n",
            "xinetd               2\n",
            "--                   1\n",
            "gdm-binary           1\n",
            "hcid                 1\n",
            "irqbalance:          1\n",
            "nfslock:             1\n",
            "portmap:             1\n",
            "random:              1\n",
            "rc:                  1\n",
            "rpcidmapd:           1\n",
            "rpc.statd            1\n",
            "sdpd                 1\n",
            "snmpd                1\n",
            "sysctl:              1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "get: `result': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop cluster\n",
        "\n",
        "When you're done with your computations, you can shut down the Hadoop cluster and stop the `sshd` service."
      ],
      "metadata": {
        "id": "IF6-Z5RotAcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hadoop-3.4.2/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "id": "IoIYG5NlsIMv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5f9f4e-ea72-4515-e403-db89e815b8a2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping namenodes on [localhost]\n",
            "Stopping datanodes\n",
            "Stopping secondary namenodes [642d0bb387c8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop the `sshd` daemon"
      ],
      "metadata": {
        "id": "RGj96_e2ccZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/etc/init.d/ssh stop"
      ],
      "metadata": {
        "id": "FUvKMpy6chQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff26311-8b5e-4342-a8ca-3095e74f3aa9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Stopping OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concluding remarks\n",
        "\n",
        "We have started a single-node Hadoop cluster and ran some simple HDFS and MapReduce commands.\n",
        "\n",
        "Even when running on a single machine, one can benefit from the parallelism provided by multiple virtual cores.\n",
        "\n",
        "Hadoop provides also a command-line utility (the CLI MiniCluster) to start and stop a single-node Hadoop cluster \"_without the need to set any environment variables or manage configuration files_\" (https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CLIMiniCluster.html). The [Hadoop MiniCluster](https://github.com/groda/big_data/blob/master/Hadoop_minicluster.ipynb) notebook serves as a guide for launching the Hadoop MiniCluster.\n",
        "\n",
        "While it can be useful to be able to start a Hadoop cluster with a single command, delving into the functionality of each component offers valuable insights into the intricacies of Hadoop architecture, thereby enriching the learning process.\n",
        "\n",
        "If you found this notebook helpful, consider exploring:\n",
        " - [Hadoop single-node cluster setup with Python](https://github.com/groda/big_data/blob/master/Hadoop_single_node_cluster_setup_Python.ipynb) similar to this but using Python in place of bash\n",
        " - [Setting up Spark Standalone on Google Colab](https://github.com/groda/big_data/blob/master/Hadoop_Setting_up_Spark_Standalone_on_Google_Colab.ipynb)\n",
        " - [Getting to know the Spark Standalone Architecture](https://github.com/groda/big_data/blob/master/Spark_Standalone_Architecture_on_Google_Colab.ipynb)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5N7tb0HSbZB"
      }
    }
  ]
}