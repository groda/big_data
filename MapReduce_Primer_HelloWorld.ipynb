{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.3.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "6d0235ea-c824-47e5-c4c4-b6c1fe86b8eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not downloading, Hadoop folder hadoop-3.4.3 already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "32ba7ae6-63d5-4c6b-d25b-ca0ae9743519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.3\n",
            "PATH is hadoop-3.4.3/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-17-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-17-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "59107e8b-0375-4167-87f7-40eb233f9092"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note Nr.2:** in this notebook we haven't started any YARN, so `mapred` by default falls back to local mode. The option `-D mapreduce.framework.name=local` wouldn't therefore be needed when running the notebook in Colab. However, we use a Github runner to test the notebook and this runner might have a cached file `mapred-site.xml` with property `mapreduce.framework.name=yarn`. So, to make sure that MapReduce runs in local mode (and not YARN) we need to add this option (or edit the `mapred-site.xml` file)."
      ],
      "metadata": {
        "id": "Ur-B0DIxF14X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -D fs.defaultFS=file:/// -rm -r my_output\n",
        "# Force 'hdfs' to use the local filesystem for the rm command\n",
        "# hdfs dfs -D fs.defaultFS=file:/// -rm -r my_output || true\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "5d4ba0df-c4df-4c48-b8dd-1280d0324927"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-28 18:55:40,234 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-28 18:55:42,747 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:55:43,106 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-28 18:55:43,137 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-28 18:55:43,497 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1009557472_0001\n",
            "2026-02-28 18:55:43,500 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-28 18:55:43,834 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-28 18:55:43,837 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-28 18:55:43,838 INFO mapreduce.Job: Running job: job_local1009557472_0001\n",
            "2026-02-28 18:55:43,840 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-28 18:55:43,859 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:43,859 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:43,922 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-28 18:55:43,928 INFO mapred.LocalJobRunner: Starting task: attempt_local1009557472_0001_m_000000_0\n",
            "2026-02-28 18:55:43,985 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:43,985 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:44,012 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:55:44,023 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:55:44,040 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-28 18:55:44,160 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-28 18:55:44,160 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-28 18:55:44,160 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-28 18:55:44,160 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-28 18:55:44,160 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-28 18:55:44,167 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-28 18:55:44,171 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-28 18:55:44,184 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-28 18:55:44,191 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-28 18:55:44,191 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-28 18:55:44,192 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-28 18:55:44,193 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-28 18:55:44,194 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-28 18:55:44,197 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-28 18:55:44,198 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-28 18:55:44,198 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-28 18:55:44,199 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-28 18:55:44,200 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-28 18:55:44,202 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-28 18:55:44,244 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-28 18:55:44,246 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2026-02-28 18:55:44,247 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-28 18:55:44,248 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-28 18:55:44,254 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:55:44,254 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-28 18:55:44,254 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-28 18:55:44,254 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2026-02-28 18:55:44,255 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2026-02-28 18:55:44,267 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-28 18:55:44,299 INFO mapred.Task: Task:attempt_local1009557472_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:55:44,304 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2026-02-28 18:55:44,305 INFO mapred.Task: Task 'attempt_local1009557472_0001_m_000000_0' done.\n",
            "2026-02-28 18:55:44,315 INFO mapred.Task: Final Counters for attempt_local1009557472_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=860979\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2026-02-28 18:55:44,316 INFO mapred.LocalJobRunner: Finishing task: attempt_local1009557472_0001_m_000000_0\n",
            "2026-02-28 18:55:44,318 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-28 18:55:44,324 INFO mapred.LocalJobRunner: Starting task: attempt_local1009557472_0001_r_000000_0\n",
            "2026-02-28 18:55:44,326 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-28 18:55:44,337 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:44,337 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:44,338 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:55:44,342 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@11b13fec\n",
            "2026-02-28 18:55:44,344 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:55:44,379 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-28 18:55:44,383 INFO reduce.EventFetcher: attempt_local1009557472_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-28 18:55:44,432 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1009557472_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2026-02-28 18:55:44,439 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1009557472_0001_m_000000_0\n",
            "2026-02-28 18:55:44,441 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2026-02-28 18:55:44,444 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-28 18:55:44,446 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:44,448 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-28 18:55:44,463 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-28 18:55:44,464 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2026-02-28 18:55:44,467 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-28 18:55:44,470 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2026-02-28 18:55:44,472 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-28 18:55:44,472 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-28 18:55:44,474 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2026-02-28 18:55:44,475 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:44,485 INFO mapred.Task: Task:attempt_local1009557472_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:55:44,488 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:44,489 INFO mapred.Task: Task attempt_local1009557472_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-28 18:55:44,491 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1009557472_0001_r_000000_0' to file:/content/my_output\n",
            "2026-02-28 18:55:44,492 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-28 18:55:44,493 INFO mapred.Task: Task 'attempt_local1009557472_0001_r_000000_0' done.\n",
            "2026-02-28 18:55:44,494 INFO mapred.Task: Final Counters for attempt_local1009557472_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142016\n",
            "\t\tFILE: Number of bytes written=861029\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-28 18:55:44,494 INFO mapred.LocalJobRunner: Finishing task: attempt_local1009557472_0001_r_000000_0\n",
            "2026-02-28 18:55:44,494 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-28 18:55:44,858 INFO mapreduce.Job: Job job_local1009557472_0001 running in uber mode : false\n",
            "2026-02-28 18:55:44,859 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-28 18:55:44,861 INFO mapreduce.Job: Job job_local1009557472_0001 completed successfully\n",
            "2026-02-28 18:55:44,872 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283954\n",
            "\t\tFILE: Number of bytes written=1722008\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=591396864\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-28 18:55:44,872 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "890ee48d-9745-40e0-862b-fa829d49c137"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "94114bf5-0fa0-4a8d-b9e9-fcfb4512462e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2026-02-28 18:55 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2026-02-28 18:55 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "ecb98c10-604a-4712-b15b-84f515fbf4bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Feb 28 18:55 part-00000\n",
            "-rw-r--r-- 1 root root  0 Feb 28 18:55 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "8fc7f1c4-dd07-4484-8253-cf98161fad46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "cbfc91e3-09fe-4f14-e52a-322a732fdb68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-28 18:55:51,901 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "245c4ae6-395f-41c9-851a-54a62869f487"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-28 18:55:54,000 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-28 18:55:56,526 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:55:56,791 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-28 18:55:56,821 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-28 18:55:57,238 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1860379658_0001\n",
            "2026-02-28 18:55:57,240 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-28 18:55:57,578 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-28 18:55:57,582 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-28 18:55:57,584 INFO mapreduce.Job: Running job: job_local1860379658_0001\n",
            "2026-02-28 18:55:57,586 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-28 18:55:57,602 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:57,602 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:57,664 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-28 18:55:57,667 INFO mapred.LocalJobRunner: Starting task: attempt_local1860379658_0001_m_000000_0\n",
            "2026-02-28 18:55:57,709 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:57,709 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:57,737 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:55:57,750 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:55:57,769 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-28 18:55:57,863 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-28 18:55:57,863 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-28 18:55:57,863 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-28 18:55:57,863 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-28 18:55:57,863 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-28 18:55:57,868 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-28 18:55:57,878 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:55:57,878 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-28 18:55:57,878 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-28 18:55:57,878 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2026-02-28 18:55:57,878 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2026-02-28 18:55:57,888 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-28 18:55:57,905 INFO mapred.Task: Task:attempt_local1860379658_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:55:57,909 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:55:57,909 INFO mapred.Task: Task 'attempt_local1860379658_0001_m_000000_0' done.\n",
            "2026-02-28 18:55:57,922 INFO mapred.Task: Final Counters for attempt_local1860379658_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=858873\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=306184192\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2026-02-28 18:55:57,922 INFO mapred.LocalJobRunner: Finishing task: attempt_local1860379658_0001_m_000000_0\n",
            "2026-02-28 18:55:57,924 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-28 18:55:57,931 INFO mapred.LocalJobRunner: Starting task: attempt_local1860379658_0001_r_000000_0\n",
            "2026-02-28 18:55:57,933 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-28 18:55:57,944 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:55:57,944 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:55:57,944 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:55:57,952 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3ddf0c86\n",
            "2026-02-28 18:55:57,955 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:55:57,987 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-28 18:55:57,992 INFO reduce.EventFetcher: attempt_local1860379658_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-28 18:55:58,051 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1860379658_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2026-02-28 18:55:58,079 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1860379658_0001_m_000000_0\n",
            "2026-02-28 18:55:58,089 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2026-02-28 18:55:58,099 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-28 18:55:58,103 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:58,103 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-28 18:55:58,117 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-28 18:55:58,118 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2026-02-28 18:55:58,124 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-28 18:55:58,125 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2026-02-28 18:55:58,127 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-28 18:55:58,127 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-28 18:55:58,130 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2026-02-28 18:55:58,133 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:58,149 INFO mapred.Task: Task:attempt_local1860379658_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:55:58,154 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-28 18:55:58,154 INFO mapred.Task: Task attempt_local1860379658_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-28 18:55:58,159 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1860379658_0001_r_000000_0' to file:/content/my_output\n",
            "2026-02-28 18:55:58,164 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-28 18:55:58,165 INFO mapred.Task: Task 'attempt_local1860379658_0001_r_000000_0' done.\n",
            "2026-02-28 18:55:58,167 INFO mapred.Task: Final Counters for attempt_local1860379658_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142030\n",
            "\t\tFILE: Number of bytes written=858931\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=306184192\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-28 18:55:58,169 INFO mapred.LocalJobRunner: Finishing task: attempt_local1860379658_0001_r_000000_0\n",
            "2026-02-28 18:55:58,169 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-28 18:55:58,602 INFO mapreduce.Job: Job job_local1860379658_0001 running in uber mode : false\n",
            "2026-02-28 18:55:58,603 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-28 18:55:58,605 INFO mapreduce.Job: Job job_local1860379658_0001 completed successfully\n",
            "2026-02-28 18:55:58,617 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283968\n",
            "\t\tFILE: Number of bytes written=1717804\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=612368384\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-28 18:55:58,617 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "0ba4af65-bec4-4075-ca37-973310264313",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "0fb84260-323b-4072-92ff-3f5b1a2b39f8",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "d4fea035-7cc8-4be3-ed00-1802f7819469"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-28 18:56:02,705 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-28 18:56:07,042 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:56:07,352 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-28 18:56:07,386 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-28 18:56:07,811 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local728983436_0001\n",
            "2026-02-28 18:56:07,813 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-28 18:56:08,131 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-28 18:56:08,134 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-28 18:56:08,135 INFO mapreduce.Job: Running job: job_local728983436_0001\n",
            "2026-02-28 18:56:08,137 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-28 18:56:08,152 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:56:08,152 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:56:08,208 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-28 18:56:08,213 INFO mapred.LocalJobRunner: Starting task: attempt_local728983436_0001_m_000000_0\n",
            "2026-02-28 18:56:08,254 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:56:08,255 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:56:08,287 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:56:08,298 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:56:08,317 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-28 18:56:08,345 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:56:08,356 INFO mapred.Task: Task:attempt_local728983436_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:56:08,362 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:56:08,362 INFO mapred.Task: Task attempt_local728983436_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-28 18:56:08,365 INFO output.FileOutputCommitter: Saved output of task 'attempt_local728983436_0001_m_000000_0' to file:/content/my_output\n",
            "2026-02-28 18:56:08,367 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:56:08,367 INFO mapred.Task: Task 'attempt_local728983436_0001_m_000000_0' done.\n",
            "2026-02-28 18:56:08,385 INFO mapred.Task: Final Counters for attempt_local728983436_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=855381\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=287309824\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-28 18:56:08,385 INFO mapred.LocalJobRunner: Finishing task: attempt_local728983436_0001_m_000000_0\n",
            "2026-02-28 18:56:08,387 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-28 18:56:09,147 INFO mapreduce.Job: Job job_local728983436_0001 running in uber mode : false\n",
            "2026-02-28 18:56:09,149 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-28 18:56:09,152 INFO mapreduce.Job: Job job_local728983436_0001 completed successfully\n",
            "2026-02-28 18:56:09,159 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=855381\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=287309824\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-28 18:56:09,159 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "411ebaf5-f474-4feb-bea4-85fc20700a70"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "3a761830-27ed-4a53-d918-0eaa163edc12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-28 18:56:13,127 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-28 18:56:15,805 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-28 18:56:16,411 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-28 18:56:16,503 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-28 18:56:17,289 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1214826201_0001\n",
            "2026-02-28 18:56:17,293 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-28 18:56:17,924 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-28 18:56:17,926 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-28 18:56:17,928 INFO mapreduce.Job: Running job: job_local1214826201_0001\n",
            "2026-02-28 18:56:17,929 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-28 18:56:17,950 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:56:17,951 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:56:18,055 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-28 18:56:18,063 INFO mapred.LocalJobRunner: Starting task: attempt_local1214826201_0001_m_000000_0\n",
            "2026-02-28 18:56:18,171 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-28 18:56:18,172 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-28 18:56:18,196 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-28 18:56:18,209 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-28 18:56:18,233 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-28 18:56:18,253 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-28 18:56:18,265 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-28 18:56:18,269 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-28 18:56:18,269 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-28 18:56:18,272 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-28 18:56:18,273 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-28 18:56:18,273 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-28 18:56:18,275 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-28 18:56:18,277 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-28 18:56:18,278 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-28 18:56:18,279 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-28 18:56:18,280 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-28 18:56:18,281 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-28 18:56:18,322 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-28 18:56:18,323 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-28 18:56:18,325 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2026-02-28 18:56:18,325 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-28 18:56:18,331 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:56:18,349 INFO mapred.Task: Task:attempt_local1214826201_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-28 18:56:18,356 INFO mapred.LocalJobRunner: \n",
            "2026-02-28 18:56:18,356 INFO mapred.Task: Task attempt_local1214826201_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-28 18:56:18,364 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1214826201_0001_m_000000_0' to file:/content/my_output\n",
            "2026-02-28 18:56:18,370 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2026-02-28 18:56:18,372 INFO mapred.Task: Task 'attempt_local1214826201_0001_m_000000_0' done.\n",
            "2026-02-28 18:56:18,405 INFO mapred.Task: Final Counters for attempt_local1214826201_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=861803\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-28 18:56:18,405 INFO mapred.LocalJobRunner: Finishing task: attempt_local1214826201_0001_m_000000_0\n",
            "2026-02-28 18:56:18,409 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-28 18:56:18,940 INFO mapreduce.Job: Job job_local1214826201_0001 running in uber mode : false\n",
            "2026-02-28 18:56:18,941 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-28 18:56:18,945 INFO mapreduce.Job: Job job_local1214826201_0001 completed successfully\n",
            "2026-02-28 18:56:18,953 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141938\n",
            "\t\tFILE: Number of bytes written=861803\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-28 18:56:18,953 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "id": "Sa1UDPr6zKKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675bf081-7cb5-4660-f668-c3ddaf13ee23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}