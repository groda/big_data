{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.2.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "ac624913-99d6-4ec4-ae9c-79a7e46efd93"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3810222082.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  extract_path = tar_ref.extractall(path='.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "02855a43-364d-4399-a7dc-ab2a19fa4b5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.2\n",
            "PATH is hadoop-3.4.2/bin:/opt/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-17-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-17-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "ee1fb3c0-2c59-4b1f-aaf1-5fd134366396"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-17-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note Nr.2:** in this notebook we haven't started any YARN, so `mapred` by default falls back to local mode. The option `-D mapreduce.framework.name=local` wouldn't therefore be needed when running the notebook in Colab. However, we use a Github runner to test the notebook and this runner might have a cached file `mapred-site.xml` with property `mapreduce.framework.name=yarn`. So, to make sure that MapReduce runs in local mode (and not YARN) we need to add this option (or edit the `mapred-site.xml` file)."
      ],
      "metadata": {
        "id": "Ur-B0DIxF14X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "047556d3-096c-4ed5-fbee-6320a6bd5162"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2026-02-23 20:14:39,850 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-23 20:14:40,255 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-23 20:14:40,285 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-23 20:14:40,852 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1892719228_0001\n",
            "2026-02-23 20:14:40,854 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-23 20:14:41,200 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-23 20:14:41,200 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-23 20:14:41,202 INFO mapreduce.Job: Running job: job_local1892719228_0001\n",
            "2026-02-23 20:14:41,210 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-23 20:14:41,227 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:14:41,227 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:14:41,293 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-23 20:14:41,298 INFO mapred.LocalJobRunner: Starting task: attempt_local1892719228_0001_m_000000_0\n",
            "2026-02-23 20:14:41,362 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:14:41,362 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:14:41,402 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-23 20:14:41,415 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-23 20:14:41,434 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-23 20:14:41,702 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-23 20:14:41,702 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-23 20:14:41,702 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-23 20:14:41,702 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-23 20:14:41,702 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-23 20:14:41,707 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-23 20:14:41,709 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-23 20:14:41,717 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-23 20:14:41,721 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-23 20:14:41,721 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-23 20:14:41,722 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-23 20:14:41,723 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-23 20:14:41,723 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-23 20:14:41,726 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-23 20:14:41,726 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-23 20:14:41,727 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-23 20:14:41,727 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-23 20:14:41,729 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-23 20:14:41,730 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-23 20:14:41,764 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-23 20:14:41,765 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-23 20:14:41,766 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2026-02-23 20:14:41,766 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-23 20:14:41,770 INFO mapred.LocalJobRunner: \n",
            "2026-02-23 20:14:41,770 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-23 20:14:41,770 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-23 20:14:41,770 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2026-02-23 20:14:41,770 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2026-02-23 20:14:41,782 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-23 20:14:41,817 INFO mapred.Task: Task:attempt_local1892719228_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-23 20:14:41,825 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2026-02-23 20:14:41,825 INFO mapred.Task: Task 'attempt_local1892719228_0001_m_000000_0' done.\n",
            "2026-02-23 20:14:41,845 INFO mapred.Task: Final Counters for attempt_local1892719228_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=859217\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=285212672\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2026-02-23 20:14:41,847 INFO mapred.LocalJobRunner: Finishing task: attempt_local1892719228_0001_m_000000_0\n",
            "2026-02-23 20:14:41,850 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-23 20:14:41,859 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-23 20:14:41,859 INFO mapred.LocalJobRunner: Starting task: attempt_local1892719228_0001_r_000000_0\n",
            "2026-02-23 20:14:41,877 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:14:41,877 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:14:41,877 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-23 20:14:41,885 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@292673af\n",
            "2026-02-23 20:14:41,887 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-23 20:14:41,925 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-23 20:14:41,928 INFO reduce.EventFetcher: attempt_local1892719228_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-23 20:14:41,990 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1892719228_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2026-02-23 20:14:41,999 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1892719228_0001_m_000000_0\n",
            "2026-02-23 20:14:42,003 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2026-02-23 20:14:42,006 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-23 20:14:42,008 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:14:42,009 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-23 20:14:42,021 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-23 20:14:42,022 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2026-02-23 20:14:42,024 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-23 20:14:42,025 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2026-02-23 20:14:42,027 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-23 20:14:42,027 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-23 20:14:42,029 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2026-02-23 20:14:42,030 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:14:42,044 INFO mapred.Task: Task:attempt_local1892719228_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-23 20:14:42,047 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:14:42,047 INFO mapred.Task: Task attempt_local1892719228_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-23 20:14:42,051 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1892719228_0001_r_000000_0' to file:/content/my_output\n",
            "2026-02-23 20:14:42,052 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-23 20:14:42,052 INFO mapred.Task: Task 'attempt_local1892719228_0001_r_000000_0' done.\n",
            "2026-02-23 20:14:42,055 INFO mapred.Task: Final Counters for attempt_local1892719228_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142018\n",
            "\t\tFILE: Number of bytes written=859267\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=285212672\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-23 20:14:42,055 INFO mapred.LocalJobRunner: Finishing task: attempt_local1892719228_0001_r_000000_0\n",
            "2026-02-23 20:14:42,055 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-23 20:14:42,223 INFO mapreduce.Job: Job job_local1892719228_0001 running in uber mode : false\n",
            "2026-02-23 20:14:42,224 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-23 20:14:42,226 INFO mapreduce.Job: Job job_local1892719228_0001 completed successfully\n",
            "2026-02-23 20:14:42,235 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283958\n",
            "\t\tFILE: Number of bytes written=1718484\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=570425344\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-23 20:14:42,235 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "521c4b3b-d111-4f22-e148-cd439cb3f46f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "165ff247-96b8-4d06-d521-73ca06fd5fe4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2026-02-23 08:19 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2026-02-23 08:19 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "672c7693-6677-416a-80d3-63090293c263"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Feb 23 08:19 part-00000\n",
            "-rw-r--r-- 1 root root  0 Feb 23 08:19 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "aaf552ac-60ea-4a46-9fcc-27e8889f668d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "cf338ecf-19b5-44cf-84b0-cd7ddf67660e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-21 06:55:12,015 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "bb03b18e-d8c5-4bc6-c9cf-1b657a719b94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-23 20:15:35,769 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-23 20:15:38,212 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-23 20:15:38,505 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-23 20:15:38,539 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-23 20:15:38,884 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local677713325_0001\n",
            "2026-02-23 20:15:38,886 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-23 20:15:39,157 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-23 20:15:39,158 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-23 20:15:39,160 INFO mapreduce.Job: Running job: job_local677713325_0001\n",
            "2026-02-23 20:15:39,162 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-23 20:15:39,175 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:15:39,177 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:15:39,245 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-23 20:15:39,249 INFO mapred.LocalJobRunner: Starting task: attempt_local677713325_0001_m_000000_0\n",
            "2026-02-23 20:15:39,299 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:15:39,299 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:15:39,327 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-23 20:15:39,339 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-23 20:15:39,363 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2026-02-23 20:15:39,451 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-23 20:15:39,452 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-23 20:15:39,452 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-23 20:15:39,452 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-23 20:15:39,452 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-23 20:15:39,459 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-23 20:15:39,467 INFO mapred.LocalJobRunner: \n",
            "2026-02-23 20:15:39,467 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-23 20:15:39,468 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-23 20:15:39,468 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2026-02-23 20:15:39,468 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2026-02-23 20:15:39,477 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-23 20:15:39,493 INFO mapred.Task: Task:attempt_local677713325_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-23 20:15:39,496 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2026-02-23 20:15:39,497 INFO mapred.Task: Task 'attempt_local677713325_0001_m_000000_0' done.\n",
            "2026-02-23 20:15:39,509 INFO mapred.Task: Final Counters for attempt_local677713325_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=853659\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2026-02-23 20:15:39,510 INFO mapred.LocalJobRunner: Finishing task: attempt_local677713325_0001_m_000000_0\n",
            "2026-02-23 20:15:39,513 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-23 20:15:39,519 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-23 20:15:39,520 INFO mapred.LocalJobRunner: Starting task: attempt_local677713325_0001_r_000000_0\n",
            "2026-02-23 20:15:39,533 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-23 20:15:39,533 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-23 20:15:39,534 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-23 20:15:39,541 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5b060b1\n",
            "2026-02-23 20:15:39,543 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-23 20:15:39,587 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-23 20:15:39,590 INFO reduce.EventFetcher: attempt_local677713325_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-23 20:15:39,640 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local677713325_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2026-02-23 20:15:39,647 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local677713325_0001_m_000000_0\n",
            "2026-02-23 20:15:39,650 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2026-02-23 20:15:39,652 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-23 20:15:39,654 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:15:39,654 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-23 20:15:39,666 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-23 20:15:39,667 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2026-02-23 20:15:39,669 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-23 20:15:39,670 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2026-02-23 20:15:39,671 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-23 20:15:39,671 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-23 20:15:39,672 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2026-02-23 20:15:39,673 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:15:39,684 INFO mapred.Task: Task:attempt_local677713325_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-23 20:15:39,686 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-23 20:15:39,686 INFO mapred.Task: Task attempt_local677713325_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-23 20:15:39,688 INFO output.FileOutputCommitter: Saved output of task 'attempt_local677713325_0001_r_000000_0' to file:/content/my_output\n",
            "2026-02-23 20:15:39,690 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-23 20:15:39,690 INFO mapred.Task: Task 'attempt_local677713325_0001_r_000000_0' done.\n",
            "2026-02-23 20:15:39,691 INFO mapred.Task: Final Counters for attempt_local677713325_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142032\n",
            "\t\tFILE: Number of bytes written=853717\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=295698432\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-23 20:15:39,691 INFO mapred.LocalJobRunner: Finishing task: attempt_local677713325_0001_r_000000_0\n",
            "2026-02-23 20:15:39,691 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-23 20:15:40,173 INFO mapreduce.Job: Job job_local677713325_0001 running in uber mode : false\n",
            "2026-02-23 20:15:40,175 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-23 20:15:40,176 INFO mapreduce.Job: Job job_local677713325_0001 completed successfully\n",
            "2026-02-23 20:15:40,186 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283972\n",
            "\t\tFILE: Number of bytes written=1707376\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=591396864\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-23 20:15:40,186 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "89633306-1f0c-45a6-9c06-b9541e3282a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "9860b4a5-fb73-4729-a40a-f864d19ce553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "c6a61e0e-71c9-4a24-e2de-ca64074c79ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-21 06:55:22,708 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-21 06:55:25,949 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-21 06:55:26,207 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-21 06:55:26,231 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-21 06:55:26,609 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local934959884_0001\n",
            "2026-02-21 06:55:26,611 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-21 06:55:26,948 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-21 06:55:26,950 INFO mapreduce.Job: Running job: job_local934959884_0001\n",
            "2026-02-21 06:55:26,951 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-21 06:55:26,952 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-21 06:55:26,964 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-21 06:55:26,964 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-21 06:55:27,015 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-21 06:55:27,019 INFO mapred.LocalJobRunner: Starting task: attempt_local934959884_0001_m_000000_0\n",
            "2026-02-21 06:55:27,053 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-21 06:55:27,053 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-21 06:55:27,074 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-21 06:55:27,083 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-21 06:55:27,099 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-21 06:55:27,116 INFO mapred.LocalJobRunner: \n",
            "2026-02-21 06:55:27,125 INFO mapred.Task: Task:attempt_local934959884_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-21 06:55:27,127 INFO mapred.LocalJobRunner: \n",
            "2026-02-21 06:55:27,127 INFO mapred.Task: Task attempt_local934959884_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-21 06:55:27,129 INFO output.FileOutputCommitter: Saved output of task 'attempt_local934959884_0001_m_000000_0' to file:/content/my_output\n",
            "2026-02-21 06:55:27,130 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2026-02-21 06:55:27,130 INFO mapred.Task: Task 'attempt_local934959884_0001_m_000000_0' done.\n",
            "2026-02-21 06:55:27,139 INFO mapred.Task: Final Counters for attempt_local934959884_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=853625\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=276824064\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-21 06:55:27,139 INFO mapred.LocalJobRunner: Finishing task: attempt_local934959884_0001_m_000000_0\n",
            "2026-02-21 06:55:27,140 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-21 06:55:27,962 INFO mapreduce.Job: Job job_local934959884_0001 running in uber mode : false\n",
            "2026-02-21 06:55:27,964 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-21 06:55:27,967 INFO mapreduce.Job: Job job_local934959884_0001 completed successfully\n",
            "2026-02-21 06:55:27,974 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=853625\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=276824064\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2026-02-21 06:55:27,974 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "719e4b6f-3531-41a1-d151-c400f60d3b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.framework.name=local \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "44bb9baf-4823-4964-96b0-b00e5dab8231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-21 06:55:31,753 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2026-02-21 06:55:34,193 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-21 06:55:34,499 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2026-02-21 06:55:34,530 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-21 06:55:35,081 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1283863364_0001\n",
            "2026-02-21 06:55:35,083 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-21 06:55:35,568 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-21 06:55:35,570 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-21 06:55:35,574 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2026-02-21 06:55:35,577 INFO mapreduce.Job: Running job: job_local1283863364_0001\n",
            "2026-02-21 06:55:35,592 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-21 06:55:35,592 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-21 06:55:35,708 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-21 06:55:35,723 INFO mapred.LocalJobRunner: Starting task: attempt_local1283863364_0001_m_000000_0\n",
            "2026-02-21 06:55:35,820 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-21 06:55:35,820 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-21 06:55:35,866 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-21 06:55:35,890 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2026-02-21 06:55:35,927 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2026-02-21 06:55:35,953 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2026-02-21 06:55:35,975 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2026-02-21 06:55:35,981 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2026-02-21 06:55:35,981 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2026-02-21 06:55:35,981 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2026-02-21 06:55:35,986 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2026-02-21 06:55:35,987 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2026-02-21 06:55:35,992 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2026-02-21 06:55:35,993 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2026-02-21 06:55:35,993 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2026-02-21 06:55:35,996 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2026-02-21 06:55:35,997 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-21 06:55:36,000 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2026-02-21 06:55:36,067 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2026-02-21 06:55:36,068 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2026-02-21 06:55:36,069 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2026-02-21 06:55:36,070 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2026-02-21 06:55:36,076 INFO mapred.LocalJobRunner: \n",
            "2026-02-21 06:55:36,097 INFO mapred.Task: Task:attempt_local1283863364_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-21 06:55:36,103 INFO mapred.LocalJobRunner: \n",
            "2026-02-21 06:55:36,103 INFO mapred.Task: Task attempt_local1283863364_0001_m_000000_0 is allowed to commit now\n",
            "2026-02-21 06:55:36,111 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1283863364_0001_m_000000_0' to file:/content/my_output\n",
            "2026-02-21 06:55:36,113 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2026-02-21 06:55:36,113 INFO mapred.Task: Task 'attempt_local1283863364_0001_m_000000_0' done.\n",
            "2026-02-21 06:55:36,136 INFO mapred.Task: Final Counters for attempt_local1283863364_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=860043\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-21 06:55:36,137 INFO mapred.LocalJobRunner: Finishing task: attempt_local1283863364_0001_m_000000_0\n",
            "2026-02-21 06:55:36,139 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-21 06:55:36,594 INFO mapreduce.Job: Job job_local1283863364_0001 running in uber mode : false\n",
            "2026-02-21 06:55:36,596 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-21 06:55:36,601 INFO mapreduce.Job: Job job_local1283863364_0001 completed successfully\n",
            "2026-02-21 06:55:36,609 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141940\n",
            "\t\tFILE: Number of bytes written=860043\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2026-02-21 06:55:36,609 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "id": "Sa1UDPr6zKKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}