{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/PySpark_miscellanea_on_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6244506",
      "metadata": {
        "id": "d6244506"
      },
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# PySpark miscellanea\n",
        "<br>\n",
        "\n",
        "\n",
        "    \n",
        "<h3>Table of Contents<span class=\"tocSkip\"></span></h3>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#How-to-get-your-application's-id-in-pyspark\" data-toc-modified-id=\"How-to-get-your-application's-id-in-pyspark-1.1\">How to get your application's id in pyspark</a></span></li><li><span><a href=\"#How-to-get/set-default-parallelism-in-pyspark\" data-toc-modified-id=\"How-to-get/set-default-parallelism-in-pyspark-1.2\">How to get/set default parallelism in pyspark</a></span></li><li><span><a href=\"#About--spark-defaults.conf\" data-toc-modified-id=\"About--spark-defaults.conf-1.3\">About  <code>spark-defaults.conf</code></a></span></li></ul></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "sQMqWsAziAhH"
      },
      "id": "sQMqWsAziAhH"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install pyspark\n",
        "pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm-8qt32hxyG",
        "outputId": "1e47b576-47e6-4607-9f24-f69172d4c87e"
      },
      "id": "jm-8qt32hxyG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n",
            "Name: pyspark\n",
            "Version: 4.0.2\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: dataproc-spark-connect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfe7c0a",
      "metadata": {
        "id": "9dfe7c0a"
      },
      "source": [
        "## How to get your application's id in PySpark\n",
        "\n",
        "\n",
        "See also: [How to extract application ID from the PySpark context](https://stackoverflow.com/questions/30983226/how-to-extract-application-id-from-the-pyspark-context)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02dbf961",
      "metadata": {
        "id": "02dbf961"
      },
      "source": [
        "### Starting from a Spark session\n",
        "\n",
        "What is a [Spark session](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd4ce7b",
      "metadata": {
        "id": "0fd4ce7b"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"App ID\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d02813d3",
      "metadata": {
        "id": "d02813d3"
      },
      "source": [
        "Get the session's context ([what is a Spark context?](https://spark.apache.org/docs/latest/rdd-programming-guide.html#initializing-spark) and [detailed documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8b99e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d8b99e3",
        "outputId": "91128c3e-8a4e-4ffc-be49-4906438f7f89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=App ID>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://16d9eb535e16:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>App ID</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57df230b",
      "metadata": {
        "id": "57df230b"
      },
      "source": [
        "Get `applicationId` from the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed1685a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bed1685a",
        "outputId": "615aa869-24e9-4d7c-fa00-a23a0fe25445"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'local-1771620809822'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "sc.applicationId"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de33f1ce",
      "metadata": {
        "id": "de33f1ce"
      },
      "source": [
        "All in one step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0023074",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0023074",
        "outputId": "7b7bbd56-2238-41df-a15b-7460abc80612"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'local-1771620809822'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "spark.sparkContext.applicationId"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc31a770",
      "metadata": {
        "id": "dc31a770"
      },
      "source": [
        "**Note:** if you're using the _pyspark shell_ (see [using the shell](https://spark.apache.org/docs/latest/rdd-programming-guide.html#initializing-spark)), `SparkContext` is created automatically and it can be accessed from the variable called `sc`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ece7a4",
      "metadata": {
        "id": "44ece7a4"
      },
      "source": [
        "## How to get/set default parallelism in PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880e6ada",
      "metadata": {
        "id": "880e6ada"
      },
      "source": [
        "Create a [Spark session](https://spark.apache.org/docs/latest/sql-getting-started.html#starting-point-sparksession)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451653da",
      "metadata": {
        "id": "451653da"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"defaultParallelism\") \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2fe39a2",
      "metadata": {
        "id": "d2fe39a2"
      },
      "source": [
        "Check the value of `defaultParallelism`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f221e3d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f221e3d1",
        "outputId": "86dade8b-dbb3-4359-c468-9a95090385bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spark.sparkContext.defaultParallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b3994c",
      "metadata": {
        "id": "69b3994c"
      },
      "source": [
        "To change a property it's necessary to stop and start a new context/session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cf47f42",
      "metadata": {
        "id": "4cf47f42"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Set parallelism\") \\\n",
        "        .config(\"spark.default.parallelism\", 8) \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6725687",
      "metadata": {
        "id": "c6725687"
      },
      "source": [
        "Default parallelism hasn't changed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b592d7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b592d7e",
        "outputId": "a94ef65a-801c-44e6-8c73-0ec0cc74e356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "spark.sparkContext.defaultParallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e168d0",
      "metadata": {
        "id": "49e168d0"
      },
      "source": [
        "Stop and start session anew."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0512cd",
      "metadata": {
        "id": "da0512cd"
      },
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"Set parallelism\") \\\n",
        "        .config(\"spark.default.parallelism\", 4) \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba24e5e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bba24e5e",
        "outputId": "146eb3b1-24a0-40c5-94cd-00255b47fd5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "spark.sparkContext.defaultParallelism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ef9f91f",
      "metadata": {
        "id": "1ef9f91f"
      },
      "source": [
        "Great! Now the context has been changed (and also the applications's name has been updated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2fbc63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "0e2fbc63",
        "outputId": "f46b8bb6-d2e0-450e-a16f-f803084461ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Set parallelism>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://16d9eb535e16:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Set parallelism</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "238722de",
      "metadata": {
        "id": "238722de"
      },
      "source": [
        "### What is `spark.default.parallelism`?\n",
        "\n",
        "This property determines the default number of chunks in which an RDD ([Resilient Distributed Dataset](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)) is partitioned.\n",
        "\n",
        "Unless specified by the user, the value of is set based on the _cluster manager_:\n",
        " - in standalone mode it is equal to the number of (virtual) cores on the local machine\n",
        " - in Mesos: 8\n",
        " - for YARN, Kubernetes: total number of cores on all executor nodes or 2, whichever is larger\n",
        "\n",
        " (see [Spark configuration/Execution behavior](https://spark.apache.org/docs/latest/configuration.html#execution-behavior))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb775c1",
      "metadata": {
        "id": "8cb775c1"
      },
      "source": [
        "## About  `spark-defaults.conf`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb5650c",
      "metadata": {
        "id": "4cb5650c"
      },
      "source": [
        "The file `spark-defaults.conf` contains the default Spark configuration properties and it is by default located in Spark's configuration directory `$SPARK_HOME/conf` (see [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)).\n",
        "\n",
        "The format of `spark-defaults.conf` is whitespace-separated lines containing property name and value, for instance:\n",
        "```\n",
        "spark.master            spark://5.6.7.8:7077\n",
        "spark.executor.memory   4g\n",
        "spark.eventLog.enabled  true\n",
        "spark.serializer        org.apache.spark.serializer.KryoSerializer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81a5dae5",
      "metadata": {
        "id": "81a5dae5"
      },
      "source": [
        "### Where is my `spark-defaults.conf`?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `findspark` to find the location of `SPARK_HOME`."
      ],
      "metadata": {
        "id": "zm4AzgMZmtjF"
      },
      "id": "zm4AzgMZmtjF"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzGcNtRWlObJ",
        "outputId": "0c114044-28a0-43bb-d0c1-3ee0fe3a028c"
      },
      "id": "AzGcNtRWlObJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "findspark.init()\n",
        "os.environ[\"SPARK_HOME\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dZghO2oqlREb",
        "outputId": "03c86bcf-3534-4ad8-b960-d9cc494fc574"
      },
      "id": "dZghO2oqlREb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.12/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look for all files called `spark-defaults*` in Spark's configuration directory:"
      ],
      "metadata": {
        "id": "obTrz_VXmmVg"
      },
      "id": "obTrz_VXmmVg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fede028",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fede028",
        "outputId": "3e2f1879-2abf-4b44-836b-e4cde53db3cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import glob\n",
        "glob.glob(os.path.join(os.environ[\"SPARK_HOME\"], \"conf\", \"spark-defaults*\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If no file `spark-defaults.conf` is contained in Spark's configuration directory, you should find a _template_ configuration file `spark-defaults.conf.template`. You can rename this to `spark-defaults.conf` and use it as default configuration file.\n"
      ],
      "metadata": {
        "id": "pqWv5HTmG3V7"
      },
      "id": "pqWv5HTmG3V7"
    },
    {
      "cell_type": "markdown",
      "id": "077ff7c0",
      "metadata": {
        "id": "077ff7c0"
      },
      "source": [
        "If neither `spark-defaults.conf` nor a template file is found, create  `spark-defaults.conf`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_conf_dir = os.path.join(os.environ[\"SPARK_HOME\"], \"conf\")\n",
        "if not os.path.exists(spark_conf_dir):\n",
        "    os.makedirs(spark_conf_dir)\n",
        "print(f\"Spark configuration directory is: {os.path.join(os.environ[\"SPARK_HOME\"], \"conf\")}\")"
      ],
      "metadata": {
        "id": "_tVY3uIiqg91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f187b19d-ad0f-45fa-a671-5bd15317c4ec"
      },
      "id": "_tVY3uIiqg91",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark configuration directory is: /usr/local/lib/python3.12/dist-packages/pyspark/conf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab0f938",
      "metadata": {
        "id": "4ab0f938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47b5c810-ada8-4309-9df6-85107585213a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content successfully written to /usr/local/lib/python3.12/dist-packages/pyspark/conf/spark-defaults.conf\n"
          ]
        }
      ],
      "source": [
        "file_path = os.path.join(spark_conf_dir, 'spark-defaults.conf')\n",
        "file_content = \"\"\"\n",
        "#\n",
        "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
        "# contributor license agreements.  See the NOTICE file distributed with\n",
        "# this work for additional information regarding copyright ownership.\n",
        "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
        "# (the \"License\"); you may not use this file except in compliance with\n",
        "# the License.  You may obtain a copy of the License at\n",
        "#\n",
        "#    http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#\n",
        "\n",
        "# Default system properties included when running spark-submit.\n",
        "# This is useful for setting default environmental settings.\n",
        "\n",
        "# Example:\n",
        "# spark.master                     spark://master:7077\n",
        "# spark.eventLog.enabled           true\n",
        "# spark.eventLog.dir               hdfs://namenode:8021/directory\n",
        "# spark.serializer                 org.apache.spark.serializer.KryoSerializer\n",
        "# spark.driver.memory              5g\n",
        "# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=\\\"one two three\\\"\n",
        "\"\"\"\n",
        "\n",
        "with open(file_path, 'w') as f:\n",
        "    f.write(file_content)\n",
        "\n",
        "print(f\"Content successfully written to {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21692085",
      "metadata": {
        "id": "21692085"
      },
      "source": [
        "As you see, everything is commented out in the template file. Just uncomment and edit the properties you want to set as defaults."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc0e3e7",
      "metadata": {
        "id": "2bc0e3e7"
      },
      "source": [
        "### How to override the default configuration directory\n",
        "\n",
        "If you want to have your Spark configuration files in a directory other than `$SPARK_HOME/conf`, you can set the environment variable `SPARK_CONF_DIR`.\n",
        "\n",
        "Spark will then look in `$SPARK_CONF_DIR` for all of its configuration files: `spark-defaults.conf`, `spark-env.sh`, `log4j2.properties`, etc. (see https://spark.apache.org/docs/latest/configuration.html#overriding-configuration-directory).\n",
        "\n",
        "Here's the list of files in the default Spark configuration directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9642c7",
      "metadata": {
        "id": "ff9642c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e463d708-b3c5-4f3e-90b6-68bd999bed30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spark-defaults.conf']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "os.listdir(os.path.join(os.environ[\"SPARK_HOME\"],\"conf\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a61ed7",
      "metadata": {
        "id": "91a61ed7"
      },
      "source": [
        "But now assume that you have no `spark-defaults.conf` or did not configure Spark anywhere else. Still, Spark has some _default values_ for several properties.\n",
        "\n",
        "Where are those properties defined and how to get their default values?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd5be6f",
      "metadata": {
        "id": "5fd5be6f"
      },
      "source": [
        "#### Spark configuration properties\n",
        "\n",
        "Spark's documentation provides the list of all [available properties](https://spark.apache.org/docs/latest/configuration.html#available-properties) grouped into several categories:\n",
        "\n",
        " - [application properties](https://spark.apache.org/docs/latest/configuration.html#application-properties)\n",
        " - [runtime environment](https://spark.apache.org/docs/latest/configuration.html#runtime-environment)\n",
        " - [shuffle behavior](https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior)\n",
        " - [Spark UI](https://spark.apache.org/docs/latest/configuration.html#spark-ui)\n",
        " - [compression and serialization](https://spark.apache.org/docs/latest/configuration.html#compression-and-serialization)\n",
        " - [memory management](https://spark.apache.org/docs/latest/configuration.html#memory-management)\n",
        " - [execution behavior](https://spark.apache.org/docs/latest/configuration.html#execution-behavior)\n",
        " - [executor metrics](https://spark.apache.org/docs/latest/configuration.html#executor-metrics)\n",
        " - [networking](https://spark.apache.org/docs/latest/configuration.html#networking)\n",
        " - [scheduling](https://spark.apache.org/docs/latest/configuration.html#scheduling)\n",
        " - [barrier execution mode](https://spark.apache.org/docs/latest/configuration.html#barrier-execution-mode)\n",
        " - [dynamic allocation](https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation)\n",
        " - [thread configurations](https://spark.apache.org/docs/latest/configuration.html#thread-configurations)\n",
        " - [security](https://spark.apache.org/docs/latest/security.html)\n",
        " - [Spark SQL](https://spark.apache.org/docs/latest/configuration.html#spark-sql)\n",
        " - [Spark streaming](https://spark.apache.org/docs/latest/configuration.html#spark-streaming)\n",
        " - [SparkR](https://spark.apache.org/docs/latest/configuration.html#sparkr)\n",
        " - [GraphX](https://spark.apache.org/docs/latest/configuration.html#graphx)\n",
        " - [Deploy](https://spark.apache.org/docs/latest/configuration.html#deploy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98e1480f",
      "metadata": {
        "id": "98e1480f"
      },
      "source": [
        "All properties have a default value that should accommodate most situations.\n",
        "\n",
        "As a beginner you might want to give your application a name by configuring `spark.app.name` and perhaps change the default values of the following properties:\n",
        " - `spark.master` and `spark.submit.deployMode` to define where the application should be deployed\n",
        " - `spark.driver.memory` and `spark.driver.maxResultSize` to control the memory usage of the driver\n",
        " - `spark.executor.memory` and `spark.executor.cores` to control executors\n",
        "\n",
        "\n",
        "For instance let's create a new session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844047f9",
      "metadata": {
        "id": "844047f9"
      },
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .appName(\"my_app\") \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3c250e0",
      "metadata": {
        "id": "f3c250e0"
      },
      "source": [
        "Show properties included in the Spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f25abd1",
      "metadata": {
        "id": "9f25abd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b12e82f-1155-4fa9-f6d2-8fb6edd5683a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.rdd.compress', 'True'),\n",
              " ('spark.hadoop.fs.s3a.vectored.read.min.seek.size', '128K'),\n",
              " ('spark.app.startTime', '1771620826013'),\n",
              " ('spark.driver.memory', '2g'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
              " ('spark.sql.artifact.isolation.enabled', 'false'),\n",
              " ('spark.app.name', 'my_app'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true'),\n",
              " ('spark.hadoop.fs.s3a.vectored.read.max.merged.size', '2M'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.app.id', 'local-1771620826154'),\n",
              " ('spark.driver.host', '16d9eb535e16'),\n",
              " ('spark.driver.port', '43049'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.app.submitTime', '1771620803951'),\n",
              " ('spark.ui.showConsoleProgress', 'true')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "spark.sparkContext.getConf().getAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}