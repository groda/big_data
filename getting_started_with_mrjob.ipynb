{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groda/big_data/blob/master/getting_started_with_mrjob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7044cbb1-b8f1-46f2-96de-7a63db0374e0",
      "metadata": {
        "id": "7044cbb1-b8f1-46f2-96de-7a63db0374e0"
      },
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\" alt=\"Logo Big Data for Beginners\"></div></a>\n",
        "<h1>\n",
        "  Getting started with mrjob\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9fc7231-9597-4ca9-9130-0fa95cdc9f76",
      "metadata": {
        "id": "c9fc7231-9597-4ca9-9130-0fa95cdc9f76"
      },
      "source": [
        "# üöÄ Meet `mrjob`: Your Friendly Python MapReduce Library\n",
        "\n",
        "[`mrjob`](https://mrjob.readthedocs.io/en/latest/)  is a powerful yet easy-to-use Python library that lets you write and run Hadoop streaming jobs with pure Python. Whether you're crunching logs, processing large datasets, or experimenting with big data workflows, `mrjob` helps you build scalable MapReduce jobs without diving into Java or complex Hadoop configurations.\n",
        "\n",
        "It works locally, on your own Hadoop cluster, or in the cloud via Amazon EMR ‚Äî no fuss. With `mrjob`, you can focus on your logic and let the library handle the messy bits of distributed computing.\n",
        "\n",
        "- ‚úÖ Write MapReduce jobs in Python\n",
        "- üåç Run them locally or at scale (Hadoop/EMR)\n",
        "- üõ†Ô∏è Easily test and debug your code\n",
        "- ‚òÅÔ∏è Seamless integration with AWS\n",
        "\n",
        "Think of it as a Swiss army knife for data processing: sharp, flexible, and ready for anything.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e19ea380-3529-4ee5-b60a-dfbfa62878e2",
      "metadata": {
        "tags": [],
        "id": "e19ea380-3529-4ee5-b60a-dfbfa62878e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72921cf-6387-426b-9557-5cf4fef09b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc3aaef-cdea-43f4-9bc6-7521c41ae9ee",
      "metadata": {
        "id": "9bc3aaef-cdea-43f4-9bc6-7521c41ae9ee"
      },
      "source": [
        "# üëã Hello, MapReduce! ‚Äî A Friendly `mrjob` Demo\n",
        "\n",
        "This simple `mrjob` script, called `MRHello`, is the MapReduce equivalent of a \"Hello, World!\" ‚Äî but with a twist! When run, it acts like a distributed greeting card that tells you:\n",
        "\n",
        "- üêç Which Python version it's using  \n",
        "- üñ•Ô∏è The hostname of the machine running each map task  \n",
        "- üåç And, of course, it says \"Hello, World!\"\n",
        "\n",
        "By emitting these values from the **mapper**, this job showcases how easy it is to run Python code across multiple machines using `mrjob`. It's perfect for getting started and verifying that your Hadoop or EMR setup is working correctly ‚Äî with a cheerful greeting built in!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install `mrjob`"
      ],
      "metadata": {
        "id": "P9j2kGghCRD5"
      },
      "id": "P9j2kGghCRD5"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uY-Ttg8CUjJ",
        "outputId": "4f1bcc5b-0f82-4612-801f-e37b0c9e6493"
      },
      "id": "2uY-Ttg8CUjJ",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.12/dist-packages (from mrjob) (6.0.3)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAxcYnN6REt3",
        "outputId": "e4ef2af4-d120-4aa3-d7f2-721eff9e5734"
      },
      "id": "VAxcYnN6REt3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: mrjob\n",
            "Version: 0.7.4\n",
            "Summary: Python MapReduce framework\n",
            "Home-page: http://github.com/Yelp/mrjob\n",
            "Author: David Marin\n",
            "Author-email: dm@davidmarin.org\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: PyYAML\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a `mrjob` script"
      ],
      "metadata": {
        "id": "tjFYxF_LCYgA"
      },
      "id": "tjFYxF_LCYgA"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3d1707de-185e-49cb-9919-0a4159c6597f",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d1707de-185e-49cb-9919-0a4159c6597f",
        "outputId": "de2f03cf-6707-49ae-eb19-d8ffb6494a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mr_hello.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mr_hello.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "import sys\n",
        "import socket\n",
        "import os\n",
        "\n",
        "class MRHello(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        yield \"Python version\", sys.version\n",
        "        yield \"Hostname\", socket.gethostname()\n",
        "        yield \"HOME\", os.getenv(\"HOME\")\n",
        "        yield \"USERNAME\", os.getenv(\"USER\")\n",
        "        yield \"Output\", \"Hello, World!\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRHello.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the job\n",
        "\n",
        "The standard way to run a job would be\n",
        "\n",
        "```\n",
        "python mr_hello.py input_file.txt\n",
        "```\n",
        "\n",
        "as suggested in the [Quickstart documentation](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#writing-your-first-job).\n",
        "\n",
        "Since our \"Hello, World!\" job is just outputting a string and not processing any input we're just going to use a [_here string_](https://askubuntu.com/a/678919)."
      ],
      "metadata": {
        "id": "Hh7DX_o-C152"
      },
      "id": "Hh7DX_o-C152"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b39e21ce-20b1-4052-a021-1f08f72d7a62",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b39e21ce-20b1-4052-a021-1f08f72d7a62",
        "outputId": "d262b5b9-d0ab-456b-a5d7-28f2e0b733a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/mr_hello.root.20251010.190232.280879\n",
            "Running step 1 of 1...\n",
            "reading from STDIN\n",
            "job output is in /tmp/mr_hello.root.20251010.190232.280879/output\n",
            "Streaming final output from /tmp/mr_hello.root.20251010.190232.280879/output...\n",
            "\"Python version\"\t\"3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\"\n",
            "\"Hostname\"\t\"87c92d6b4b45\"\n",
            "\"HOME\"\t\"/root\"\n",
            "\"USERNAME\"\tnull\n",
            "\"Output\"\t\"Hello, World!\"\n",
            "Removing temp directory /tmp/mr_hello.root.20251010.190232.280879...\n"
          ]
        }
      ],
      "source": [
        "!python mr_hello.py <<<\"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write the output to a folder (let us call it `out`)"
      ],
      "metadata": {
        "id": "3xfndzADExIV"
      },
      "id": "3xfndzADExIV"
    },
    {
      "cell_type": "code",
      "source": [
        "!python mr_hello.py --output-dir out <<<\"test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw7PiUCCExTl",
        "outputId": "fa1f41a2-4e3d-4ec5-8856-be86c9d75207"
      },
      "id": "Nw7PiUCCExTl",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Running step 1 of 1...\n",
            "Creating temp directory /tmp/mr_hello.root.20251010.190232.788120\n",
            "reading from STDIN\n",
            "job output is in out\n",
            "Removing temp directory /tmp/mr_hello.root.20251010.190232.788120...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the contents of `out`"
      ],
      "metadata": {
        "id": "7NoYN-0rE_4o"
      },
      "id": "7NoYN-0rE_4o"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kywbRgiEz3C",
        "outputId": "2eab6cb7-05cd-4ef6-8b7b-9a7473569103"
      },
      "id": "9kywbRgiEz3C",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat out/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUum9kcRFDGw",
        "outputId": "b583b7b4-3fe3-4037-eb00-e31e76b88862"
      },
      "id": "fUum9kcRFDGw",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Python version\"\t\"3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\"\n",
            "\"Hostname\"\t\"87c92d6b4b45\"\n",
            "\"HOME\"\t\"/root\"\n",
            "\"USERNAME\"\tnull\n",
            "\"Output\"\t\"Hello, World!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd605e3-e646-4a6c-ba7a-7195a6691906",
      "metadata": {
        "id": "4bd605e3-e646-4a6c-ba7a-7195a6691906"
      },
      "source": [
        "## Another example: wordcount\n",
        "\n",
        "Here's another example, the classic \"wordcount\". The source of the following code is https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#writing-your-first-job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cc54d254-abd4-41fe-b37c-c0b561422c40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc54d254-abd4-41fe-b37c-c0b561422c40",
        "outputId": "bac396d2-3c15-4f7c-ab94-d6f7c9a2b1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mr_wordcount.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mr_wordcount.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "class MRWordFrequencyCount(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        yield \"chars\", len(line)\n",
        "        yield \"words\", len(line.split())\n",
        "        yield \"lines\", 1\n",
        "\n",
        "    def reducer(self, key, values):\n",
        "        yield key, sum(values)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRWordFrequencyCount.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need an input file. Let us download \"Alice's Adventures in Wonderland\" from Project Gutenberg to `input.txt`."
      ],
      "metadata": {
        "id": "yz1qq38eGkKs"
      },
      "id": "yz1qq38eGkKs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57ec11b0",
        "outputId": "ad33aa98-5e0f-4899-d3b9-0518bf7bee64"
      },
      "source": [
        "!wget --no-clobber https://www.gutenberg.org/files/11/11-0.txt -O input.txt"
      ],
      "id": "57ec11b0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-10 19:02:33--  https://www.gutenberg.org/files/11/11-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 151191 (148K) [text/plain]\n",
            "Saving to: ‚Äòinput.txt‚Äô\n",
            "\n",
            "input.txt           100%[===================>] 147.65K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-10-10 19:02:33 (1.96 MB/s) - ‚Äòinput.txt‚Äô saved [151191/151191]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute word frequencies."
      ],
      "metadata": {
        "id": "OjZC_n3XGv7f"
      },
      "id": "OjZC_n3XGv7f"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "54071cbd-09eb-4eab-b42f-6e393028a5ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54071cbd-09eb-4eab-b42f-6e393028a5ef",
        "outputId": "a72dcb50-aa49-4a5d-f6c8-98ceb94fc0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Running step 1 of 1...\n",
            "Creating temp directory /tmp/mr_wordcount.root.20251010.190234.316778\n",
            "job output is in out_wordcount\n",
            "Removing temp directory /tmp/mr_wordcount.root.20251010.190234.316778...\n"
          ]
        }
      ],
      "source": [
        "!python mr_wordcount.py --output-dir out_wordcount input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls out_wordcount"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fgd7qKNHCH6",
        "outputId": "ebe57a76-23b5-4e8b-ceac-e6429f535779"
      },
      "id": "_Fgd7qKNHCH6",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  part-00001\tpart-00002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get this output:\n",
        "\n",
        "```\n",
        "\"chars\"\t141312\n",
        "\"lines\"\t3384\n",
        "\"words\"\t26543\n",
        "```"
      ],
      "metadata": {
        "id": "F9IYrsY6N7cQ"
      },
      "id": "F9IYrsY6N7cQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat out_wordcount/part-*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L8s3WfT2k3W",
        "outputId": "dcbdcb81-c15a-4cd5-d3de-54323e3935f0"
      },
      "id": "6L8s3WfT2k3W",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"chars\"\t141312\n",
            "\"lines\"\t3384\n",
            "\"words\"\t26543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch job on a Hadoop cluster\n",
        "\n",
        "All jobs until now ran on the locally as simple Python processes. If you have a Hadoop cluster you can launch the job on the cluster leveraging distributed computing (see [running your job different ways](https://mrjob.readthedocs.io/en/latest/guides/quickstart.html#running-your-job-different-ways) from mrjob's quickstard guide)."
      ],
      "metadata": {
        "id": "p6SrORPuJDnk"
      },
      "id": "p6SrORPuJDnk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to first start a Hadoop cluster usig BigTop.\n",
        "\n",
        "### Why BigTop `3.3.0`?\n",
        "\n",
        "Note that as of 30/5/2025 BigTop's stable version is `3.4.0`. But this version only supports Ubuntu `24.04` while Google Colaboratory still runs on Ubuntu `22.04`. The last BigTop version supporting Ubuntu `22.04` is `3.3.0` from 20/6/2024.\n",
        "\n",
        "You can find a list of all archived BigTop distributions at https://archive.apache.org/dist/bigtop/ while current releases can be found at https://downloads.apache.org/."
      ],
      "metadata": {
        "id": "kizgiEc8J-5X"
      },
      "id": "kizgiEc8J-5X"
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNGJeuJkO64A",
        "outputId": "3e10b8db-adbe-445c-cd4c-4b80369c7d61"
      },
      "id": "XNGJeuJkO64A",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Bigtop packages"
      ],
      "metadata": {
        "id": "WGzj5PAcWTTQ"
      },
      "id": "WGzj5PAcWTTQ"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Add Bigtop repository\n",
        "echo \"Adding Bigtop repository...\"\n",
        "curl -o /etc/apt/sources.list.d/bigtop-3.2.1.list https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/$(lsb_release -is | tr '[:upper:]' '[:lower:]')-$(lsb_release -rs)/bigtop.list\n",
        "\n",
        "# Download and add the Bigtop GPG key\n",
        "echo \"Adding Bigtop GPG key...\"\n",
        "wget --no-clobber -qO - https://archive.apache.org/dist/bigtop/bigtop-3.3.0/repos/GPG-KEY-bigtop | sudo apt-key add -\n",
        "\n",
        "# Update package cache\n",
        "echo \"Updating package cache...\"\n",
        "apt update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMmZm-xTMUcL",
        "outputId": "710d8a3c-a1f5-4cd2-b9af-0081de029f9f"
      },
      "id": "zMmZm-xTMUcL",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding Bigtop repository...\n",
            "Adding Bigtop GPG key...\n",
            "OK\n",
            "Updating package cache...\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop InRelease [2,502 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 https://cli.github.com/packages stable InRelease\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 Packages [18.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,077 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,813 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,276 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,751 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,353 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,425 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,584 kB]\n",
            "Fetched 24.7 MB in 9s (2,761 kB/s)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100    86  100    86    0     0    159      0 --:--:-- --:--:-- --:--:--   159\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "W: http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64/dists/bigtop/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo 'List all available packages that match \"bigtop\"'\n",
        "apt search bigtop\n",
        "\n",
        "echo 'List all available packages that match \"hadoop\"'\n",
        "apt search hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwR1g-TrRLas",
        "outputId": "41cd07ad-c9e6-44fd-95cb-983c60a96ce3"
      },
      "id": "OwR1g-TrRLas",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all available packages that match \"bigtop\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "bigtop-groovy/stable 2.5.4-1 all\n",
            "  An agile and dynamic language for the Java Virtual Machine\n",
            "\n",
            "bigtop-jsvc/stable 1.2.4-1 amd64\n",
            "  Application to launch java daemon\n",
            "\n",
            "bigtop-utils/stable 3.3.0-1 all\n",
            "  Collection of useful tools for Bigtop\n",
            "\n",
            "List all available packages that match \"hadoop\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "hadoop/stable 3.3.6-1 amd64\n",
            "  Hadoop is a software platform for processing vast amounts of data\n",
            "\n",
            "hadoop-client/stable 3.3.6-1 amd64\n",
            "  Hadoop client side dependencies\n",
            "\n",
            "hadoop-conf-pseudo/stable 3.3.6-1 amd64\n",
            "  Pseudo-distributed Hadoop configuration\n",
            "\n",
            "hadoop-doc/stable 3.3.6-1 all\n",
            "  Hadoop Documentation\n",
            "\n",
            "hadoop-hdfs/stable 3.3.6-1 amd64\n",
            "  The Hadoop Distributed File System\n",
            "\n",
            "hadoop-hdfs-datanode/stable 3.3.6-1 amd64\n",
            "  Hadoop Data Node\n",
            "\n",
            "hadoop-hdfs-dfsrouter/stable 3.3.6-1 amd64\n",
            "  Hadoop HDFS Router\n",
            "\n",
            "hadoop-hdfs-fuse/stable 3.3.6-1 amd64\n",
            "  Mountable HDFS\n",
            "\n",
            "hadoop-hdfs-journalnode/stable 3.3.6-1 amd64\n",
            "  Hadoop HDFS JournalNode\n",
            "\n",
            "hadoop-hdfs-namenode/stable 3.3.6-1 amd64\n",
            "  The Hadoop namenode manages the block locations of HDFS files\n",
            "\n",
            "hadoop-hdfs-secondarynamenode/stable 3.3.6-1 amd64\n",
            "  Hadoop Secondary namenode\n",
            "\n",
            "hadoop-hdfs-zkfc/stable 3.3.6-1 amd64\n",
            "  Hadoop HDFS failover controller\n",
            "\n",
            "hadoop-httpfs/stable 3.3.6-1 amd64\n",
            "  HTTPFS for Hadoop\n",
            "\n",
            "hadoop-kms/stable 3.3.6-1 amd64\n",
            "  KMS for Hadoop\n",
            "\n",
            "hadoop-mapreduce/stable 3.3.6-1 amd64\n",
            "  The Hadoop MapReduce (MRv2)\n",
            "\n",
            "hadoop-mapreduce-historyserver/stable 3.3.6-1 amd64\n",
            "  MapReduce History Server\n",
            "\n",
            "hadoop-yarn/stable 3.3.6-1 amd64\n",
            "  The Hadoop NextGen MapReduce (YARN)\n",
            "\n",
            "hadoop-yarn-nodemanager/stable 3.3.6-1 amd64\n",
            "  YARN Node Manager\n",
            "\n",
            "hadoop-yarn-proxyserver/stable 3.3.6-1 amd64\n",
            "  YARN Web Proxy\n",
            "\n",
            "hadoop-yarn-resourcemanager/stable 3.3.6-1 amd64\n",
            "  YARN Resource Manager\n",
            "\n",
            "hadoop-yarn-router/stable 3.3.6-1 amd64\n",
            "  YARN Router Server\n",
            "\n",
            "hadoop-yarn-timelineserver/stable 3.3.6-1 amd64\n",
            "  YARN Timeline Server\n",
            "\n",
            "hbase/stable 2.4.17-1 amd64\n",
            "  HBase is the Hadoop database. Use it when you need random, realtime read/write access to your Big Data. This project's goal is the hosting of very large tables -- billions of rows X millions of columns -- atop clusters of commodity hardware.\n",
            "\n",
            "hbase-master/stable 2.4.17-1 all\n",
            "  The Hadoop HBase master Server.\n",
            "\n",
            "hbase-regionserver/stable 2.4.17-1 all\n",
            "  The Hadoop HBase RegionServer server.\n",
            "\n",
            "hbase-thrift/stable 2.4.17-1 all\n",
            "  The Hadoop HBase Thrift Interface\n",
            "\n",
            "hive/stable 3.1.3-1 all\n",
            "  Data warehouse infrastructure built on top of Hadoop\n",
            "\n",
            "hive-hcatalog/stable 3.1.3-1 all\n",
            "  Table and storage management service for Apache Hadoop.\n",
            "\n",
            "hive-webhcat/stable 3.1.3-1 all\n",
            "  A REST-like web API for HCatalog and related Hadoop components.\n",
            "\n",
            "libhdfs0/stable 3.3.6-1 amd64\n",
            "  Hadoop Filesystem Library\n",
            "\n",
            "libhdfspp/stable 3.3.6-1 amd64\n",
            "  Hadoop Filesystem Library for C++\n",
            "\n",
            "livy/stable 0.8.0-1 all\n",
            "  Livy is an open source REST interface for interacting with Apache Spark from anywhere.\n",
            "\n",
            "python3-sahara-plugin-vanilla/jammy 7.0.0-0ubuntu1 all\n",
            "  OpenStack data processing cluster as a service - Vanilla/Hadoop plugin\n",
            "\n",
            "r-cran-hive/jammy 0.2-2-1.ca2204.1 all\n",
            "  CRAN Package 'hive' (Hadoop InteractiVE)\n",
            "\n",
            "r-cran-implyr/jammy 0.5.0-1.ca2204.1 all\n",
            "  CRAN Package 'implyr' (R Interface for Apache Impala)\n",
            "\n",
            "r-cran-paws.analytics/jammy 0.9.0-1.ca2204.1 all\n",
            "  CRAN Package 'paws.analytics' ('Amazon Web Services' Analytics Services)\n",
            "\n",
            "r-cran-pmmltransformations/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'pmmlTransformations' (Transforms Input Data from a PMML Perspective)\n",
            "\n",
            "r-cran-storm/jammy 1.2-1.ca2204.1 all\n",
            "  CRAN Package 'Storm' (Write Storm Bolts in R using the Storm Multi-Language Protocol)\n",
            "\n",
            "ranger/stable 2.4.0-1 all\n",
            "  Apache Ranger is a framework to enable,\n",
            "\n",
            "tez/stable 0.10.2-1 all\n",
            "  Framework for building complex DAG of tasks.\n",
            "\n",
            "zookeeper-server/stable 3.7.2-1 all\n",
            "  The Hadoop Zookeeper server\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to install the Bigtop packages needed for running:\n",
        "\n",
        "- HDFS\n",
        "  - `hadoop-hdfs-namenode`\n",
        "  - `hadoop-hdfs-datanode`\n",
        "- MapReduce\n",
        "  - `hadoop-mapreduce`\n",
        "- YARN\n",
        "  - `hadoop-yarn`\n",
        "  - `hadoop-yarn-nodemanager`\n",
        "  - `hadoop-yarn-resourcemanager`\n",
        "\n",
        "All these service are going to run on a single machine (the local host)."
      ],
      "metadata": {
        "id": "xUAgcEhYPm5b"
      },
      "id": "xUAgcEhYPm5b"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y hadoop-mapreduce hadoop-hdfs-namenode hadoop-hdfs-datanode \\\n",
        "                hadoop-yarn hadoop-yarn-nodemanager hadoop-yarn-resourcemanager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPanUJrlUigS",
        "outputId": "bdc54d07-2c88-4646-b854-bdb45f4b7447"
      },
      "id": "XPanUJrlUigS",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-hdfs netcat-openbsd\n",
            "  zookeeper\n",
            "The following NEW packages will be installed:\n",
            "  bigtop-groovy bigtop-jsvc bigtop-utils hadoop hadoop-hdfs\n",
            "  hadoop-hdfs-datanode hadoop-hdfs-namenode hadoop-mapreduce hadoop-yarn\n",
            "  hadoop-yarn-nodemanager hadoop-yarn-resourcemanager netcat-openbsd zookeeper\n",
            "0 upgraded, 13 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 469 MB of archives.\n",
            "After this operation, 609 MB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-utils all 3.3.0-1 [5,422 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netcat-openbsd amd64 1.218-4ubuntu1 [39.4 kB]\n",
            "Get:3 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-groovy all 2.5.4-1 [4,832 kB]\n",
            "Get:4 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 bigtop-jsvc amd64 1.2.4-1 [29.0 kB]\n",
            "Get:5 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 zookeeper all 3.7.2-1 [21.2 MB]\n",
            "Get:6 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop amd64 3.3.6-1 [328 MB]\n",
            "Get:7 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-hdfs amd64 3.3.6-1 [58.3 MB]\n",
            "Get:8 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-hdfs-datanode amd64 3.3.6-1 [5,346 B]\n",
            "Get:9 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-hdfs-namenode amd64 3.3.6-1 [5,400 B]\n",
            "Get:10 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-yarn amd64 3.3.6-1 [51.8 MB]\n",
            "Get:11 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-mapreduce amd64 3.3.6-1 [4,895 kB]\n",
            "Get:12 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-yarn-nodemanager amd64 3.3.6-1 [5,276 B]\n",
            "Get:13 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-yarn-resourcemanager amd64 3.3.6-1 [5,200 B]\n",
            "Fetched 469 MB in 15s (32.2 MB/s)\n",
            "Selecting previously unselected package netcat-openbsd.\n",
            "(Reading database ... 126675 files and directories currently installed.)\n",
            "Preparing to unpack .../00-netcat-openbsd_1.218-4ubuntu1_amd64.deb ...\n",
            "Unpacking netcat-openbsd (1.218-4ubuntu1) ...\n",
            "Selecting previously unselected package bigtop-utils.\n",
            "Preparing to unpack .../01-bigtop-utils_3.3.0-1_all.deb ...\n",
            "Unpacking bigtop-utils (3.3.0-1) ...\n",
            "Selecting previously unselected package bigtop-groovy.\n",
            "Preparing to unpack .../02-bigtop-groovy_2.5.4-1_all.deb ...\n",
            "Unpacking bigtop-groovy (2.5.4-1) ...\n",
            "Selecting previously unselected package bigtop-jsvc.\n",
            "Preparing to unpack .../03-bigtop-jsvc_1.2.4-1_amd64.deb ...\n",
            "Unpacking bigtop-jsvc (1.2.4-1) ...\n",
            "Selecting previously unselected package zookeeper.\n",
            "Preparing to unpack .../04-zookeeper_3.7.2-1_all.deb ...\n",
            "Unpacking zookeeper (3.7.2-1) ...\n",
            "Selecting previously unselected package hadoop.\n",
            "Preparing to unpack .../05-hadoop_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-hdfs.\n",
            "Preparing to unpack .../06-hadoop-hdfs_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-hdfs (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-hdfs-datanode.\n",
            "Preparing to unpack .../07-hadoop-hdfs-datanode_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-hdfs-datanode (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-hdfs-namenode.\n",
            "Preparing to unpack .../08-hadoop-hdfs-namenode_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-hdfs-namenode (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-yarn.\n",
            "Preparing to unpack .../09-hadoop-yarn_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-yarn (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-mapreduce.\n",
            "Preparing to unpack .../10-hadoop-mapreduce_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-mapreduce (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-yarn-nodemanager.\n",
            "Preparing to unpack .../11-hadoop-yarn-nodemanager_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-yarn-nodemanager (3.3.6-1) ...\n",
            "Selecting previously unselected package hadoop-yarn-resourcemanager.\n",
            "Preparing to unpack .../12-hadoop-yarn-resourcemanager_3.3.6-1_amd64.deb ...\n",
            "Unpacking hadoop-yarn-resourcemanager (3.3.6-1) ...\n",
            "Setting up netcat-openbsd (1.218-4ubuntu1) ...\n",
            "update-alternatives: using /bin/nc.openbsd to provide /bin/nc (nc) in auto mode\n",
            "Setting up bigtop-utils (3.3.0-1) ...\n",
            "Setting up zookeeper (3.7.2-1) ...\n",
            "update-alternatives: using /etc/zookeeper/conf.dist to provide /etc/zookeeper/conf (zookeeper-conf) in auto mode\n",
            "Setting up bigtop-groovy (2.5.4-1) ...\n",
            "Setting up hadoop (3.3.6-1) ...\n",
            "update-alternatives: using /etc/hadoop/conf.empty to provide /etc/hadoop/conf (hadoop-conf) in auto mode\n",
            "Setting up bigtop-jsvc (1.2.4-1) ...\n",
            "Setting up hadoop-yarn (3.3.6-1) ...\n",
            "Setting up hadoop-hdfs (3.3.6-1) ...\n",
            "Setting up hadoop-hdfs-namenode (3.3.6-1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up hadoop-yarn-resourcemanager (3.3.6-1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up hadoop-mapreduce (3.3.6-1) ...\n",
            "Setting up hadoop-yarn-nodemanager (3.3.6-1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up hadoop-hdfs-datanode (3.3.6-1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minimal configuration\n",
        "\n",
        "following \"Yarn on a Single Node\" ([https://hadoop.apache.org/docs/stable/.../SingleCluster.html#YARN_on_a_Single_Node](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node)).\n"
      ],
      "metadata": {
        "id": "FChSV4PYMhVr"
      },
      "id": "FChSV4PYMhVr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us set the default filesystem in Hadoop's configuration file `core-site.xml`."
      ],
      "metadata": {
        "id": "ZQtg56phX_Xj"
      },
      "id": "ZQtg56phX_Xj"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cat > /etc/hadoop/conf/core-site.xml << ‚¨ÖÔ∏è\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "‚¨ÖÔ∏è"
      ],
      "metadata": {
        "id": "fjiSjI7wLwOp"
      },
      "id": "fjiSjI7wLwOp",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure `mapred-site.xml` and `yarn-site.xml`."
      ],
      "metadata": {
        "id": "Tp1dJyRaRqcG"
      },
      "id": "Tp1dJyRaRqcG"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cat > /etc/hadoop/conf/mapred-site.xml << ‚¨ÖÔ∏è\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>mapreduce.framework.name</name>\n",
        "        <value>yarn</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>mapreduce.application.classpath</name>\n",
        "        <value>/usr/lib/hadoop-mapreduce:/usr/lib/hadoop:/usr/lib/hadoop/tools/lib:/usr/lib/hadoop/*</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "‚¨ÖÔ∏è"
      ],
      "metadata": {
        "id": "Ws2plAj9ckZx"
      },
      "id": "Ws2plAj9ckZx",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not sure if specifying `yarn.application.classpath` is needed in `yarn-site.xml`."
      ],
      "metadata": {
        "id": "NF3LpQE3VhrW"
      },
      "id": "NF3LpQE3VhrW"
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop classpath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zR9nrFCO9Is",
        "outputId": "e078b82c-98df-4d2a-ebdc-1ef865078d15"
      },
      "id": "_zR9nrFCO9Is",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/etc/hadoop/conf:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/.//*:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/.//*:/usr/lib/hadoop-mapreduce/.//*:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/.//*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "cat > /etc/hadoop/conf/yarn-site.xml << ‚¨ÖÔ∏è\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>yarn.nodemanager.aux-services</name>\n",
        "        <value>mapreduce_shuffle</value>\n",
        "    </property>\n",
        "    <property>\n",
        "    <name>yarn.application.classpath</name>\n",
        "       <value>/etc/hadoop/conf:/usr/lib/hadoop/lib/*:/usr/lib/hadoop/.//*:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/*:/usr/lib/hadoop-hdfs/.//*:/usr/lib/hadoop-mapreduce/.//*:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/*:/usr/lib/hadoop-yarn/.//*</value>\n",
        "       <description>output of hadoop classpath</description>\n",
        "   </property>\n",
        "</configuration>\n",
        "‚¨ÖÔ∏è"
      ],
      "metadata": {
        "id": "8qTQxLlQIpGu"
      },
      "id": "8qTQxLlQIpGu",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format HDFS"
      ],
      "metadata": {
        "id": "YaXRvzhAWsoy"
      },
      "id": "YaXRvzhAWsoy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the Hadoop Filesystem (HDFS)."
      ],
      "metadata": {
        "id": "gMK8tkAfSOIG"
      },
      "id": "gMK8tkAfSOIG"
    },
    {
      "cell_type": "code",
      "source": [
        "# erase the HDFS filesystem in case it already exists\n",
        "!rm -rf /tmp/hadoop-hdfs/ /tmp/hadoop-root/ 2>/dev/null\n",
        "\n",
        "# initialize the namenode\n",
        "!sudo -u hdfs hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKXUS-0ic-az",
        "outputId": "ba36b7c9-a581-4b80-e82c-658f66f51d7b"
      },
      "id": "pKXUS-0ic-az",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 19:03:32,214 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 87c92d6b4b45/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.3.6\n",
            "STARTUP_MSG:   classpath = /etc/hadoop/conf:/usr/lib/hadoop/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-9.8.1.jar:/usr/lib/hadoop/lib/jersey-server-1.19.4.jar:/usr/lib/hadoop/lib/netty-transport-4.1.89.Final.jar:/usr/lib/hadoop/lib/gson-2.9.0.jar:/usr/lib/hadoop/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/lib/hadoop/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/lib/hadoop/lib/jetty-util-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/jetty-io-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar:/usr/lib/hadoop/lib/snappy-java-1.1.8.2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.36.jar:/usr/lib/hadoop/lib/commons-compress-1.21.jar:/usr/lib/hadoop/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/lib/hadoop/lib/netty-codec-http2-4.1.89.Final.jar:/usr/lib/hadoop/lib/jetty-server-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/reload4j-1.2.22.jar:/usr/lib/hadoop/lib/netty-codec-http-4.1.89.Final.jar:/usr/lib/hadoop/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop/lib/netty-transport-udt-4.1.89.Final.jar:/usr/lib/hadoop/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/lib/hadoop/lib/jsch-0.1.55.jar:/usr/lib/hadoop/lib/guava-27.0-jre.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jackson-annotations-2.12.7.jar:/usr/lib/hadoop/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/lib/hadoop/lib/curator-client-5.2.0.jar:/usr/lib/hadoop/lib/failureaccess-1.0.jar:/usr/lib/hadoop/lib/jetty-security-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop/lib/re2j-1.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.13.jar:/usr/lib/hadoop/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/lib/hadoop/lib/commons-configuration2-2.8.0.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jetty-http-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/lib/hadoop/lib/netty-codec-socks-4.1.89.Final.jar:/usr/lib/hadoop/lib/netty-common-4.1.89.Final.jar:/usr/lib/hadoop/lib/jettison-1.5.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.2.jar:/usr/lib/hadoop/lib/commons-text-1.10.0.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/hadoop-auth-3.3.6.jar:/usr/lib/hadoop/lib/curator-framework-5.2.0.jar:/usr/lib/hadoop/lib/animal-sniffer-annotations-1.17.jar:/usr/lib/hadoop/lib/jersey-core-1.19.4.jar:/usr/lib/hadoop/lib/netty-buffer-4.1.89.Final.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop/lib/hadoop-annotations-3.3.6.jar:/usr/lib/hadoop/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop/lib/netty-resolver-4.1.89.Final.jar:/usr/lib/hadoop/lib/zookeeper-3.7.2.jar:/usr/lib/hadoop/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/netty-all-4.1.89.Final.jar:/usr/lib/hadoop/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/lib/hadoop/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/checker-qual-2.5.2.jar:/usr/lib/hadoop/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.20.jar:/usr/lib/hadoop/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/lib/hadoop/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop/lib/commons-io-2.8.0.jar:/usr/lib/hadoop/lib/hadoop-shaded-guava-1.1.1.jar:/usr/lib/hadoop/lib/netty-handler-4.1.89.Final.jar:/usr/lib/hadoop/lib/commons-lang3-3.12.0.jar:/usr/lib/hadoop/lib/netty-codec-4.1.89.Final.jar:/usr/lib/hadoop/lib/jetty-xml-9.4.51.v20230217.jar:/usr/lib/hadoop/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/lib/hadoop/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop/lib/commons-codec-1.15.jar:/usr/lib/hadoop/lib/curator-recipes-5.2.0.jar:/usr/lib/hadoop/lib/netty-codec-dns-4.1.89.Final.jar:/usr/lib/hadoop/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop/lib/token-provider-1.0.1.jar:/usr/lib/hadoop/lib/netty-codec-xml-4.1.89.Final.jar:/usr/lib/hadoop/lib/jackson-core-2.12.7.jar:/usr/lib/hadoop/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/lib/hadoop/lib/httpcore-4.4.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.12.0.jar:/usr/lib/hadoop/lib/jul-to-slf4j-1.7.36.jar:/usr/lib/hadoop/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/lib/hadoop/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/lib/hadoop/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/lib/hadoop/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/lib/hadoop/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop/lib/netty-codec-redis-4.1.89.Final.jar:/usr/lib/hadoop/lib/jersey-servlet-1.19.4.jar:/usr/lib/hadoop/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/lib/hadoop/lib/jackson-databind-2.12.7.1.jar:/usr/lib/hadoop/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop/lib/j2objc-annotations-1.1.jar:/usr/lib/hadoop/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop/lib/zookeeper-jute-3.7.2.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop/lib/commons-net-3.9.0.jar:/usr/lib/hadoop/.//hadoop-registry.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-nfs-3.3.6.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-kms.jar:/usr/lib/hadoop/.//hadoop-kms-3.3.6.jar:/usr/lib/hadoop/.//hadoop-registry-3.3.6.jar:/usr/lib/hadoop/.//hadoop-common-3.3.6-tests.jar:/usr/lib/hadoop/.//hadoop-common-3.3.6.jar:/usr/lib/hadoop/.//hadoop-auth-3.3.6.jar:/usr/lib/hadoop/.//hadoop-annotations-3.3.6.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/kerb-common-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.19.4.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/gson-2.9.0.jar:/usr/lib/hadoop-hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/jetty-io-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/kerby-asn1-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/lib/hadoop-hdfs/lib/snappy-java-1.1.8.2.jar:/usr/lib/hadoop-hdfs/lib/commons-compress-1.21.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-server-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/reload4j-1.2.22.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-http-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jsch-0.1.55.jar:/usr/lib/hadoop-hdfs/lib/guava-27.0-jre.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.12.7.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/curator-client-5.2.0.jar:/usr/lib/hadoop-hdfs/lib/failureaccess-1.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-security-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/lib/hadoop-hdfs/lib/re2j-1.1.jar:/usr/lib/hadoop-hdfs/lib/httpclient-4.5.13.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-hdfs/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-hdfs/lib/avro-1.7.7.jar:/usr/lib/hadoop-hdfs/lib/kotlin-stdlib-1.4.10.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-configuration2-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-hdfs/lib/jetty-http-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-common-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jettison-1.5.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.2.jar:/usr/lib/hadoop-hdfs/lib/commons-text-1.10.0.jar:/usr/lib/hadoop-hdfs/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/usr/lib/hadoop-hdfs/lib/hadoop-auth-3.3.6.jar:/usr/lib/hadoop-hdfs/lib/curator-framework-5.2.0.jar:/usr/lib/hadoop-hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.19.4.jar:/usr/lib/hadoop-hdfs/lib/netty-buffer-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-hdfs/lib/kerb-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/hadoop-annotations-3.3.6.jar:/usr/lib/hadoop-hdfs/lib/kerb-crypto-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-3.7.2.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/woodstox-core-5.4.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/checker-qual-2.5.2.jar:/usr/lib/hadoop-hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/kerb-identity-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-json-1.20.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-admin-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-lang3-3.12.0.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/usr/lib/hadoop-hdfs/lib/dnsjava-2.1.7.jar:/usr/lib/hadoop-hdfs/lib/jaxb-api-2.2.11.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/kerby-util-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.15.jar:/usr/lib/hadoop-hdfs/lib/curator-recipes-5.2.0.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/stax2-api-4.2.1.jar:/usr/lib/hadoop-hdfs/lib/token-provider-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.12.7.jar:/usr/lib/hadoop-hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/paranamer-2.3.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/okhttp-4.9.3.jar:/usr/lib/hadoop-hdfs/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-hdfs/lib/httpcore-4.4.13.jar:/usr/lib/hadoop-hdfs/lib/audience-annotations-0.12.0.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/lib/hadoop-hdfs/lib/json-simple-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/usr/lib/hadoop-hdfs/lib/kerb-client-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/kerb-core-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/usr/lib/hadoop-hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-servlet-1.19.4.jar:/usr/lib/hadoop-hdfs/lib/kerby-config-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-pkix-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/j2objc-annotations-1.1.jar:/usr/lib/hadoop-hdfs/lib/kerby-xdr-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/metrics-core-3.2.4.jar:/usr/lib/hadoop-hdfs/lib/zookeeper-jute-3.7.2.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/kerb-server-1.0.1.jar:/usr/lib/hadoop-hdfs/lib/okio-2.8.0.jar:/usr/lib/hadoop-hdfs/lib/commons-net-3.9.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-httpfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.3.6-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-3.3.6.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-uploader-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-nativetask-3.3.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-yarn/./:/usr/lib/hadoop-yarn/lib/jna-5.2.0.jar:/usr/lib/hadoop-yarn/lib/snakeyaml-2.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.19.4.jar:/usr/lib/hadoop-yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/guice-4.0.jar:/usr/lib/hadoop-yarn/lib/asm-tree-9.4.jar:/usr/lib/hadoop-yarn/lib/websocket-server-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/websocket-api-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.19.4.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-plus-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/swagger-annotations-1.5.4.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/websocket-common-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/websocket-client-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/lib/hadoop-yarn/lib/jline-3.9.0.jar:/usr/lib/hadoop-yarn/lib/bcpkix-jdk15on-1.68.jar:/usr/lib/hadoop-yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/javax.websocket-api-1.0.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/objenesis-2.6.jar:/usr/lib/hadoop-yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/bcprov-jdk15on-1.68.jar:/usr/lib/hadoop-yarn/lib/jetty-client-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/asm-commons-9.4.jar:/usr/lib/hadoop-yarn/lib/javax.websocket-client-api-1.0.jar:/usr/lib/hadoop-yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-4.0.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-api-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-mawo-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-services-core.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-mawo-core-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-3.3.6.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/bigtop.git -r 6478bc3006ba881a109ca55df3c27488c523eae8; compiled by 'jenkins' on 2024-05-28T18:11Z\n",
            "STARTUP_MSG:   java = 11.0.28\n",
            "************************************************************/\n",
            "2025-10-10 19:03:32,310 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2025-10-10 19:03:32,500 INFO namenode.NameNode: createNameNode [-format]\n",
            "2025-10-10 19:03:33,671 INFO namenode.NameNode: Formatting using clusterid: CID-f7cccf46-d13b-41f4-a484-20ec244bd868\n",
            "2025-10-10 19:03:33,726 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2025-10-10 19:03:33,799 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2025-10-10 19:03:33,804 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2025-10-10 19:03:33,804 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2025-10-10 19:03:33,857 INFO namenode.FSNamesystem: fsOwner                = hdfs (auth:SIMPLE)\n",
            "2025-10-10 19:03:33,858 INFO namenode.FSNamesystem: supergroup             = supergroup\n",
            "2025-10-10 19:03:33,858 INFO namenode.FSNamesystem: isPermissionEnabled    = true\n",
            "2025-10-10 19:03:33,858 INFO namenode.FSNamesystem: isStoragePolicyEnabled = true\n",
            "2025-10-10 19:03:33,859 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2025-10-10 19:03:33,943 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2025-10-10 19:03:34,153 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit : configured=1000, counted=60, effected=1000\n",
            "2025-10-10 19:03:34,153 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2025-10-10 19:03:34,158 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2025-10-10 19:03:34,159 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Oct 10 19:03:34\n",
            "2025-10-10 19:03:34,161 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2025-10-10 19:03:34,161 INFO util.GSet: VM type       = 64-bit\n",
            "2025-10-10 19:03:34,163 INFO util.GSet: 2.0% max memory 3.2 GB = 64.9 MB\n",
            "2025-10-10 19:03:34,163 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2025-10-10 19:03:34,191 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2025-10-10 19:03:34,191 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2025-10-10 19:03:34,199 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.999\n",
            "2025-10-10 19:03:34,199 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2025-10-10 19:03:34,199 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2025-10-10 19:03:34,200 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2025-10-10 19:03:34,201 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2025-10-10 19:03:34,261 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2025-10-10 19:03:34,261 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2025-10-10 19:03:34,261 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2025-10-10 19:03:34,261 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2025-10-10 19:03:34,289 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2025-10-10 19:03:34,289 INFO util.GSet: VM type       = 64-bit\n",
            "2025-10-10 19:03:34,290 INFO util.GSet: 1.0% max memory 3.2 GB = 32.4 MB\n",
            "2025-10-10 19:03:34,290 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2025-10-10 19:03:34,302 INFO namenode.FSDirectory: ACLs enabled? true\n",
            "2025-10-10 19:03:34,303 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2025-10-10 19:03:34,303 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2025-10-10 19:03:34,303 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2025-10-10 19:03:34,312 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2025-10-10 19:03:34,315 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2025-10-10 19:03:34,322 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2025-10-10 19:03:34,323 INFO util.GSet: VM type       = 64-bit\n",
            "2025-10-10 19:03:34,323 INFO util.GSet: 0.25% max memory 3.2 GB = 8.1 MB\n",
            "2025-10-10 19:03:34,323 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2025-10-10 19:03:34,345 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2025-10-10 19:03:34,345 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2025-10-10 19:03:34,345 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2025-10-10 19:03:34,356 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2025-10-10 19:03:34,356 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2025-10-10 19:03:34,358 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2025-10-10 19:03:34,359 INFO util.GSet: VM type       = 64-bit\n",
            "2025-10-10 19:03:34,359 INFO util.GSet: 0.029999999329447746% max memory 3.2 GB = 996.6 KB\n",
            "2025-10-10 19:03:34,359 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2025-10-10 19:03:34,404 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1591793333-172.28.0.12-1760123014388\n",
            "2025-10-10 19:03:34,486 INFO common.Storage: Storage directory /tmp/hadoop-hdfs/dfs/name has been successfully formatted.\n",
            "2025-10-10 19:03:34,595 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2025-10-10 19:03:34,880 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hdfs/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2025-10-10 19:03:34,907 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2025-10-10 19:03:34,993 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2025-10-10 19:03:34,994 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2025-10-10 19:03:35,002 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2025-10-10 19:03:35,003 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 87c92d6b4b45/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Hadoop services\n",
        "\n",
        "Start the HDFS Namenode and Datanode services."
      ],
      "metadata": {
        "id": "uc0BE9GkSbdj"
      },
      "id": "uc0BE9GkSbdj"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "service hadoop-hdfs-namenode start\n",
        "service hadoop-hdfs-datanode start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-muvlAQMCOC",
        "outputId": "7000233b-5aae-4fbc-e7e4-72c574322114"
      },
      "id": "U-muvlAQMCOC",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Started Hadoop namenode: \n",
            " * Started Hadoop datanode (hadoop-hdfs-datanode): \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Use of this script to start HDFS daemons is deprecated.\n",
            "WARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\n",
            "WARNING: Use of this script to start HDFS daemons is deprecated.\n",
            "WARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify that they are up and running, you should see in the output:\n",
        "\n",
        "```\n",
        "3664 NameNode\n",
        "3835 DataNode\n",
        "```"
      ],
      "metadata": {
        "id": "SM2PFbUGTNqd"
      },
      "id": "SM2PFbUGTNqd"
    },
    {
      "cell_type": "code",
      "source": [
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFEuIQ2aMGug",
        "outputId": "43393e2a-d9d6-432b-a0cd-dd486f4cffe0"
      },
      "id": "KFEuIQ2aMGug",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2641 DataNode\n",
            "2471 NameNode\n",
            "2744 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also verify the status of the Hadoop filesystem with `hdfs dfsadmin -report`.\n",
        "\n",
        "Note that `hdfs` is the _superuser_ for HDFS (as the account that formatted the NameNode), so we need to run this command as `hdfs` and not as `root`."
      ],
      "metadata": {
        "id": "zca5olsaTeS6"
      },
      "id": "zca5olsaTeS6"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY-mRTv2LYuw",
        "outputId": "6915120f-8eab-4d64-c06a-94fefff7ef38"
      },
      "id": "ZY-mRTv2LYuw",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 72659013632 (67.67 GB)\n",
            "DFS Remaining: 72658989056 (67.67 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: 87c92d6b4b45\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 42982400000 (40.03 GB)\n",
            "DFS Remaining: 72658989056 (67.67 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 62.82%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 0\n",
            "Last contact: Fri Oct 10 19:03:50 UTC 2025\n",
            "Last Block Report: Fri Oct 10 19:03:47 UTC 2025\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the YARN services (node manager and resource manager)."
      ],
      "metadata": {
        "id": "wZrASMl3U6Q9"
      },
      "id": "wZrASMl3U6Q9"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "service hadoop-yarn-nodemanager start\n",
        "service hadoop-yarn-resourcemanager start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWWgE6DYmbaP",
        "outputId": "13b5774f-1b9e-4027-c613-104d15c837af"
      },
      "id": "RWWgE6DYmbaP",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Started Hadoop nodemanager: \n",
            " * Started Hadoop resourcemanager: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Use of this script to start YARN daemons is deprecated.\n",
            "WARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\n",
            "WARNING: Use of this script to start YARN daemons is deprecated.\n",
            "WARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgkkXYwBe4vd",
        "outputId": "0d8a9fc3-8b1b-448f-a574-7e836ba8c47b"
      },
      "id": "mgkkXYwBe4vd",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2641 DataNode\n",
            "3186 Jps\n",
            "2915 NodeManager\n",
            "3111 ResourceManager\n",
            "2471 NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link various folders for easy access (can be useful for debugging by opening the files in the left panel)."
      ],
      "metadata": {
        "id": "72q4BsK9VEbx"
      },
      "id": "72q4BsK9VEbx"
    },
    {
      "cell_type": "code",
      "source": [
        "!ln -s /var/log ./\n",
        "!ln -s /etc/hadoop ./\n",
        "!ln -s /etc .\n",
        "!ln -s /lib ."
      ],
      "metadata": {
        "id": "U8RHOB6ofpKp"
      },
      "id": "U8RHOB6ofpKp",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy input file to HDFS"
      ],
      "metadata": {
        "id": "CNcNOwRNXIw9"
      },
      "id": "CNcNOwRNXIw9"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "sudo -u hdfs hdfs dfs -mkdir -p /user/hdfs\n",
        "sudo -u hdfs hdfs dfs -chown hdfs:hdfs /user/hdfs\n",
        "sudo -u hdfs hdfs dfs -put -f input.txt hdfs:///user/hdfs/"
      ],
      "metadata": {
        "id": "ObO_R-Zmwuyw"
      },
      "id": "ObO_R-Zmwuyw",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the job on the cluster\n",
        "\n",
        "With the option `-r hadoop` the mrjob script is launched on the hadoop cluster. It is necessary to specify the location of the Hadoop streaming jar (to locate it use the command `find /usr/lib/hadoop -name \"hadoop-streaming*.jar`).\n",
        "\n",
        "With the option `-v` the job runs in verbose mode."
      ],
      "metadata": {
        "id": "Y85KkPuWXMnv"
      },
      "id": "Y85KkPuWXMnv"
    },
    {
      "cell_type": "code",
      "source": [
        "!find /usr/lib/hadoop -name \"hadoop-streaming*.jar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygz9_0K6X2jp",
        "outputId": "3eb1332e-c307-47e2-b2d0-69829ff0a7ef"
      },
      "id": "ygz9_0K6X2jp",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#export HADOOP_ROOT_LOGGER=DEBUG,console\n",
        "# Run the mrjob wordcount job using the hadoop runner\n",
        "sudo -u hdfs \\\n",
        "hdfs dfs -rm -r myOutputDir 2>/dev/null\n",
        "\n",
        "sudo -u hdfs python mr_wordcount.py -v -r hadoop \\\n",
        "    --output-dir myOutputDir \\\n",
        "    --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    hdfs:///user/hdfs/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDk27mmE2U8o",
        "outputId": "b8f19f22-d0b2-4ae8-9144-b0a05b439d38"
      },
      "id": "tDk27mmE2U8o",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "making runner: HadoopJobRunner(hadoop_streaming_jar=/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar, input_paths=['hdfs:///user/hdfs/input.txt'], mr_job_script=/content/mr_wordcount.py, output_dir=myOutputDir, stdin=<_io.BufferedReader name='<stdin>'>, steps=[{'type': 'streaming', 'mapper': {'type': 'script'}, 'reducer': {'type': 'script'}}], ...)\n",
            "Looking for configs in /var/lib/hadoop-hdfs/.mrjob.conf\n",
            "Looking for configs in /etc/mrjob.conf\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for hadoop runner\n",
            "Active configuration:\n",
            "{'bootstrap_mrjob': None,\n",
            " 'check_input_paths': True,\n",
            " 'cleanup': ['ALL'],\n",
            " 'cleanup_on_failure': ['NONE'],\n",
            " 'cmdenv': {},\n",
            " 'hadoop_bin': None,\n",
            " 'hadoop_extra_args': [],\n",
            " 'hadoop_log_dirs': [],\n",
            " 'hadoop_streaming_jar': '/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar',\n",
            " 'hadoop_tmp_dir': 'tmp/mrjob',\n",
            " 'jobconf': {},\n",
            " 'label': None,\n",
            " 'libjars': [],\n",
            " 'local_tmp_dir': None,\n",
            " 'owner': 'hdfs',\n",
            " 'py_files': [],\n",
            " 'python_bin': None,\n",
            " 'read_logs': True,\n",
            " 'setup': [],\n",
            " 'sh_bin': None,\n",
            " 'spark_args': [],\n",
            " 'spark_deploy_mode': None,\n",
            " 'spark_master': None,\n",
            " 'spark_submit_bin': None,\n",
            " 'task_python_bin': None,\n",
            " 'upload_archives': [],\n",
            " 'upload_dirs': [],\n",
            " 'upload_files': []}\n",
            "Looking for hadoop binary in $PATH...\n",
            "Found hadoop binary: /usr/bin/hadoop\n",
            "> /usr/bin/hadoop fs -ls hdfs:///user/hdfs/input.txt\n",
            "STDOUT: -rw-r--r--   3 hdfs hdfs     151191 2025-10-10 19:04 hdfs:///user/hdfs/input.txt\n",
            "> /usr/bin/hadoop version\n",
            "Using Hadoop version 3.3.6\n",
            "Creating temp directory /tmp/mr_wordcount.hdfs.20251010.190427.632669\n",
            "archiving /usr/local/lib/python3.12/dist-packages/mrjob -> /tmp/mr_wordcount.hdfs.20251010.190427.632669/mrjob.zip as mrjob/\n",
            "Writing streaming setup wrapper script to /tmp/mr_wordcount.hdfs.20251010.190427.632669/setup-wrapper.sh:\n",
            "  # store $PWD\n",
            "  __mrjob_PWD=$PWD\n",
            "  \n",
            "  # obtain exclusive file lock\n",
            "  exec 9>/tmp/wrapper.lock.mr_wordcount.hdfs.20251010.190427.632669\n",
            "  python3 -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
            "  \n",
            "  # setup commands\n",
            "  {\n",
            "    export PYTHONPATH=$__mrjob_PWD/mrjob.zip:$PYTHONPATH\n",
            "  } 0</dev/null 1>&2\n",
            "  \n",
            "  # release exclusive file lock\n",
            "  exec 9>&-\n",
            "  \n",
            "  # run task from the original working directory\n",
            "  cd $__mrjob_PWD\n",
            "  \"$@\"\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd\n",
            "uploading working dir files to hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd...\n",
            "  /content/mr_wordcount.py -> hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mr_wordcount.py\n",
            "> /usr/bin/hadoop fs -put /content/mr_wordcount.py hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mr_wordcount.py\n",
            "  /tmp/mr_wordcount.hdfs.20251010.190427.632669/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/mr_wordcount.hdfs.20251010.190427.632669/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mrjob.zip\n",
            "  /tmp/mr_wordcount.hdfs.20251010.190427.632669/setup-wrapper.sh -> hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -put /tmp/mr_wordcount.hdfs.20251010.190427.632669/setup-wrapper.sh hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/\n",
            "Copying other local files to hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/\n",
            "  /tmp/mr_wordcount.hdfs.20251010.190427.632669/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/mr_wordcount.hdfs.20251010.190427.632669/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/mrjob.zip\n",
            "Running step 1 of 1...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mr_wordcount.py#mr_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669/files/wd/setup-wrapper.sh#setup-wrapper.sh' -input hdfs:///user/hdfs/input.txt -output hdfs:///user/hdfs/myOutputDir -mapper '/bin/sh -ex setup-wrapper.sh python3 mr_wordcount.py --step-num=0 --mapper' -reducer '/bin/sh -ex setup-wrapper.sh python3 mr_wordcount.py --step-num=0 --reducer'\n",
            "  with environment: [('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python mr_wordcount.py -v -r hadoop --output-dir myOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar17500197031947422468/] [] /tmp/streamjob17359222088626623829.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0001\n",
            "  Total input files to process : 1\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0001\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0001\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0001/\n",
            "  Running job: job_1760123048431_0001\n",
            "  Job job_1760123048431_0001 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 50% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "   map 100% reduce 100%\n",
            "  Job job_1760123048431_0001 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/myOutputDir\n",
            "Counters: 54\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=155287\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=42\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=126040\n",
            "\t\tFILE: Number of bytes written=1094286\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=155473\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=42\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=25721856\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7612416\n",
            "\t\tTotal time spent by all map tasks (ms)=25119\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=25119\n",
            "\t\tTotal time spent by all reduce tasks (ms)=7434\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=7434\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=25119\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=7434\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=3860\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=288\n",
            "\t\tInput split bytes=186\n",
            "\t\tMap input records=3384\n",
            "\t\tMap output bytes=105730\n",
            "\t\tMap output materialized bytes=126046\n",
            "\t\tMap output records=10152\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tPeak Map Physical memory (bytes)=321220608\n",
            "\t\tPeak Map Virtual memory (bytes)=2720620544\n",
            "\t\tPeak Reduce Physical memory (bytes)=237522944\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2727239680\n",
            "\t\tPhysical memory (bytes) snapshot=868556800\n",
            "\t\tReduce input groups=3\n",
            "\t\tReduce input records=10152\n",
            "\t\tReduce output records=3\n",
            "\t\tReduce shuffle bytes=126046\n",
            "\t\tShuffled Maps =2\n",
            "\t\tSpilled Records=20304\n",
            "\t\tTotal committed heap usage (bytes)=898629632\n",
            "\t\tVirtual memory (bytes) snapshot=8166907904\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "job output is in hdfs:///user/hdfs/myOutputDir\n",
            "Removing HDFS temp directory hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669...\n",
            "> /usr/bin/hadoop fs -rm -R -f -skipTrash hdfs:///user/hdfs/tmp/mrjob/mr_wordcount.hdfs.20251010.190427.632669\n",
            "Removing temp directory /tmp/mr_wordcount.hdfs.20251010.190427.632669...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is in `myOutputDir` together with a `_SUCCESS` is everything went well."
      ],
      "metadata": {
        "id": "eqlihMXMZScZ"
      },
      "id": "eqlihMXMZScZ"
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIDgc-rRgOGS",
        "outputId": "52d15f5b-13a4-4895-b85c-6f6efd8cd7f2"
      },
      "id": "EIDgc-rRgOGS",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 items\n",
            "-rw-r--r--   3 hdfs hdfs     151191 2025-10-10 19:04 input.txt\n",
            "drwxr-xr-x   - hdfs hdfs          0 2025-10-10 19:05 myOutputDir\n",
            "drwxr-xr-x   - hdfs hdfs          0 2025-10-10 19:04 tmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -ls myOutputDir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMAFzN0c3483",
        "outputId": "4567f71c-0ffb-495b-91b7-dcf3ba0e02c6"
      },
      "id": "hMAFzN0c3483",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   3 hdfs hdfs          0 2025-10-10 19:05 myOutputDir/_SUCCESS\n",
            "-rw-r--r--   3 hdfs hdfs         42 2025-10-10 19:05 myOutputDir/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16528de2-3012-462f-886f-aa946538d012",
      "metadata": {
        "id": "16528de2-3012-462f-886f-aa946538d012"
      },
      "source": [
        "Verify the contents of the output file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -cat myOutputDir/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoyeMnvvGnI8",
        "outputId": "38f8b883-5edc-4a4e-e32b-8cb912e29b11"
      },
      "id": "PoyeMnvvGnI8",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"chars\"\t141312\n",
            "\"lines\"\t3384\n",
            "\"words\"\t26543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® Hybrid Word Count\n",
        "\n",
        "This Python script defines a `mrjob` job called `HybridWordCount` that performs a word count with a _hybrid_ approach, using both classic MapReduce steps and a Spark step.\n",
        "\n",
        "The method `steps(self)` defines the sequence of steps in the job:\n",
        "- `MRStep(mapper=self.mapper_get_words)`: The first step is a classic MapReduce mapper (mapper_get_words) that takes each line of input, splits it into words, converts them to lowercase, and yields each word with a count of 1 (e.g., \"hello\", 1).\n",
        "- `SparkStep(self.spark_wordcount)`: The second step is a Spark step (spark_wordcount). This step takes the output of the previous mapper as input and uses Spark's reduceByKey to efficiently sum the counts for each word. It then saves the aggregated word counts to an output location.\n",
        "- `MRStep(reducer=self.reducer_aggregate_counts)`: The third step is a classic MapReduce reducer (reducer_aggregate_counts). It receives the output from the Spark step (which might be split across multiple files). For each word, it sums the counts from all the input files for that word. It yields None as the key and a tuple of (total_count, word) as the value. Yielding None as the key sends all pairs to a single reducer in the next step.\n",
        "- `MRStep(reducer=self.reducer_find_top_n)`: The final step is another classic MapReduce reducer (reducer_find_top_n). It receives all the (count, word) pairs from the previous reducer. It sorts these pairs in descending order based on the count and then yields the top 10 words and their counts.\n",
        "\n",
        "In essence, this script first uses a classic mapper to tokenize the input, then leverages Spark for efficient word counting, and finally uses classic reducers to aggregate results and find the top words.\n",
        "\n"
      ],
      "metadata": {
        "id": "lWy6UFFIMAE1"
      },
      "id": "lWy6UFFIMAE1"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hy_wordcount.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep, SparkStep\n",
        "\n",
        "class HybridWordCount(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words),\n",
        "            SparkStep(self.spark_wordcount),\n",
        "            MRStep(reducer=self.reducer_aggregate_counts),\n",
        "            MRStep(reducer=self.reducer_find_top_n)\n",
        "        ]\n",
        "\n",
        "    # --- Step 1: Mapper (classic MR)\n",
        "    def mapper_get_words(self, _, line):\n",
        "        for word in line.strip().split():\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    # --- Step 2: SparkStep\n",
        "    def spark_wordcount(self, input_uri, output_uri):\n",
        "        \"\"\"\n",
        "        Spark job: aggregate counts per word\n",
        "        \"\"\"\n",
        "        from pyspark.sql import SparkSession\n",
        "\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        sc = spark.sparkContext\n",
        "\n",
        "        # Load key-value pairs emitted by MR step\n",
        "        rdd = sc.textFile(input_uri)\n",
        "\n",
        "        # Each line is like: \"word\\t1\"\n",
        "        def parse_line(line):\n",
        "            parts = line.split('\\t')\n",
        "            return parts[0], int(parts[1])\n",
        "\n",
        "        word_rdd = rdd.map(parse_line)\n",
        "\n",
        "        # Aggregate using Spark reduceByKey\n",
        "        counts = word_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "        # Convert to text output\n",
        "        counts.map(lambda kv: f\"{kv[0]}\\t{kv[1]}\").saveAsTextFile(output_uri)\n",
        "\n",
        "        spark.stop()\n",
        "\n",
        "    # --- Step 3: Reducer (classic MR)\n",
        "    def reducer_aggregate_counts(self, word, counts):\n",
        "        # The values yielded by the Spark step are counts for each word\n",
        "        # in different partitions. We need to sum these up.\n",
        "        total_count = sum(counts)\n",
        "        yield None, (total_count, word) # Yield None as key to group all results\n",
        "\n",
        "    def reducer_find_top_n(self, _, count_word_pairs):\n",
        "        # Receive (count, word) pairs from the previous reducer\n",
        "        # Sort and take the top N (e.g., top 20)\n",
        "        top_n = sorted(count_word_pairs, reverse=True)[:20]\n",
        "        for count, word in top_n:\n",
        "            yield word, count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    HybridWordCount.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLzgYItUMAo_",
        "outputId": "f0162d26-0e29-44d9-a63c-8f195d6ed41e"
      },
      "id": "BLzgYItUMAo_",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hy_wordcount.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run locally"
      ],
      "metadata": {
        "id": "3mM9O7k6OOxU"
      },
      "id": "3mM9O7k6OOxU"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%bash\n",
        "\n",
        "# clean output directory\n",
        "rm hyOutputDir/* 2>/dev/null\n",
        "\n",
        "python hy_wordcount.py \\\n",
        "    --output-dir hyOutputDir \\\n",
        "    input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXGBSTTF6NL",
        "outputId": "9fe5234a-4bc4-4060-a3da-b1356624a655"
      },
      "id": "hOXGBSTTF6NL",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Running step 1 of 4...\n",
            "Creating temp directory /tmp/hy_wordcount.root.20251010.190557.718609\n",
            "Running step 2 of 4...\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/10/10 19:06:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "\r[Stage 0:>                                                          (0 + 2) / 4]\r\r[Stage 0:=============================>                             (2 + 2) / 4]\r\r[Stage 1:==============>                                            (1 + 2) / 4]\r\r[Stage 1:=============================>                             (2 + 2) / 4]\r\r                                                                                \rRunning step 3 of 4...\n",
            "Running step 4 of 4...\n",
            "job output is in hyOutputDir\n",
            "Removing temp directory /tmp/hy_wordcount.root.20251010.190557.718609...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.11 ms, sys: 300 ¬µs, total: 4.41 ms\n",
            "Wall time: 17.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l hyOutputDir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJKafhw1HRsF",
        "outputId": "c7556478-1493-47d5-e590-0a2162585c80"
      },
      "id": "JJKafhw1HRsF",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 196 Oct 10 19:06 part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat hyOutputDir/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPWlOGQgOZDQ",
        "outputId": "2d610adb-705c-4b12-873b-8e27a29bfddb"
      },
      "id": "KPWlOGQgOZDQ",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"the\"\t1614\n",
            "\"and\"\t767\n",
            "\"to\"\t706\n",
            "\"a\"\t619\n",
            "\"she\"\t518\n",
            "\"of\"\t496\n",
            "\"said\"\t420\n",
            "\"it\"\t362\n",
            "\"in\"\t351\n",
            "\"was\"\t328\n",
            "\"you\"\t257\n",
            "\"i\"\t249\n",
            "\"as\"\t249\n",
            "\"alice\"\t221\n",
            "\"that\"\t216\n",
            "\"her\"\t207\n",
            "\"at\"\t204\n",
            "\"had\"\t176\n",
            "\"with\"\t170\n",
            "\"all\"\t154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on the cluster\n",
        "\n",
        "We are going to launch the same job but this time on the cluster through YARN."
      ],
      "metadata": {
        "id": "_5MMN9UcOVu0"
      },
      "id": "_5MMN9UcOVu0"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%bash\n",
        "sudo -u hdfs \\\n",
        "hdfs dfs -rm -r hyOutputDir 2>/dev/null\n",
        "\n",
        "# Copy the script to a location accessible by the hdfs user\n",
        "cp /content/hy_wordcount.py /tmp/hy_wordcount.py\n",
        "\n",
        "# Run the mrjob wordcount job using the hadoop runner with HADOOP_CONF_DIR set for the hdfs user\n",
        "sudo -u hdfs env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop \\\n",
        "    --output-dir hyOutputDir \\\n",
        "    --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar \\\n",
        "    hdfs:///user/hdfs/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6tXU1dRNt5s",
        "outputId": "38a02bfc-64f4-4cb9-fd64-d4f6803df951"
      },
      "id": "g6tXU1dRNt5s",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "making runner: HadoopJobRunner(hadoop_streaming_jar=/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar, input_paths=['hdfs:///user/hdfs/input.txt'], mr_job_script=/tmp/hy_wordcount.py, output_dir=hyOutputDir, stdin=<_io.BufferedReader name='<stdin>'>, steps=[{'type': 'streaming', 'mapper': {'type': 'script'}}, {'jobconf': {}, 'spark_args': [], 'type': 'spark'}, {'type': 'streaming', 'reducer': {'type': 'script'}}, {'type': 'streaming', 'reducer': {'type': 'script'}}], ...)\n",
            "Looking for configs in /var/lib/hadoop-hdfs/.mrjob.conf\n",
            "Looking for configs in /etc/mrjob.conf\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for hadoop runner\n",
            "Active configuration:\n",
            "{'bootstrap_mrjob': None,\n",
            " 'check_input_paths': True,\n",
            " 'cleanup': ['ALL'],\n",
            " 'cleanup_on_failure': ['NONE'],\n",
            " 'cmdenv': {},\n",
            " 'hadoop_bin': None,\n",
            " 'hadoop_extra_args': [],\n",
            " 'hadoop_log_dirs': [],\n",
            " 'hadoop_streaming_jar': '/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar',\n",
            " 'hadoop_tmp_dir': 'tmp/mrjob',\n",
            " 'jobconf': {},\n",
            " 'label': None,\n",
            " 'libjars': [],\n",
            " 'local_tmp_dir': None,\n",
            " 'owner': 'hdfs',\n",
            " 'py_files': [],\n",
            " 'python_bin': None,\n",
            " 'read_logs': True,\n",
            " 'setup': [],\n",
            " 'sh_bin': None,\n",
            " 'spark_args': [],\n",
            " 'spark_deploy_mode': None,\n",
            " 'spark_master': None,\n",
            " 'spark_submit_bin': None,\n",
            " 'task_python_bin': None,\n",
            " 'upload_archives': [],\n",
            " 'upload_dirs': [],\n",
            " 'upload_files': []}\n",
            "Looking for hadoop binary in $PATH...\n",
            "Found hadoop binary: /usr/bin/hadoop\n",
            "> /usr/bin/hadoop fs -ls hdfs:///user/hdfs/input.txt\n",
            "STDOUT: -rw-r--r--   3 hdfs hdfs     151191 2025-10-10 19:04 hdfs:///user/hdfs/input.txt\n",
            "> /usr/bin/hadoop version\n",
            "Using Hadoop version 3.3.6\n",
            "Looking for spark-submit binary in $PATH...\n",
            "Found spark-submit binary: /usr/local/bin/spark-submit\n",
            "Creating temp directory /tmp/hy_wordcount.hdfs.20251010.190619.485219\n",
            "archiving /usr/local/lib/python3.12/dist-packages/mrjob -> /tmp/hy_wordcount.hdfs.20251010.190619.485219/mrjob.zip as mrjob/\n",
            "Writing streaming setup wrapper script to /tmp/hy_wordcount.hdfs.20251010.190619.485219/setup-wrapper.sh:\n",
            "  # store $PWD\n",
            "  __mrjob_PWD=$PWD\n",
            "  \n",
            "  # obtain exclusive file lock\n",
            "  exec 9>/tmp/wrapper.lock.hy_wordcount.hdfs.20251010.190619.485219\n",
            "  python3 -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
            "  \n",
            "  # setup commands\n",
            "  {\n",
            "    export PYTHONPATH=$__mrjob_PWD/mrjob.zip:$PYTHONPATH\n",
            "  } 0</dev/null 1>&2\n",
            "  \n",
            "  # release exclusive file lock\n",
            "  exec 9>&-\n",
            "  \n",
            "  # run task from the original working directory\n",
            "  cd $__mrjob_PWD\n",
            "  \"$@\"\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd\n",
            "uploading working dir files to hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd...\n",
            "  /tmp/hy_wordcount.py -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount.py hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py\n",
            "  /tmp/hy_wordcount.hdfs.20251010.190619.485219/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount.hdfs.20251010.190619.485219/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip\n",
            "  /tmp/hy_wordcount.hdfs.20251010.190619.485219/setup-wrapper.sh -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount.hdfs.20251010.190619.485219/setup-wrapper.sh hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/\n",
            "Copying other local files to hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/\n",
            "  /tmp/hy_wordcount.hdfs.20251010.190619.485219/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount.hdfs.20251010.190619.485219/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/mrjob.zip\n",
            "Running step 1 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py#hy_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh#setup-wrapper.sh' -D mapreduce.job.reduces=0 -input hdfs:///user/hdfs/input.txt -output hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0000 -mapper '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount.py --step-num=0 --mapper'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/bin/env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop --output-dir hyOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar13893741162304980828/] [] /tmp/streamjob8576497833958191313.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0002\n",
            "  Total input files to process : 1\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0002\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0002\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0002/\n",
            "  Running job: job_1760123048431_0002\n",
            "  Job job_1760123048431_0002 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 50% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "  Job job_1760123048431_0002 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0000\n",
            "Counters: 33\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=155287\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=265634\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=0\n",
            "\t\tFILE: Number of bytes written=560698\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=155473\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=265634\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=14\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=25437184\n",
            "\t\tTotal time spent by all map tasks (ms)=24841\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=24841\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=24841\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=2370\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=153\n",
            "\t\tInput split bytes=186\n",
            "\t\tMap input records=3384\n",
            "\t\tMap output records=26543\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tPeak Map Physical memory (bytes)=233246720\n",
            "\t\tPeak Map Virtual memory (bytes)=2723844096\n",
            "\t\tPhysical memory (bytes) snapshot=464093184\n",
            "\t\tSpilled Records=0\n",
            "\t\tTotal committed heap usage (bytes)=591396864\n",
            "\t\tVirtual memory (bytes) snapshot=5444644864\n",
            "Running step 2 of 4...\n",
            "> /usr/local/bin/spark-submit --conf spark.executorEnv.PYSPARK_PYTHON=python3 --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --master yarn --deploy-mode client --files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh --py-files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/mrjob.zip /tmp/hy_wordcount.py --step-num=1 --spark hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0000 hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0001\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', 'python3'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/bin/env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop --output-dir hyOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "> /usr/local/bin/spark-submit --conf spark.executorEnv.PYSPARK_PYTHON=python3 --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --master yarn --deploy-mode client --files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh --py-files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/mrjob.zip /tmp/hy_wordcount.py --step-num=1 --spark hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0000 hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0001\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', 'python3'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/bin/env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop --output-dir hyOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking spark-submit via PTY\n",
            "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "  Running Spark version 3.5.1\n",
            "  OS info Linux, 6.6.97+, amd64\n",
            "  Java version 11.0.28\n",
            "  ==============================================================\n",
            "  No custom resources configured for spark.driver.\n",
            "  ==============================================================\n",
            "  Submitted application: hy_wordcount.py\n",
            "  Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "  Limiting resource is cpus at 1 tasks per executor\n",
            "  Added ResourceProfile id: 0\n",
            "  Changing view acls to: hdfs\n",
            "  Changing modify acls to: hdfs\n",
            "  Changing view acls groups to: \n",
            "  Changing modify acls groups to: \n",
            "  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n",
            "  Successfully started service 'sparkDriver' on port 40041.\n",
            "  Registering MapOutputTracker\n",
            "  Registering BlockManagerMaster\n",
            "  Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "  BlockManagerMasterEndpoint up\n",
            "  Registering BlockManagerMasterHeartbeat\n",
            "  Created local directory at /tmp/blockmgr-0ac7575c-648d-47b7-9352-486b6a386e06\n",
            "  MemoryStore started with capacity 434.4 MiB\n",
            "  Registering OutputCommitCoordinator\n",
            "  Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "  Successfully started service 'SparkUI' on port 4040.\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
            "  Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
            "  Setting up container launch context for our AM\n",
            "  Setting up the launch environment for our AM container\n",
            "  Preparing resources for our AM container\n",
            "  Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
            "  Uploading resource file:/tmp/spark-32b16261-47fc-40ca-9d0c-7273b84abbd8/__spark_libs__12079501346082130907.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0003/__spark_libs__12079501346082130907.zip\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh\n",
            "  Uploading resource file:/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0003/pyspark.zip\n",
            "  Uploading resource file:/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0003/py4j-0.10.9.7-src.zip\n",
            "  Same name resource hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/mrjob.zip added multiple times to distributed cache\n",
            "  Uploading resource file:/tmp/spark-32b16261-47fc-40ca-9d0c-7273b84abbd8/__spark_conf__13617813326820281444.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0003/__spark_conf__.zip\n",
            "  Changing view acls to: hdfs\n",
            "  Changing modify acls to: hdfs\n",
            "  Changing view acls groups to: \n",
            "  Changing modify acls groups to: \n",
            "  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n",
            "  Submitting application application_1760123048431_0003 to ResourceManager\n",
            "  Submitted application application_1760123048431_0003\n",
            "  Application report for application_1760123048431_0003 (state: ACCEPTED)\n",
            "  \n",
            "\t client token: N/A\n",
            "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
            "\t ApplicationMaster host: N/A\n",
            "  -1\n",
            "\t queue: default\n",
            "\t start time: 1760123262150\n",
            "\t final status: UNDEFINED\n",
            "\t tracking URL: http://87c92d6b4b45:8088/proxy/application_1760123048431_0003/\n",
            "\t user: hdfs\n",
            "  Application report for application_1760123048431_0003 (state: RUNNING)\n",
            "  \n",
            "\t client token: N/A\n",
            "\t diagnostics: N/A\n",
            "\t ApplicationMaster host: 172.28.0.12\n",
            "  -1\n",
            "\t queue: default\n",
            "\t start time: 1760123262150\n",
            "\t final status: UNDEFINED\n",
            "\t tracking URL: http://87c92d6b4b45:8088/proxy/application_1760123048431_0003/\n",
            "\t user: hdfs\n",
            "  Application application_1760123048431_0003 has started running.\n",
            "  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36723.\n",
            "  Server created on 87c92d6b4b45:36723\n",
            "  Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "  Registering BlockManager BlockManagerId(driver, 87c92d6b4b45, 36723, None)\n",
            "  Registering block manager 87c92d6b4b45:36723 with 434.4 MiB RAM, BlockManagerId(driver, 87c92d6b4b45, 36723, None)\n",
            "  Registered BlockManager BlockManagerId(driver, 87c92d6b4b45, 36723, None)\n",
            "  Initialized BlockManager: BlockManagerId(driver, 87c92d6b4b45, 36723, None)\n",
            "  Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> 87c92d6b4b45, PROXY_URI_BASES -> http://87c92d6b4b45:8088/proxy/application_1760123048431_0003), /proxy/application_1760123048431_0003\n",
            "  Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
            "  SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
            "  Block broadcast_0 stored as values in memory (estimated size 221.6 KiB, free 434.2 MiB)\n",
            "  Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 434.2 MiB)\n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:36723 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "  Total input files to process : 2\n",
            "  mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "  Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "  File Output Committer Algorithm version is 1\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "  Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "  Registering RDD 3 (reduceByKey at /tmp/hy_wordcount.py:41) as input to shuffle 0\n",
            "  Got job 0 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
            "  Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
            "  Parents of final stage: List(ShuffleMapStage 0)\n",
            "  Missing parents: List(ShuffleMapStage 0)\n",
            "  Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/hy_wordcount.py:41), which has no missing parents\n",
            "  Block broadcast_1 stored as values in memory (estimated size 13.6 KiB, free 434.1 MiB)\n",
            "  Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.1 MiB)\n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:36723 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "  Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/hy_wordcount.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "  Adding task set 0.0 with 2 tasks resource profile 0\n",
            "  Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:44134) with ID 1,  ResourceProfileId 0\n",
            "  Registering block manager 87c92d6b4b45:36523 with 434.4 MiB RAM, BlockManagerId(1, 87c92d6b4b45, 36523, None)\n",
            "  Starting task 0.0 in stage 0.0 (TID 0) (87c92d6b4b45, executor 1, partition 0, NODE_LOCAL, 7741 bytes) \n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:36523 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:43432) with ID 2,  ResourceProfileId 0\n",
            "  Registering block manager 87c92d6b4b45:42469 with 434.4 MiB RAM, BlockManagerId(2, 87c92d6b4b45, 42469, None)\n",
            "  Starting task 1.0 in stage 0.0 (TID 1) (87c92d6b4b45, executor 2, partition 1, NODE_LOCAL, 7741 bytes) \n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:36523 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:42469 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:42469 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Finished task 0.0 in stage 0.0 (TID 0) in 10523 ms on 87c92d6b4b45 (executor 1) (1/2)\n",
            "  Connected to AccumulatorServer at host: 127.0.0.1 port: 37007\n",
            "  Finished task 1.0 in stage 0.0 (TID 1) in 9411 ms on 87c92d6b4b45 (executor 2) (2/2)\n",
            "  Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "  ShuffleMapStage 0 (reduceByKey at /tmp/hy_wordcount.py:41) finished in 13.457 s\n",
            "  looking for newly runnable stages\n",
            "  running: Set()\n",
            "  waiting: Set(ResultStage 1)\n",
            "  failed: Set()\n",
            "  Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "  Block broadcast_2 stored as values in memory (estimated size 110.4 KiB, free 434.0 MiB)\n",
            "  Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.1 KiB, free 434.0 MiB)\n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:36723 (size: 42.1 KiB, free: 434.3 MiB)\n",
            "  Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "  Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "  Adding task set 1.0 with 2 tasks resource profile 0\n",
            "  Starting task 0.0 in stage 1.0 (TID 2) (87c92d6b4b45, executor 1, partition 0, NODE_LOCAL, 7444 bytes) \n",
            "  Starting task 1.0 in stage 1.0 (TID 3) (87c92d6b4b45, executor 2, partition 1, NODE_LOCAL, 7444 bytes) \n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:36523 (size: 42.1 KiB, free: 434.3 MiB)\n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:42469 (size: 42.1 KiB, free: 434.3 MiB)\n",
            "  Asked to send map output locations for shuffle 0 to 172.28.0.12:43432\n",
            "  Asked to send map output locations for shuffle 0 to 172.28.0.12:44134\n",
            "  Finished task 0.0 in stage 1.0 (TID 2) in 1908 ms on 87c92d6b4b45 (executor 1) (1/2)\n",
            "  Finished task 1.0 in stage 1.0 (TID 3) in 1908 ms on 87c92d6b4b45 (executor 2) (2/2)\n",
            "  Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "  ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 1.965 s\n",
            "  Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "  Killing all running tasks in stage 1: Stage finished\n",
            "  Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 15.952852 s\n",
            "  Start to commit write Job job_202510101908091297897920972161998_0008.\n",
            "  Write Job job_202510101908091297897920972161998_0008 committed. Elapsed time: 90 ms.\n",
            "  SparkContext is stopping with exitCode 0.\n",
            "  Stopped Spark web UI at http://87c92d6b4b45:4040\n",
            "  Interrupting monitor thread\n",
            "  Shutting down all executors\n",
            "  Asking each executor to shut down\n",
            "  YARN client scheduler backend Stopped\n",
            "  MapOutputTrackerMasterEndpoint stopped!\n",
            "  MemoryStore cleared\n",
            "  BlockManager stopped\n",
            "  BlockManagerMaster stopped\n",
            "  OutputCommitCoordinator stopped!\n",
            "  Successfully stopped SparkContext\n",
            "  Shutdown hook called\n",
            "  Deleting directory /tmp/spark-073940aa-bb82-42b5-a296-56fb718e0da4\n",
            "  Deleting directory /tmp/spark-32b16261-47fc-40ca-9d0c-7273b84abbd8\n",
            "  Deleting directory /tmp/spark-32b16261-47fc-40ca-9d0c-7273b84abbd8/pyspark-aa55998b-fa16-445e-9f18-276329673198\n",
            "Running step 3 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py#hy_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh#setup-wrapper.sh' -input hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0001 -output hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0002 -mapper cat -reducer '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount.py --step-num=2 --reducer'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/bin/env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop --output-dir hyOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar6123821324325566370/] [] /tmp/streamjob10152134356090108992.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0004\n",
            "  Total input files to process : 2\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0004\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0004\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0004/\n",
            "  Running job: job_1760123048431_0004\n",
            "  Job job_1760123048431_0004 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "   map 100% reduce 100%\n",
            "  Job job_1760123048431_0004 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0002\n",
            "Counters: 54\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=66732\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=107012\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=76808\n",
            "\t\tFILE: Number of bytes written=995940\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=67056\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=107012\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=23300096\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7573504\n",
            "\t\tTotal time spent by all map tasks (ms)=22754\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=22754\n",
            "\t\tTotal time spent by all reduce tasks (ms)=7396\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=7396\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=22754\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=7396\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=3820\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=288\n",
            "\t\tInput split bytes=324\n",
            "\t\tMap input records=5035\n",
            "\t\tMap output bytes=66732\n",
            "\t\tMap output materialized bytes=76814\n",
            "\t\tMap output records=5035\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tPeak Map Physical memory (bytes)=327340032\n",
            "\t\tPeak Map Virtual memory (bytes)=2720354304\n",
            "\t\tPeak Reduce Physical memory (bytes)=230449152\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2731880448\n",
            "\t\tPhysical memory (bytes) snapshot=870887424\n",
            "\t\tReduce input groups=5035\n",
            "\t\tReduce input records=5035\n",
            "\t\tReduce output records=5035\n",
            "\t\tReduce shuffle bytes=76814\n",
            "\t\tShuffled Maps =2\n",
            "\t\tSpilled Records=10070\n",
            "\t\tTotal committed heap usage (bytes)=838860800\n",
            "\t\tVirtual memory (bytes) snapshot=8171671552\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "Running step 4 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/hy_wordcount.py#hy_wordcount.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/files/wd/setup-wrapper.sh#setup-wrapper.sh' -input hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219/step-output/0002 -output hdfs:///user/hdfs/hyOutputDir -mapper cat -reducer '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount.py --step-num=3 --reducer'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/bin/env HADOOP_CONF_DIR=/etc/hadoop/conf python /tmp/hy_wordcount.py -v -r hadoop --output-dir hyOutputDir --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar8610157534822769967/] [] /tmp/streamjob4468937332045203600.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0005\n",
            "  Total input files to process : 1\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0005\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0005\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0005/\n",
            "  Running job: job_1760123048431_0005\n",
            "  Job job_1760123048431_0005 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "   map 100% reduce 100%\n",
            "  Job job_1760123048431_0005 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/hyOutputDir\n",
            "Counters: 54\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=111108\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=196\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=117088\n",
            "\t\tFILE: Number of bytes written=1076317\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=111432\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=196\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=24059904\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7206912\n",
            "\t\tTotal time spent by all map tasks (ms)=23496\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=23496\n",
            "\t\tTotal time spent by all reduce tasks (ms)=7038\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=7038\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=23496\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=7038\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=3570\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=328\n",
            "\t\tInput split bytes=324\n",
            "\t\tMap input records=5035\n",
            "\t\tMap output bytes=107012\n",
            "\t\tMap output materialized bytes=117094\n",
            "\t\tMap output records=5035\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tPeak Map Physical memory (bytes)=335306752\n",
            "\t\tPeak Map Virtual memory (bytes)=2728157184\n",
            "\t\tPeak Reduce Physical memory (bytes)=213741568\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2728374272\n",
            "\t\tPhysical memory (bytes) snapshot=852684800\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce input records=5035\n",
            "\t\tReduce output records=20\n",
            "\t\tReduce shuffle bytes=117094\n",
            "\t\tShuffled Maps =2\n",
            "\t\tSpilled Records=10070\n",
            "\t\tTotal committed heap usage (bytes)=838860800\n",
            "\t\tVirtual memory (bytes) snapshot=8177082368\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "job output is in hdfs:///user/hdfs/hyOutputDir\n",
            "Removing HDFS temp directory hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219...\n",
            "> /usr/bin/hadoop fs -rm -R -f -skipTrash hdfs:///user/hdfs/tmp/mrjob/hy_wordcount.hdfs.20251010.190619.485219\n",
            "Removing temp directory /tmp/hy_wordcount.hdfs.20251010.190619.485219...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22.2 ms, sys: 9 ms, total: 31.2 ms\n",
            "Wall time: 3min 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -ls hyOutputDir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF-ltZUdMA9u",
        "outputId": "a45db816-c3a8-4732-fe77-88ca401b640e"
      },
      "id": "bF-ltZUdMA9u",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   3 hdfs hdfs          0 2025-10-10 19:09 hyOutputDir/_SUCCESS\n",
            "-rw-r--r--   3 hdfs hdfs        196 2025-10-10 19:09 hyOutputDir/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -cat hyOutputDir/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4Yoz7qqFQdE",
        "outputId": "c8e7941a-06e6-47fc-ccd1-109c3bd0dd38"
      },
      "id": "C4Yoz7qqFQdE",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"the\"\t1614\n",
            "\"and\"\t767\n",
            "\"to\"\t706\n",
            "\"a\"\t619\n",
            "\"she\"\t518\n",
            "\"of\"\t496\n",
            "\"said\"\t420\n",
            "\"it\"\t362\n",
            "\"in\"\t351\n",
            "\"was\"\t328\n",
            "\"you\"\t257\n",
            "\"i\"\t249\n",
            "\"as\"\t249\n",
            "\"alice\"\t221\n",
            "\"that\"\t216\n",
            "\"her\"\t207\n",
            "\"at\"\t204\n",
            "\"had\"\t176\n",
            "\"with\"\t170\n",
            "\"all\"\t154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, this worked. But the job didn't _quite_ run on the cluster. The MapReduce steps did, but the Spark step ran locally (with the Spark that comes with PySpark).\n",
        "\n",
        "In order to run also the Spark step on the cluster we need to install Spark. So, first of all, let us install Spark from the Bigtop distribution.\n"
      ],
      "metadata": {
        "id": "oqmu2OPwHVkL"
      },
      "id": "oqmu2OPwHVkL"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo 'List all available packages that match \"spark\"'\n",
        "apt search  spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjZQNrvpSYp6",
        "outputId": "10efd25b-b2c3-4a33-dd40-fdcd9887b1d3"
      },
      "id": "ZjZQNrvpSYp6",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all available packages that match \"spark\"\n",
            "Sorting...\n",
            "Full Text Search...\n",
            "alluxio/stable 2.9.3-1 all\n",
            "  Reliable file sharing at memory speed across cluster frameworks\n",
            "\n",
            "libjs-jquery.sparkline/jammy 2.1.2-3 all\n",
            "  library for jQuery to generate sparklines\n",
            "\n",
            "libsparkline-php/jammy 0.2-7 all\n",
            "  sparkline graphing library for php\n",
            "\n",
            "livy/stable 0.8.0-1 all\n",
            "  Livy is an open source REST interface for interacting with Apache Spark from anywhere.\n",
            "\n",
            "node-sparkles/jammy 1.0.1-2 all\n",
            "  Namespaced global event emitter\n",
            "\n",
            "nspark/jammy 1.7.8B2+git20210317.cb30779-2 amd64\n",
            "  Unarchiver for Spark and ArcFS files\n",
            "\n",
            "pcp-export-pcp2spark/jammy 5.3.6-1build1 amd64\n",
            "  Tool for exporting data from PCP to Apache Spark\n",
            "\n",
            "python3-sahara-plugin-spark/jammy 7.0.0-0ubuntu1 all\n",
            "  OpenStack data processing cluster as a service - Spark plugin\n",
            "\n",
            "python3-sparkpost/jammy 1.3.10-1 all\n",
            "  SparkPost Python API client (Python 3)\n",
            "\n",
            "r-cran-analysispipelines/jammy 1.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'analysisPipelines' (Compose Interoperable Analysis Pipelines & Put Them inProduction)\n",
            "\n",
            "r-cran-apache.sedona/jammy 1.8.0-1.ca2204.1 all\n",
            "  CRAN Package 'apache.sedona' (R Interface for Apache Sedona)\n",
            "\n",
            "r-cran-catalog/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'catalog' (Access the 'Spark Catalog' API via 'sparklyr')\n",
            "\n",
            "r-cran-databaseconnector/jammy 7.0.0-1.ca2204.1 all\n",
            "  CRAN Package 'DatabaseConnector' (Connecting to Various Database Platforms)\n",
            "\n",
            "r-cran-fabricqueryr/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'fabricQueryR' (Query Data in 'Microsoft Fabric')\n",
            "\n",
            "r-cran-geospark/jammy 0.3.1-1.ca2204.1 all\n",
            "  CRAN Package 'geospark' (Bring Local Sf to Spark)\n",
            "\n",
            "r-cran-ggspark/jammy 0.0.2-1.ca2204.1 all\n",
            "  CRAN Package 'ggspark' ('ggplot2' Functions to Create Tufte Style Sparklines)\n",
            "\n",
            "r-cran-graphframes/jammy 0.1.2-1.ca2204.1 all\n",
            "  CRAN Package 'graphframes' (Interface for 'GraphFrames')\n",
            "\n",
            "r-cran-hatchr/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'hatchR' (Predict Fish Hatch and Emergence Timing)\n",
            "\n",
            "r-cran-ibmdbr/jammy 1.51.0-1.ca2204.1 all\n",
            "  CRAN Package 'ibmdbR' (IBM in-Database Analytics for R)\n",
            "\n",
            "r-cran-ltxsparklines/jammy 1.1.3-1.ca2204.1 all\n",
            "  CRAN Package 'ltxsparklines' (Lightweight Sparklines for a LaTeX Document)\n",
            "\n",
            "r-cran-microplot/jammy 1.0-47-1.ca2204.1 all\n",
            "  CRAN Package 'microplot' (Microplots (Sparklines) in 'LaTeX', 'Word', 'HTML', 'Excel')\n",
            "\n",
            "r-cran-notebookutils/jammy 1.5.3-1.ca2204.1 all\n",
            "  CRAN Package 'notebookutils' (Dummy R APIs Used in 'Azure Synapse Analytics' for LocalDevelopments)\n",
            "\n",
            "r-cran-oenokpm/jammy 2.4.1-1.ca2204.1 all\n",
            "  CRAN Package 'OenoKPM' (Modeling the Kinetics of Carbon Dioxide Production in AlcoholicFermentation)\n",
            "\n",
            "r-cran-parsnip/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'parsnip' (A Common API to Modeling and Analysis Functions)\n",
            "\n",
            "r-cran-paws.analytics/jammy 0.9.0-1.ca2204.1 all\n",
            "  CRAN Package 'paws.analytics' ('Amazon Web Services' Analytics Services)\n",
            "\n",
            "r-cran-pmmltransformations/jammy 1.3.3-1.ca2204.1 all\n",
            "  CRAN Package 'pmmlTransformations' (Transforms Input Data from a PMML Perspective)\n",
            "\n",
            "r-cran-pointblank/jammy 0.12.2-1.ca2204.1 all\n",
            "  CRAN Package 'pointblank' (Data Validation and Organization of Metadata for Local andRemote Tables)\n",
            "\n",
            "r-cran-pysparklyr/jammy 0.1.9-1.ca2204.1 all\n",
            "  CRAN Package 'pysparklyr' (Provides a 'PySpark' Back-End for the 'sparklyr' Package)\n",
            "\n",
            "r-cran-reactablefmtr/jammy 2.0.0-1.ca2204.1 all\n",
            "  CRAN Package 'reactablefmtr' (Streamlined Table Styling and Formatting for Reactable)\n",
            "\n",
            "r-cran-rquery/jammy 1.4.99-1.ca2204.1 all\n",
            "  CRAN Package 'rquery' (Relational Query Generator for Data Manipulation at Scale)\n",
            "\n",
            "r-cran-rsparkling/jammy 0.2.19-1.ca2204.1 all\n",
            "  CRAN Package 'rsparkling' (R Interface for H2O Sparkling Water)\n",
            "\n",
            "r-cran-s3.resourcer/jammy 1.1.2-1.ca2204.1 all\n",
            "  CRAN Package 's3.resourcer' (S3 Resource Resolver)\n",
            "\n",
            "r-cran-shinyml/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'shinyML' (Compare Supervised Machine Learning Models Using Shiny App)\n",
            "\n",
            "r-cran-skimr/jammy 2.2.1-1.ca2204.1 all\n",
            "  CRAN Package 'skimr' (Compact and Flexible Summaries of Data)\n",
            "\n",
            "r-cran-spark.sas7bdat/jammy 1.4-1.ca2204.1 all\n",
            "  CRAN Package 'spark.sas7bdat' (Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkavro/jammy 0.3.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkavro' (Load Avro file into 'Apache Spark')\n",
            "\n",
            "r-cran-sparkbq/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkbq' (Google 'BigQuery' Support for 'sparklyr')\n",
            "\n",
            "r-cran-sparkhail/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkhail' (A 'Sparklyr' Extension for 'Hail')\n",
            "\n",
            "r-cran-sparkline/jammy 2.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparkline' ('jQuery' Sparkline 'htmlwidget')\n",
            "\n",
            "r-cran-sparklyr/jammy 1.9.2-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr' (R Interface to Apache Spark)\n",
            "\n",
            "r-cran-sparklyr.flint/jammy 0.2.2-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.flint' (Sparklyr Extension for 'Flint')\n",
            "\n",
            "r-cran-sparklyr.nested/jammy 0.0.4-1.ca2204.1 all\n",
            "  CRAN Package 'sparklyr.nested' (A 'sparklyr' Extension for Nested Data)\n",
            "\n",
            "r-cran-sparktex/jammy 0.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparktex' (Generate LaTeX sparklines in R)\n",
            "\n",
            "r-cran-sparktf/jammy 0.1.0-1.ca2204.1 all\n",
            "  CRAN Package 'sparktf' (Interface for 'TensorFlow' 'TFRecord' Files with 'Apache Spark')\n",
            "\n",
            "r-cran-sparkwarc/jammy 0.1.6-1.ca2204.1 amd64\n",
            "  CRAN Package 'sparkwarc' (Load WARC Files into Apache Spark)\n",
            "\n",
            "r-cran-sparkxgb/jammy 0.2.1-1.ca2204.1 all\n",
            "  CRAN Package 'sparkxgb' (Interface for 'XGBoost' on 'Apache Spark')\n",
            "\n",
            "r-cran-sqlrender/jammy 1.19.4-1.ca2204.1 all\n",
            "  CRAN Package 'SqlRender' (Rendering Parameterized SQL and Translation to Dialects)\n",
            "\n",
            "r-cran-tidier/jammy 0.2.0-1.ca2204.1 all\n",
            "  CRAN Package 'tidier' (Enhanced 'mutate')\n",
            "\n",
            "r-cran-timevizpro/jammy 1.0.1-1.ca2204.1 all\n",
            "  CRAN Package 'TimeVizPro' (Dynamic Data Explorer: Visualize and Forecast with 'TimeVizPro')\n",
            "\n",
            "r-cran-variantspark/jammy 0.1.1-1.ca2204.1 all\n",
            "  CRAN Package 'variantspark' (A 'Sparklyr' Extension for 'VariantSpark')\n",
            "\n",
            "spark-core/stable 3.3.4-1 all\n",
            "  Lightning-Fast Cluster Computing\n",
            "\n",
            "spark-datanucleus/stable 3.3.4-1 all\n",
            "  DataNucleus libraries for Apache Spark\n",
            "\n",
            "spark-external/stable 3.3.4-1 all\n",
            "  External libraries for Apache Spark\n",
            "\n",
            "spark-history-server/stable 3.3.4-1 all\n",
            "  History server for Apache Spark\n",
            "\n",
            "spark-master/stable 3.3.4-1 all\n",
            "  Server for Spark master\n",
            "\n",
            "spark-python/stable 3.3.4-1 all\n",
            "  Python client for Spark\n",
            "\n",
            "spark-sparkr/stable 3.3.4-1 all\n",
            "  R package for Apache Spark\n",
            "\n",
            "spark-thriftserver/stable 3.3.4-1 all\n",
            "  Thrift server for Spark SQL\n",
            "\n",
            "spark-worker/stable 3.3.4-1 all\n",
            "  Server for Spark worker\n",
            "\n",
            "spark-yarn-shuffle/stable 3.3.4-1 all\n",
            "  Spark YARN Shuffle Service\n",
            "\n",
            "sparkleshare/jammy 3.28+git20190525+cf446c0-3 all\n",
            "  distributed collaboration and sharing tool\n",
            "\n",
            "zeppelin/stable 0.11.0-1 all\n",
            "  Web-based notebook for data analysts\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "for p in spark-core spark-master spark-worker; do\n",
        "  echo \"üõ†Ô∏è Installing $p\"\n",
        "  apt install -y $p\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIDMYoRqSkRV",
        "outputId": "04a77c33-8030-4af2-dded-07390683eb02"
      },
      "id": "YIDMYoRqSkRV",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Installing spark-core\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  hadoop-client\n",
            "The following NEW packages will be installed:\n",
            "  hadoop-client spark-core\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 263 MB of archives.\n",
            "After this operation, 296 MB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 hadoop-client amd64 3.3.6-1 [5,330 B]\n",
            "Get:2 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-core all 3.3.4-1 [263 MB]\n",
            "Fetched 263 MB in 8s (35.0 MB/s)\n",
            "Selecting previously unselected package hadoop-client.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 129140 files and directories currently installed.)\r\n",
            "Preparing to unpack .../hadoop-client_3.3.6-1_amd64.deb ...\r\n",
            "Unpacking hadoop-client (3.3.6-1) ...\r\n",
            "Selecting previously unselected package spark-core.\r\n",
            "Preparing to unpack .../spark-core_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-core (3.3.4-1) ...\r\n",
            "Setting up hadoop-client (3.3.6-1) ...\r\n",
            "Setting up spark-core (3.3.4-1) ...\r\n",
            "update-alternatives: using /etc/spark/conf.dist to provide /etc/spark/conf (spark-conf) in auto mode\r\n",
            "üõ†Ô∏è Installing spark-master\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  spark-master\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,688 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-master all 3.3.4-1 [4,688 B]\n",
            "Fetched 4,688 B in 0s (17.1 kB/s)\n",
            "Selecting previously unselected package spark-master.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 130353 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-master_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-master (3.3.4-1) ...\r\n",
            "Setting up spark-master (3.3.4-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n",
            "üõ†Ô∏è Installing spark-worker\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following NEW packages will be installed:\n",
            "  spark-worker\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,662 B of archives.\n",
            "After this operation, 19.5 kB of additional disk space will be used.\n",
            "Get:1 http://repos.bigtop.apache.org/releases/3.3.0/ubuntu/22.04/amd64 bigtop/contrib amd64 spark-worker all 3.3.4-1 [4,662 B]\n",
            "Fetched 4,662 B in 0s (40.1 kB/s)\n",
            "Selecting previously unselected package spark-worker.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 130357 files and directories currently installed.)\r\n",
            "Preparing to unpack .../spark-worker_3.3.4-1_all.deb ...\r\n",
            "Unpacking spark-worker (3.3.4-1) ...\r\n",
            "Setting up spark-worker (3.3.4-1) ...\r\n",
            "invoke-rc.d: could not determine current runlevel\r\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the Spark services"
      ],
      "metadata": {
        "id": "-GhU3uxmUD9Z"
      },
      "id": "-GhU3uxmUD9Z"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "for p in spark-master spark-worker; do\n",
        "  echo \"Starting $p\"\n",
        "  # systemctl start $p\n",
        "  service $p start\n",
        "done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvexxlRRUEHy",
        "outputId": "e3ba0fb8-cc1d-4336-cfe5-840a09cb5788"
      },
      "id": "uvexxlRRUEHy",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting spark-master\n",
            " * Starting Spark master (spark-master): \n",
            "Starting spark-worker\n",
            " * Starting Spark worker (spark-worker): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the `spark-master` is running. `7077` is the default port for `spark-master`."
      ],
      "metadata": {
        "id": "JKPHEvlDUV_x"
      },
      "id": "JKPHEvlDUV_x"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a1c1165",
        "outputId": "09db9ab2-9ee7-4cc2-8b0a-1724c5a5dbba"
      },
      "source": [
        "%%bash\n",
        "# Retry the command up to 3 times with a 5-second delay\n",
        "for i in {1..3}; do\n",
        "  ss -tuln | grep 7077 2>/dev/null && exit 0\n",
        "  sleep 5\n",
        "done\n",
        "exit 1 # Exit with an error code if the process is not found after retries"
      ],
      "id": "6a1c1165",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcp   LISTEN 0      4096     172.28.0.12:7077       0.0.0.0:*          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us call the new script `hy_wordcount_sparkCluster.py`. The script itself is actually the same as before."
      ],
      "metadata": {
        "id": "wN6_bMJXUpqU"
      },
      "id": "wN6_bMJXUpqU"
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hy_wordcount_sparkCluster.py\n",
        "\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep, SparkStep\n",
        "\n",
        "class HybridWordCount(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_words),\n",
        "            SparkStep(self.spark_wordcount),\n",
        "            MRStep(reducer=self.reducer_aggregate_counts),\n",
        "            MRStep(reducer=self.reducer_find_top_n)\n",
        "        ]\n",
        "\n",
        "    # --- Step 1: Mapper (classic MR)\n",
        "    def mapper_get_words(self, _, line):\n",
        "        for word in line.strip().split():\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    # --- Step 2: SparkStep\n",
        "    def spark_wordcount(self, input_uri, output_uri):\n",
        "        \"\"\"\n",
        "        Spark job: aggregate counts per word\n",
        "        \"\"\"\n",
        "        from pyspark.sql import SparkSession\n",
        "\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        sc = spark.sparkContext\n",
        "\n",
        "        # Load key-value pairs emitted by MR step\n",
        "        rdd = sc.textFile(input_uri)\n",
        "\n",
        "        # Each line is like: \"word\\t1\"\n",
        "        def parse_line(line):\n",
        "            parts = line.split('\\t')\n",
        "            return parts[0], int(parts[1])\n",
        "\n",
        "        word_rdd = rdd.map(parse_line)\n",
        "\n",
        "        # Aggregate using Spark reduceByKey\n",
        "        counts = word_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "        # Convert to text output\n",
        "        counts.map(lambda kv: f\"{kv[0]}\\t{kv[1]}\").saveAsTextFile(output_uri)\n",
        "\n",
        "        spark.stop()\n",
        "\n",
        "    # --- Step 3: Reducer (classic MR)\n",
        "    def reducer_aggregate_counts(self, word, counts):\n",
        "        # The values yielded by the Spark step are counts for each word\n",
        "        # in different partitions. We need to sum these up.\n",
        "        total_count = sum(counts)\n",
        "        yield None, (total_count, word) # Yield None as key to group all results\n",
        "\n",
        "    def reducer_find_top_n(self, _, count_word_pairs):\n",
        "        # Receive (count, word) pairs from the previous reducer\n",
        "        # Sort and take the top N (e.g., top 20)\n",
        "        top_n = sorted(count_word_pairs, reverse=True)[:20]\n",
        "        for count, word in top_n:\n",
        "            yield word, count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    HybridWordCount.run()"
      ],
      "metadata": {
        "id": "DMlYGptYVQ3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663894c9-cb26-40f3-9411-fafb9032dee2"
      },
      "id": "DMlYGptYVQ3l",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hy_wordcount_sparkCluster.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to configure Spark to run on YARN we need to make Spark aware of our Hadoop/YARN configuration. This is typically achieved by setting environment variables and ensuring Hadoop's configuration files are accessible.\n",
        "\n",
        "Here are the core variables we need in order to get YARN and Spark work together.\n",
        "\n",
        "```bash\n",
        "export HADOOP_CONF_DIR=/etc/hadoop/conf\n",
        "export YARN_CONF_DIR=/etc/hadoop/conf # Redundant but safe\n",
        "export SPARK_HOME=/usr/lib/spark\n",
        "export PATH=$PATH:$SPARK_HOME/bin\n",
        "```\n",
        "\n",
        "We also need to define `PYTHONPATH`. This ensures the Python environment (where the `mrjob` script runs) can find the `pyspark` module."
      ],
      "metadata": {
        "id": "L8K58cSEH96X"
      },
      "id": "L8K58cSEH96X"
    },
    {
      "cell_type": "code",
      "source": [
        "!grep SPARK_HOME /etc/spark/conf/spark-env.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP-XtXCCKFeZ",
        "outputId": "b6dcfc70-4bb0-43c3-a5fe-d9db116ce491"
      },
      "id": "RP-XtXCCKFeZ",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "export SPARK_HOME=${SPARK_HOME:-/usr/lib/spark}\n",
            "export SPARK_LIBRARY_PATH=${SPARK_LIBRARY_PATH:-${SPARK_HOME}/lib}\n",
            "export SCALA_LIBRARY_PATH=${SCALA_LIBRARY_PATH:-${SPARK_HOME}/lib}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%bash\n",
        "sudo -u hdfs \\\n",
        "hdfs dfs -rm -r hyOutputDir_sparkCluster 2>/dev/null\n",
        "\n",
        "# Copy the script to a location accessible by the hdfs user\n",
        "cp /content/hy_wordcount_sparkCluster.py /tmp/hy_wordcount_sparkCluster.py\n",
        "\n",
        "# Run the mrjob wordcount job using the hadoop runner with HADOOP_CONF_DIR set for the hdfs user\n",
        "# and specifying the Spark master as yarn\n",
        "# Define environment variables for clarity\n",
        "HADOOP_CONF_DIR_VAL=/etc/hadoop/conf\n",
        "SPARK_HOME_VAL=/usr/lib/spark\n",
        "HADOOP_STREAMING_JAR=/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
        "PYTHON_PACKAGES_PATH=/usr/local/lib/python3.12/dist-packages/pyspark/python\n",
        "\n",
        "sudo -u hdfs \\\n",
        "  HADOOP_CONF_DIR=$HADOOP_CONF_DIR_VAL \\\n",
        "  PYSPARK_PYTHON=/usr/bin/python3 \\\n",
        "  PYTHONPATH=$PYTHON_PACKAGES_PATH:$PYTHONPATH \\\n",
        "  python /tmp/hy_wordcount_sparkCluster.py \\\n",
        "    -v -r hadoop \\\n",
        "    --output-dir hyOutputDir_sparkCluster \\\n",
        "    --hadoop-streaming-jar $HADOOP_STREAMING_JAR \\\n",
        "    hdfs:///user/hdfs/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zykx0nisPwOP",
        "outputId": "2842c7cc-5625-4f57-f4e7-20195c16c3e7"
      },
      "id": "Zykx0nisPwOP",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "making runner: HadoopJobRunner(hadoop_streaming_jar=/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar, input_paths=['hdfs:///user/hdfs/input.txt'], mr_job_script=/tmp/hy_wordcount_sparkCluster.py, output_dir=hyOutputDir_sparkCluster, stdin=<_io.BufferedReader name='<stdin>'>, steps=[{'type': 'streaming', 'mapper': {'type': 'script'}}, {'jobconf': {}, 'spark_args': [], 'type': 'spark'}, {'type': 'streaming', 'reducer': {'type': 'script'}}, {'type': 'streaming', 'reducer': {'type': 'script'}}], ...)\n",
            "Looking for configs in /var/lib/hadoop-hdfs/.mrjob.conf\n",
            "Looking for configs in /etc/mrjob.conf\n",
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for hadoop runner\n",
            "Active configuration:\n",
            "{'bootstrap_mrjob': None,\n",
            " 'check_input_paths': True,\n",
            " 'cleanup': ['ALL'],\n",
            " 'cleanup_on_failure': ['NONE'],\n",
            " 'cmdenv': {},\n",
            " 'hadoop_bin': None,\n",
            " 'hadoop_extra_args': [],\n",
            " 'hadoop_log_dirs': [],\n",
            " 'hadoop_streaming_jar': '/usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar',\n",
            " 'hadoop_tmp_dir': 'tmp/mrjob',\n",
            " 'jobconf': {},\n",
            " 'label': None,\n",
            " 'libjars': [],\n",
            " 'local_tmp_dir': None,\n",
            " 'owner': 'hdfs',\n",
            " 'py_files': [],\n",
            " 'python_bin': None,\n",
            " 'read_logs': True,\n",
            " 'setup': [],\n",
            " 'sh_bin': None,\n",
            " 'spark_args': [],\n",
            " 'spark_deploy_mode': None,\n",
            " 'spark_master': None,\n",
            " 'spark_submit_bin': None,\n",
            " 'task_python_bin': None,\n",
            " 'upload_archives': [],\n",
            " 'upload_dirs': [],\n",
            " 'upload_files': []}\n",
            "Looking for hadoop binary in $PATH...\n",
            "Found hadoop binary: /usr/bin/hadoop\n",
            "> /usr/bin/hadoop fs -ls hdfs:///user/hdfs/input.txt\n",
            "STDOUT: -rw-r--r--   3 hdfs hdfs     151191 2025-10-10 19:04 hdfs:///user/hdfs/input.txt\n",
            "> /usr/bin/hadoop version\n",
            "Using Hadoop version 3.3.6\n",
            "Looking for spark-submit binary in $PATH...\n",
            "Found spark-submit binary: /usr/local/bin/spark-submit\n",
            "Creating temp directory /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488\n",
            "archiving /usr/local/lib/python3.12/dist-packages/mrjob -> /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/mrjob.zip as mrjob/\n",
            "Writing streaming setup wrapper script to /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/setup-wrapper.sh:\n",
            "  # store $PWD\n",
            "  __mrjob_PWD=$PWD\n",
            "  \n",
            "  # obtain exclusive file lock\n",
            "  exec 9>/tmp/wrapper.lock.hy_wordcount_sparkCluster.hdfs.20251010.191056.189488\n",
            "  python3 -c 'import fcntl; fcntl.flock(9, fcntl.LOCK_EX)'\n",
            "  \n",
            "  # setup commands\n",
            "  {\n",
            "    export PYTHONPATH=$__mrjob_PWD/mrjob.zip:$PYTHONPATH\n",
            "  } 0</dev/null 1>&2\n",
            "  \n",
            "  # release exclusive file lock\n",
            "  exec 9>&-\n",
            "  \n",
            "  # run task from the original working directory\n",
            "  cd $__mrjob_PWD\n",
            "  \"$@\"\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd\n",
            "uploading working dir files to hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd...\n",
            "  /tmp/hy_wordcount_sparkCluster.py -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount_sparkCluster.py hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py\n",
            "  /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip\n",
            "  /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/setup-wrapper.sh -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/setup-wrapper.sh hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh\n",
            "> /usr/bin/hadoop fs -mkdir -p hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/\n",
            "Copying other local files to hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/\n",
            "  /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/mrjob.zip -> hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/mrjob.zip\n",
            "> /usr/bin/hadoop fs -put /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/mrjob.zip hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/mrjob.zip\n",
            "Running step 1 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py#hy_wordcount_sparkCluster.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh#setup-wrapper.sh' -D mapreduce.job.reduces=0 -input hdfs:///user/hdfs/input.txt -output hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0000 -mapper '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount_sparkCluster.py --step-num=0 --mapper'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', '/usr/bin/python3'), ('PYTHONPATH', '/usr/local/lib/python3.12/dist-packages/pyspark/python:/env/python'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python /tmp/hy_wordcount_sparkCluster.py -v -r hadoop --output-dir hyOutputDir_sparkCluster --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar15899290368137890632/] [] /tmp/streamjob13325940240403226146.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0006\n",
            "  Total input files to process : 1\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0006\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0006\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0006/\n",
            "  Running job: job_1760123048431_0006\n",
            "  Job job_1760123048431_0006 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 50% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "  Job job_1760123048431_0006 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0000\n",
            "Counters: 33\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=155287\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=265634\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=0\n",
            "\t\tFILE: Number of bytes written=561038\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=155473\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=265634\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=14\n",
            "\t\tHDFS: Number of write operations=4\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=29523968\n",
            "\t\tTotal time spent by all map tasks (ms)=28832\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=28832\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=28832\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=2680\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=278\n",
            "\t\tInput split bytes=186\n",
            "\t\tMap input records=3384\n",
            "\t\tMap output records=26543\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tPeak Map Physical memory (bytes)=228352000\n",
            "\t\tPeak Map Virtual memory (bytes)=2728554496\n",
            "\t\tPhysical memory (bytes) snapshot=451878912\n",
            "\t\tSpilled Records=0\n",
            "\t\tTotal committed heap usage (bytes)=678428672\n",
            "\t\tVirtual memory (bytes) snapshot=5456367616\n",
            "Running step 2 of 4...\n",
            "> /usr/local/bin/spark-submit --conf spark.executorEnv.PYSPARK_PYTHON=python3 --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --master yarn --deploy-mode client --files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh --py-files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/mrjob.zip /tmp/hy_wordcount_sparkCluster.py --step-num=1 --spark hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0000 hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0001\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', 'python3'), ('PYTHONPATH', '/usr/local/lib/python3.12/dist-packages/pyspark/python:/env/python'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python /tmp/hy_wordcount_sparkCluster.py -v -r hadoop --output-dir hyOutputDir_sparkCluster --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "> /usr/local/bin/spark-submit --conf spark.executorEnv.PYSPARK_PYTHON=python3 --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=python3 --master yarn --deploy-mode client --files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh --py-files hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/mrjob.zip /tmp/hy_wordcount_sparkCluster.py --step-num=1 --spark hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0000 hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0001\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', 'python3'), ('PYTHONPATH', '/usr/local/lib/python3.12/dist-packages/pyspark/python:/env/python'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python /tmp/hy_wordcount_sparkCluster.py -v -r hadoop --output-dir hyOutputDir_sparkCluster --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking spark-submit via PTY\n",
            "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "  Running Spark version 3.5.1\n",
            "  OS info Linux, 6.6.97+, amd64\n",
            "  Java version 11.0.28\n",
            "  ==============================================================\n",
            "  No custom resources configured for spark.driver.\n",
            "  ==============================================================\n",
            "  Submitted application: hy_wordcount_sparkCluster.py\n",
            "  Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "  Limiting resource is cpus at 1 tasks per executor\n",
            "  Added ResourceProfile id: 0\n",
            "  Changing view acls to: hdfs\n",
            "  Changing modify acls to: hdfs\n",
            "  Changing view acls groups to: \n",
            "  Changing modify acls groups to: \n",
            "  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n",
            "  Successfully started service 'sparkDriver' on port 44723.\n",
            "  Registering MapOutputTracker\n",
            "  Registering BlockManagerMaster\n",
            "  Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "  BlockManagerMasterEndpoint up\n",
            "  Registering BlockManagerMasterHeartbeat\n",
            "  Created local directory at /tmp/blockmgr-c0097f9b-155e-4ae5-92e6-099d192c0951\n",
            "  MemoryStore started with capacity 434.4 MiB\n",
            "  Registering OutputCommitCoordinator\n",
            "  Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "  Successfully started service 'SparkUI' on port 4040.\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)\n",
            "  Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
            "  Setting up container launch context for our AM\n",
            "  Setting up the launch environment for our AM container\n",
            "  Preparing resources for our AM container\n",
            "  Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
            "  Uploading resource file:/tmp/spark-761dd896-d250-4ffb-8751-9e40b5af3d06/__spark_libs__9571183166094031069.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0007/__spark_libs__9571183166094031069.zip\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip\n",
            "  Source and destination file systems are the same. Not copying hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh\n",
            "  Uploading resource file:/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/pyspark.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0007/pyspark.zip\n",
            "  Uploading resource file:/usr/local/lib/python3.12/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0007/py4j-0.10.9.7-src.zip\n",
            "  Same name resource hdfs://localhost:9000/user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/mrjob.zip added multiple times to distributed cache\n",
            "  Uploading resource file:/tmp/spark-761dd896-d250-4ffb-8751-9e40b5af3d06/__spark_conf__5300357461221156999.zip -> hdfs://localhost:9000/user/hdfs/.sparkStaging/application_1760123048431_0007/__spark_conf__.zip\n",
            "  Changing view acls to: hdfs\n",
            "  Changing modify acls to: hdfs\n",
            "  Changing view acls groups to: \n",
            "  Changing modify acls groups to: \n",
            "  SecurityManager: authentication disabled; ui acls disabled; users with view permissions: hdfs; groups with view permissions: EMPTY; users with modify permissions: hdfs; groups with modify permissions: EMPTY\n",
            "  Submitting application application_1760123048431_0007 to ResourceManager\n",
            "  Submitted application application_1760123048431_0007\n",
            "  Application report for application_1760123048431_0007 (state: ACCEPTED)\n",
            "  \n",
            "\t client token: N/A\n",
            "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
            "\t ApplicationMaster host: N/A\n",
            "  -1\n",
            "\t queue: default\n",
            "\t start time: 1760123541273\n",
            "\t final status: UNDEFINED\n",
            "\t tracking URL: http://87c92d6b4b45:8088/proxy/application_1760123048431_0007/\n",
            "\t user: hdfs\n",
            "  Application report for application_1760123048431_0007 (state: RUNNING)\n",
            "  \n",
            "\t client token: N/A\n",
            "\t diagnostics: N/A\n",
            "\t ApplicationMaster host: 172.28.0.12\n",
            "  -1\n",
            "\t queue: default\n",
            "\t start time: 1760123541273\n",
            "\t final status: UNDEFINED\n",
            "\t tracking URL: http://87c92d6b4b45:8088/proxy/application_1760123048431_0007/\n",
            "\t user: hdfs\n",
            "  Application application_1760123048431_0007 has started running.\n",
            "  Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42515.\n",
            "  Server created on 87c92d6b4b45:42515\n",
            "  Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "  Registering BlockManager BlockManagerId(driver, 87c92d6b4b45, 42515, None)\n",
            "  Registering block manager 87c92d6b4b45:42515 with 434.4 MiB RAM, BlockManagerId(driver, 87c92d6b4b45, 42515, None)\n",
            "  Registered BlockManager BlockManagerId(driver, 87c92d6b4b45, 42515, None)\n",
            "  Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> 87c92d6b4b45, PROXY_URI_BASES -> http://87c92d6b4b45:8088/proxy/application_1760123048431_0007), /proxy/application_1760123048431_0007\n",
            "  Initialized BlockManager: BlockManagerId(driver, 87c92d6b4b45, 42515, None)\n",
            "  Adding filter to /jobs: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/pool: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/pool/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/rdd: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /storage/rdd/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /environment: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /environment/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/threadDump: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/threadDump/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/heapHistogram: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /executors/heapHistogram/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /static: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /api: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /jobs/job/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /stages/stage/kill: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  Adding filter to /metrics/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
            "  ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
            "  SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\n",
            "  Block broadcast_0 stored as values in memory (estimated size 221.6 KiB, free 434.2 MiB)\n",
            "  Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.8 KiB, free 434.2 MiB)\n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:42515 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "  Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:49424) with ID 1,  ResourceProfileId 0\n",
            "  Total input files to process : 2\n",
            "  Registering block manager 87c92d6b4b45:39729 with 434.4 MiB RAM, BlockManagerId(1, 87c92d6b4b45, 39729, None)\n",
            "  mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "  Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "  File Output Committer Algorithm version is 1\n",
            "  FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "  Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "  Registering RDD 3 (reduceByKey at /tmp/hy_wordcount_sparkCluster.py:41) as input to shuffle 0\n",
            "  Got job 0 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
            "  Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
            "  Parents of final stage: List(ShuffleMapStage 0)\n",
            "  Missing parents: List(ShuffleMapStage 0)\n",
            "  Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/hy_wordcount_sparkCluster.py:41), which has no missing parents\n",
            "  Block broadcast_1 stored as values in memory (estimated size 13.6 KiB, free 434.1 MiB)\n",
            "  Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.1 MiB)\n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:42515 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "  Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/hy_wordcount_sparkCluster.py:41) (first 15 tasks are for partitions Vector(0, 1))\n",
            "  Adding task set 0.0 with 2 tasks resource profile 0\n",
            "  Starting task 0.0 in stage 0.0 (TID 0) (87c92d6b4b45, executor 1, partition 0, NODE_LOCAL, 7754 bytes) \n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:39729 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:39729 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.28.0.12:43056) with ID 2,  ResourceProfileId 0\n",
            "  Registering block manager 87c92d6b4b45:38449 with 434.4 MiB RAM, BlockManagerId(2, 87c92d6b4b45, 38449, None)\n",
            "  Starting task 1.0 in stage 0.0 (TID 1) (87c92d6b4b45, executor 2, partition 1, NODE_LOCAL, 7754 bytes) \n",
            "  Added broadcast_1_piece0 in memory on 87c92d6b4b45:38449 (size: 8.4 KiB, free: 434.4 MiB)\n",
            "  Added broadcast_0_piece0 in memory on 87c92d6b4b45:38449 (size: 32.8 KiB, free: 434.4 MiB)\n",
            "  Finished task 0.0 in stage 0.0 (TID 0) in 9648 ms on 87c92d6b4b45 (executor 1) (1/2)\n",
            "  Connected to AccumulatorServer at host: 127.0.0.1 port: 41243\n",
            "  Finished task 1.0 in stage 0.0 (TID 1) in 8560 ms on 87c92d6b4b45 (executor 2) (2/2)\n",
            "  Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "  ShuffleMapStage 0 (reduceByKey at /tmp/hy_wordcount_sparkCluster.py:41) finished in 12.369 s\n",
            "  looking for newly runnable stages\n",
            "  running: Set()\n",
            "  waiting: Set(ResultStage 1)\n",
            "  failed: Set()\n",
            "  Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "  Block broadcast_2 stored as values in memory (estimated size 110.4 KiB, free 434.0 MiB)\n",
            "  Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.2 KiB, free 434.0 MiB)\n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:42515 (size: 42.2 KiB, free: 434.3 MiB)\n",
            "  Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
            "  Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "  Adding task set 1.0 with 2 tasks resource profile 0\n",
            "  Starting task 0.0 in stage 1.0 (TID 2) (87c92d6b4b45, executor 2, partition 0, NODE_LOCAL, 7444 bytes) \n",
            "  Starting task 1.0 in stage 1.0 (TID 3) (87c92d6b4b45, executor 1, partition 1, NODE_LOCAL, 7444 bytes) \n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:38449 (size: 42.2 KiB, free: 434.3 MiB)\n",
            "  Added broadcast_2_piece0 in memory on 87c92d6b4b45:39729 (size: 42.2 KiB, free: 434.3 MiB)\n",
            "  Asked to send map output locations for shuffle 0 to 172.28.0.12:43056\n",
            "  Asked to send map output locations for shuffle 0 to 172.28.0.12:49424\n",
            "  Removed broadcast_1_piece0 on 87c92d6b4b45:42515 in memory (size: 8.4 KiB, free: 434.3 MiB)\n",
            "  Removed broadcast_1_piece0 on 87c92d6b4b45:38449 in memory (size: 8.4 KiB, free: 434.3 MiB)\n",
            "  Finished task 0.0 in stage 1.0 (TID 2) in 1952 ms on 87c92d6b4b45 (executor 2) (1/2)\n",
            "  Removed broadcast_1_piece0 on 87c92d6b4b45:39729 in memory (size: 8.4 KiB, free: 434.3 MiB)\n",
            "  Finished task 1.0 in stage 1.0 (TID 3) in 2031 ms on 87c92d6b4b45 (executor 1) (2/2)\n",
            "  Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "  ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 2.099 s\n",
            "  Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "  Killing all running tasks in stage 1: Stage finished\n",
            "  Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 14.875945 s\n",
            "  Start to commit write Job job_202510101912491607304965414097860_0008.\n",
            "  Write Job job_202510101912491607304965414097860_0008 committed. Elapsed time: 77 ms.\n",
            "  SparkContext is stopping with exitCode 0.\n",
            "  Stopped Spark web UI at http://87c92d6b4b45:4040\n",
            "  Interrupting monitor thread\n",
            "  Shutting down all executors\n",
            "  Asking each executor to shut down\n",
            "  YARN client scheduler backend Stopped\n",
            "  MapOutputTrackerMasterEndpoint stopped!\n",
            "  MemoryStore cleared\n",
            "  BlockManager stopped\n",
            "  BlockManagerMaster stopped\n",
            "  OutputCommitCoordinator stopped!\n",
            "  Successfully stopped SparkContext\n",
            "  Shutdown hook called\n",
            "  Deleting directory /tmp/spark-761dd896-d250-4ffb-8751-9e40b5af3d06\n",
            "  Deleting directory /tmp/spark-b6d33b1a-7bd4-4215-84c7-0384e3a8ee91\n",
            "  Deleting directory /tmp/spark-761dd896-d250-4ffb-8751-9e40b5af3d06/pyspark-59721f5c-a259-4db2-a899-8aaaa92d1c5b\n",
            "Running step 3 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py#hy_wordcount_sparkCluster.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh#setup-wrapper.sh' -input hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0001 -output hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0002 -mapper cat -reducer '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount_sparkCluster.py --step-num=2 --reducer'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', '/usr/bin/python3'), ('PYTHONPATH', '/usr/local/lib/python3.12/dist-packages/pyspark/python:/env/python'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python /tmp/hy_wordcount_sparkCluster.py -v -r hadoop --output-dir hyOutputDir_sparkCluster --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar2711865851942031851/] [] /tmp/streamjob15558291804957006348.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0008\n",
            "  Total input files to process : 2\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0008\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0008\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0008/\n",
            "  Running job: job_1760123048431_0008\n",
            "  Job job_1760123048431_0008 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "   map 100% reduce 100%\n",
            "  Job job_1760123048431_0008 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0002\n",
            "Counters: 54\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=66732\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=107012\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=76808\n",
            "\t\tFILE: Number of bytes written=996486\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=67082\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=107012\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=24272896\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=7826432\n",
            "\t\tTotal time spent by all map tasks (ms)=23704\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=23704\n",
            "\t\tTotal time spent by all reduce tasks (ms)=7643\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=7643\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=23704\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=7643\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=3820\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=306\n",
            "\t\tInput split bytes=350\n",
            "\t\tMap input records=5035\n",
            "\t\tMap output bytes=66732\n",
            "\t\tMap output materialized bytes=76814\n",
            "\t\tMap output records=5035\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tPeak Map Physical memory (bytes)=327098368\n",
            "\t\tPeak Map Virtual memory (bytes)=2726289408\n",
            "\t\tPeak Reduce Physical memory (bytes)=222072832\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2724089856\n",
            "\t\tPhysical memory (bytes) snapshot=869281792\n",
            "\t\tReduce input groups=5035\n",
            "\t\tReduce input records=5035\n",
            "\t\tReduce output records=5035\n",
            "\t\tReduce shuffle bytes=76814\n",
            "\t\tShuffled Maps =2\n",
            "\t\tSpilled Records=10070\n",
            "\t\tTotal committed heap usage (bytes)=827326464\n",
            "\t\tVirtual memory (bytes) snapshot=8175906816\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "Running step 4 of 4...\n",
            "> /usr/bin/hadoop jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -files 'hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/hy_wordcount_sparkCluster.py#hy_wordcount_sparkCluster.py,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/mrjob.zip#mrjob.zip,hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/files/wd/setup-wrapper.sh#setup-wrapper.sh' -input hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488/step-output/0002 -output hdfs:///user/hdfs/hyOutputDir_sparkCluster -mapper cat -reducer '/bin/sh -ex setup-wrapper.sh python3 hy_wordcount_sparkCluster.py --step-num=3 --reducer'\n",
            "  with environment: [('HADOOP_CONF_DIR', '/etc/hadoop/conf'), ('HOME', '/var/lib/hadoop-hdfs'), ('HOSTNAME', '87c92d6b4b45'), ('LANG', 'en_US.UTF-8'), ('LANGUAGE', 'en_US'), ('LC_ALL', 'en_US.UTF-8'), ('LOGNAME', 'hdfs'), ('MAIL', '/var/mail/hdfs'), ('PATH', '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'), ('PWD', '/content'), ('PYSPARK_PYTHON', '/usr/bin/python3'), ('PYTHONPATH', '/usr/local/lib/python3.12/dist-packages/pyspark/python:/env/python'), ('SHELL', '/bin/bash'), ('SHLVL', '0'), ('SUDO_COMMAND', '/usr/local/bin/python /tmp/hy_wordcount_sparkCluster.py -v -r hadoop --output-dir hyOutputDir_sparkCluster --hadoop-streaming-jar /usr/lib/hadoop/tools/lib/hadoop-streaming-3.3.6.jar hdfs:///user/hdfs/input.txt'), ('SUDO_GID', '0'), ('SUDO_UID', '0'), ('SUDO_USER', 'root'), ('TERM', 'xterm-color'), ('USER', 'hdfs')]\n",
            "Invoking Hadoop via PTY\n",
            "  packageJobJar: [/tmp/hadoop-unjar10462628647109652230/] [] /tmp/streamjob2010190145943810160.jar tmpDir=null\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Connecting to ResourceManager at /0.0.0.0:8032\n",
            "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/hdfs/.staging/job_1760123048431_0009\n",
            "  Total input files to process : 1\n",
            "  number of splits:2\n",
            "  Submitting tokens for job: job_1760123048431_0009\n",
            "  Executing with tokens: []\n",
            "  resource-types.xml not found\n",
            "  Unable to find 'resource-types.xml'.\n",
            "  Submitted application application_1760123048431_0009\n",
            "  The url to track the job: http://87c92d6b4b45:8088/proxy/application_1760123048431_0009/\n",
            "  Running job: job_1760123048431_0009\n",
            "  Job job_1760123048431_0009 running in uber mode : false\n",
            "   map 0% reduce 0%\n",
            "   map 50% reduce 0%\n",
            "   map 100% reduce 0%\n",
            "   map 100% reduce 100%\n",
            "  Job job_1760123048431_0009 completed successfully\n",
            "  Output directory: hdfs:///user/hdfs/hyOutputDir_sparkCluster\n",
            "Counters: 54\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=111108\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=196\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=117088\n",
            "\t\tFILE: Number of bytes written=1076875\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=111458\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\t\tHDFS: Number of bytes written=196\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\tJob Counters \n",
            "\t\tData-local map tasks=2\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=23866368\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=6901760\n",
            "\t\tTotal time spent by all map tasks (ms)=23307\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=23307\n",
            "\t\tTotal time spent by all reduce tasks (ms)=6740\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=6740\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=23307\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=6740\n",
            "\tMap-Reduce Framework\n",
            "\t\tCPU time spent (ms)=3940\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tGC time elapsed (ms)=344\n",
            "\t\tInput split bytes=350\n",
            "\t\tMap input records=5035\n",
            "\t\tMap output bytes=107012\n",
            "\t\tMap output materialized bytes=117094\n",
            "\t\tMap output records=5035\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tPeak Map Physical memory (bytes)=330506240\n",
            "\t\tPeak Map Virtual memory (bytes)=2727870464\n",
            "\t\tPeak Reduce Physical memory (bytes)=224280576\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2725429248\n",
            "\t\tPhysical memory (bytes) snapshot=865652736\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce input records=5035\n",
            "\t\tReduce output records=20\n",
            "\t\tReduce shuffle bytes=117094\n",
            "\t\tShuffled Maps =2\n",
            "\t\tSpilled Records=10070\n",
            "\t\tTotal committed heap usage (bytes)=787480576\n",
            "\t\tVirtual memory (bytes) snapshot=8175362048\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "job output is in hdfs:///user/hdfs/hyOutputDir_sparkCluster\n",
            "Removing HDFS temp directory hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488...\n",
            "> /usr/bin/hadoop fs -rm -R -f -skipTrash hdfs:///user/hdfs/tmp/mrjob/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488\n",
            "Removing temp directory /tmp/hy_wordcount_sparkCluster.hdfs.20251010.191056.189488...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25.3 ms, sys: 8.66 ms, total: 33.9 ms\n",
            "Wall time: 3min 49s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -cat hyOutputDir_sparkCluster/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3uthcC1QhCR",
        "outputId": "ea54f597-e00e-4987-e1ee-6c04f08d7949"
      },
      "id": "u3uthcC1QhCR",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"the\"\t1614\n",
            "\"and\"\t767\n",
            "\"to\"\t706\n",
            "\"a\"\t619\n",
            "\"she\"\t518\n",
            "\"of\"\t496\n",
            "\"said\"\t420\n",
            "\"it\"\t362\n",
            "\"in\"\t351\n",
            "\"was\"\t328\n",
            "\"you\"\t257\n",
            "\"i\"\t249\n",
            "\"as\"\t249\n",
            "\"alice\"\t221\n",
            "\"that\"\t216\n",
            "\"her\"\t207\n",
            "\"at\"\t204\n",
            "\"had\"\t176\n",
            "\"with\"\t170\n",
            "\"all\"\t154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u hdfs hdfs dfs -ls hyOutputDir_sparkCluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tews5WWXQkqt",
        "outputId": "44880b18-02c3-4198-8379-12f6fa150c34"
      },
      "id": "tews5WWXQkqt",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   3 hdfs hdfs          0 2025-10-10 19:14 hyOutputDir_sparkCluster/_SUCCESS\n",
            "-rw-r--r--   3 hdfs hdfs        196 2025-10-10 19:14 hyOutputDir_sparkCluster/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** for small amounts of data using a cluster may result in longer running times because of the overhead of setting up the job on the cluster. To really appreciate the advantages of Hadoop/Spark you need to work with large amounts of data (and with more than 2 virtual CPUs, as in the Google Colab environment)."
      ],
      "metadata": {
        "id": "ieSIB47pBBkR"
      },
      "id": "ieSIB47pBBkR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}